<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Cemal Turkoglu]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>https://turkogluc.com/</link><image><url>https://turkogluc.com/favicon.png</url><title>Cemal Turkoglu</title><link>https://turkogluc.com/</link></image><generator>Ghost 5.49</generator><lastBuildDate>Thu, 25 May 2023 09:22:16 GMT</lastBuildDate><atom:link href="https://turkogluc.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Centralized Logging and Monitoring with Elastic Stack]]></title><description><![CDATA[Elasticsearch host and docker container monitoring and collecting logs, sending email alerts based on the metric thresholds.
Elastic alert email actions. Spring java logback config.]]></description><link>https://turkogluc.com/centralised-logging-and-monitoring-with-elastic-stack/</link><guid isPermaLink="false">646f24c59b6311000195da66</guid><category><![CDATA[Elasticsearch]]></category><category><![CDATA[Java]]></category><category><![CDATA[devops]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Sun, 13 Feb 2022 21:29:19 GMT</pubDate><media:content url="https://turkogluc.com/content/images/2022/02/5ad917357c0c50d0337ce7773184d0ed.png" medium="image"/><content:encoded><![CDATA[<img src="https://turkogluc.com/content/images/2022/02/5ad917357c0c50d0337ce7773184d0ed.png" alt="Centralized Logging and Monitoring with Elastic Stack"><p>Elasticsearch comes with very handy features in the new versions for collecting logs and also monitoring the hosts or docker containers. We can run <code>elasticsearch</code> and <code>kibana</code> with the following docker-compose file.</p><pre><code class="language-yaml">version: &apos;2.2&apos;
services:
  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    container_name: es01
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;
      - xpack.security.enabled=true
      - xpack.security.authc.api_key.enabled=true
      - xpack.security.audit.enabled=true
      - ELASTIC_PASSWORD=somethingsecret
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data01:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      - elastic


  kib01:
    image: docker.elastic.co/kibana/kibana:7.17.0
    container_name: kib01
    ports:
      - 5601:5601
    environment:
      ELASTICSEARCH_URL: http://es01:9200
      ELASTICSEARCH_HOSTS: &apos;[&quot;http://es01:9200&quot;]&apos;
      ELASTICSEARCH_USERNAME: elastic
      ELASTICSEARCH_PASSWORD: somethingsecret
      SERVER_PUBLICBASEURL: http://localhost:5601
      XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY: &quot;something_at_least_32_characters&quot;
      XPACK_REPORTING_ENCRYPTIONKEY: &quot;something_at_least_32_characters&quot;
      XPACK_SECURITY_ENCRYPTIONKEY: &quot;something_at_least_32_characters&quot;
    depends_on:
      - es01
    networks:
      - elastic


volumes:
  data01:
    driver: local

networks:
  elastic:
    driver: bridge</code></pre><p>Reference on configuring the kibana security:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.elastic.co/guide/en/kibana/current/using-kibana-with-security.html?ref=localhost#security-configure-settings"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Configure security in Kibana | Kibana Guide [8.0] | Elastic</div><div class="kg-bookmark-description">A list of the supported authentication mechanisms in Kibana.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.elastic.co/android-chrome-192x192.png" alt="Centralized Logging and Monitoring with Elastic Stack"><span class="kg-bookmark-author">Elastic</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.elastic.co/static/images/elastic-logo-200.png" alt="Centralized Logging and Monitoring with Elastic Stack"></div></a></figure><h2 id="collecting-metrics-with-metricbeat">Collecting Metrics with Metricbeat</h2><p>We can run the following docker container in each host that we want to collect metrics from.</p><pre><code class="language-yaml">version: &apos;3.8&apos;
services:

  metricbeat:
    image: docker.elastic.co/beats/metricbeat:7.17.0
    user: root
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./metricbeat.docker.yml:/usr/share/metricbeat/metricbeat.yml:ro
      - /sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro
      - /proc:/hostfs/proc:ro
      - /:/hostfs:ro
      - /var/log:/var/log:rw
      - /var/lib/docker/containers:/var/lib/docker/containers:rw
    network_mode: &quot;host&quot;</code></pre><p><code>metricbeat</code> configuration is provided with the <code>metricbeat.docker.yml</code> file:</p><pre><code class="language-yaml">metricbeat.max_start_delay: 10s
# setup.ilm.enabled: true
setup.dashboards.enabled: true
setup.dashboards.beat: metricbeat


#==========================  Modules configuration =============================
metricbeat.modules:

#-------------------------------- System Module --------------------------------
- module: system
  metricsets:
    - cpu             # CPU usage
    - load            # CPU load averages
    - memory          # Memory usage
    - network         # Network IO
    - process         # Per process metrics
    - process_summary # Process summary
    - uptime          # System Uptime
    - socket_summary  # Socket summary
    #- core           # Per CPU core usage
    #- diskio         # Disk IO
    #- filesystem     # File system usage for each mountpoint
    #- fsstat         # File system summary metrics
    #- raid           # Raid
    #- socket         # Sockets and connection info (linux only)
    #- service        # systemd service information
  enabled: true
  period: 10s
  processes: [&apos;.*&apos;]

  # Configure the mount point of the host&#x2019;s filesystem for use in monitoring a host from within a container
  #system.hostfs: &quot;/hostfs&quot;

  # Configure the metric types that are included by these metricsets.
  cpu.metrics:  [&quot;percentages&quot;,&quot;normalized_percentages&quot;]  # The other available option is ticks.
  core.metrics: [&quot;percentages&quot;]  # The other available option is ticks.

#-------------------------------- Docker Module --------------------------------
- module: docker
  metricsets:
    - &quot;container&quot;
    - &quot;cpu&quot;
    - &quot;diskio&quot;
    - &quot;event&quot;
    - &quot;healthcheck&quot;
    - &quot;info&quot;
    #- &quot;image&quot;
    - &quot;memory&quot;
    - &quot;network&quot;
    #- &quot;network_summary&quot;
  hosts: [&quot;unix:///var/run/docker.sock&quot;]
  period: 10s
  enabled: true

# ---------------------------- Elasticsearch Output ----------------------------
output.elasticsearch:
  hosts: [&quot;localhost:9200&quot;]
  username: &quot;elastic&quot;
  password: &quot;somethingsecret&quot;

setup.kibana:
    host: &quot;localhost:5601&quot;
    username: &quot;elastic&quot;
    password: &quot;somethingsecret&quot;</code></pre><p>We use <code>system</code> module to collect the host metrics and <code>docker</code> module to collect stats via docker engine. <code>Metricbeat</code> supports many modules that can be integrated and used to collect metrics, such as we can collect metrics from docker, kubernetes, many databases, message queues like rabbitmq, kafka, and many more applications. You can find the modules and their configuration from here:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-modules.html?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Modules | Metricbeat Reference [8.0] | Elastic</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.elastic.co/android-chrome-192x192.png" alt="Centralized Logging and Monitoring with Elastic Stack"><span class="kg-bookmark-author">Elastic</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.elastic.co/static/images/elastic-logo-200.png" alt="Centralized Logging and Monitoring with Elastic Stack"></div></a></figure><p>It is very easy to configure these modules and collect logs from various systems. For example we can collect metrics from rabbitmq server with the following configuration:</p><pre><code class="language-yaml">#------------------------------ Rabbit Module ---------------------------------

- module: rabbitmq
  metricsets: [&quot;node&quot;, &quot;queue&quot;, &quot;connection&quot;, &quot;exchange&quot;]
  enabled: true
  period: 10s
  hosts: [&quot;rabbithost:15672&quot;]
  username: admin
  password: rabbit-password</code></pre><p>As we enabled <code>setup.dashboards.enabled</code> config, metricbeat loads ready dashboards to kibana. We can customise or create new views or dashboards. Some of the ready dashboards looks as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2022/02/Screenshot-2022-02-13-at-13.44.30.png" class="kg-image" alt="Centralized Logging and Monitoring with Elastic Stack" loading="lazy" width="2000" height="1118" srcset="https://turkogluc.com/content/images/size/w600/2022/02/Screenshot-2022-02-13-at-13.44.30.png 600w, https://turkogluc.com/content/images/size/w1000/2022/02/Screenshot-2022-02-13-at-13.44.30.png 1000w, https://turkogluc.com/content/images/size/w1600/2022/02/Screenshot-2022-02-13-at-13.44.30.png 1600w, https://turkogluc.com/content/images/size/w2400/2022/02/Screenshot-2022-02-13-at-13.44.30.png 2400w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2022/02/Screenshot-2022-02-13-at-13.45.04.png" class="kg-image" alt="Centralized Logging and Monitoring with Elastic Stack" loading="lazy" width="2000" height="1063" srcset="https://turkogluc.com/content/images/size/w600/2022/02/Screenshot-2022-02-13-at-13.45.04.png 600w, https://turkogluc.com/content/images/size/w1000/2022/02/Screenshot-2022-02-13-at-13.45.04.png 1000w, https://turkogluc.com/content/images/size/w1600/2022/02/Screenshot-2022-02-13-at-13.45.04.png 1600w, https://turkogluc.com/content/images/size/w2400/2022/02/Screenshot-2022-02-13-at-13.45.04.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>From the observability -&gt; inventory we can view hosts and/or containers with metrics.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2022/02/Screenshot-2022-02-13-at-13.46.18.png" class="kg-image" alt="Centralized Logging and Monitoring with Elastic Stack" loading="lazy" width="2000" height="1035" srcset="https://turkogluc.com/content/images/size/w600/2022/02/Screenshot-2022-02-13-at-13.46.18.png 600w, https://turkogluc.com/content/images/size/w1000/2022/02/Screenshot-2022-02-13-at-13.46.18.png 1000w, https://turkogluc.com/content/images/size/w1600/2022/02/Screenshot-2022-02-13-at-13.46.18.png 1600w, https://turkogluc.com/content/images/size/w2400/2022/02/Screenshot-2022-02-13-at-13.46.18.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>We can check the the logs (if we are collecting) or metrics of any host or container by clicking on the views here.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2022/02/Screenshot-2022-02-13-at-13.46.56.png" class="kg-image" alt="Centralized Logging and Monitoring with Elastic Stack" loading="lazy" width="2000" height="1121" srcset="https://turkogluc.com/content/images/size/w600/2022/02/Screenshot-2022-02-13-at-13.46.56.png 600w, https://turkogluc.com/content/images/size/w1000/2022/02/Screenshot-2022-02-13-at-13.46.56.png 1000w, https://turkogluc.com/content/images/size/w1600/2022/02/Screenshot-2022-02-13-at-13.46.56.png 1600w, https://turkogluc.com/content/images/size/w2400/2022/02/Screenshot-2022-02-13-at-13.46.56.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>As mentioned before we can collect metrics from the databases i.e. <code>postgresql</code> and view the logs as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2022/02/metricbeat-postgresql-overview.png" class="kg-image" alt="Centralized Logging and Monitoring with Elastic Stack" loading="lazy" width="2000" height="1068" srcset="https://turkogluc.com/content/images/size/w600/2022/02/metricbeat-postgresql-overview.png 600w, https://turkogluc.com/content/images/size/w1000/2022/02/metricbeat-postgresql-overview.png 1000w, https://turkogluc.com/content/images/size/w1600/2022/02/metricbeat-postgresql-overview.png 1600w, https://turkogluc.com/content/images/size/w2400/2022/02/metricbeat-postgresql-overview.png 2400w" sizes="(min-width: 720px) 720px"></figure><h2 id="index-lifecycle-management">Index Lifecycle Management</h2><p>One of the nice features of elasticsearch in the new versions is the ILM (Index Lifecycle Management) which provides out of box feature to clean up documents from the indexes and storage. As far as I know, for the older versions, people had to run a separate process (curator) to clean old records. </p><p>At the moment Elasticsearch provides the following phases for ILM:</p><ul><li><strong>Hot</strong>: The index is actively being updated and queried.</li><li><strong>Warm</strong>: The index is no longer being updated but is still being queried.</li><li><strong>Cold</strong>: The index is no longer being updated and is queried infrequently. The information still needs to be searchable, but it&#x2019;s okay if those queries are slower.</li><li><strong>Frozen</strong>: The index is no longer being updated and is queried rarely. The information still needs to be searchable, but it&#x2019;s okay if those queries are extremely slow.</li><li><strong>Delete</strong>: The index is no longer needed and can safely be removed.</li></ul><p>ILM moves indices through the lifecycle according to their age. You can set how many days the records should stay in a phase and then it will go to the next phase and finally will be deleted (if configured so).</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2022/02/Screenshot-2022-02-13-at-14.16.44.png" class="kg-image" alt="Centralized Logging and Monitoring with Elastic Stack" loading="lazy" width="2000" height="1097" srcset="https://turkogluc.com/content/images/size/w600/2022/02/Screenshot-2022-02-13-at-14.16.44.png 600w, https://turkogluc.com/content/images/size/w1000/2022/02/Screenshot-2022-02-13-at-14.16.44.png 1000w, https://turkogluc.com/content/images/size/w1600/2022/02/Screenshot-2022-02-13-at-14.16.44.png 1600w, https://turkogluc.com/content/images/size/w2400/2022/02/Screenshot-2022-02-13-at-14.16.44.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>As we configured the metricbeat to collect the metrics per 10 second, it will pretty fast increase the disk size and collect GBs per day, in order to get rid of the older ones we can clean up via ILM policies. For further details please check the documentation:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-index-lifecycle.html?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Index lifecycle | Elasticsearch Guide [8.0] | Elastic</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.elastic.co/android-chrome-192x192.png" alt="Centralized Logging and Monitoring with Elastic Stack"><span class="kg-bookmark-author">Elastic</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.elastic.co/static/images/elastic-logo-200.png" alt="Centralized Logging and Monitoring with Elastic Stack"></div></a></figure><h2 id="setting-alerts-for-metrics">Setting Alerts For Metrics</h2><p>To create an alert on metrics, we can go to <code>Stack Management</code> -&gt; <code>Alerts and Insights</code> -&gt; <code>Rules and Connectors</code> -&gt; <code>Create Rule</code>. There are multiple rule types we can use, but I would like to show <code>Metric Threshold</code> type. We need to set a <code>name</code> for the rule, an <code>interval</code> to define how often to check, as the <code>notify</code> config we can select <code>only on status change</code>. We need to define which metrics to collect, for example: <code>system.cpu.total.norm.pct</code> average above 80%. We can also group by a field.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2022/02/Screenshot-2022-02-13-at-14.26.33.png" class="kg-image" alt="Centralized Logging and Monitoring with Elastic Stack" loading="lazy" width="812" height="1796" srcset="https://turkogluc.com/content/images/size/w600/2022/02/Screenshot-2022-02-13-at-14.26.33.png 600w, https://turkogluc.com/content/images/2022/02/Screenshot-2022-02-13-at-14.26.33.png 812w" sizes="(min-width: 720px) 720px"></figure><p>The actions in the free version is limited to <code>Index</code> and <code>Server Log</code>. In the paid versions of subscriptions there are many more options as Email, Slack, Jira, Webhook etc. As I am using the free version for now, I will select the <code>Index</code> action, which will write alert details to an index. When we select the alert action we need to define which fields from the alert event we want to record as a json document.</p><pre><code class="language-json">{
    &quot;actionGroup&quot;: &quot;{{alert.actionGroup}}&quot;,
    &quot;actionGroupName&quot;:&quot;{{alert.actionGroupName}}&quot;,
    &quot;actionSubgroup&quot;:&quot;{{alert.actionSubgroup}}&quot;,
    &quot;alertId&quot;:&quot;{{alert.id}}&quot;,
    &quot;alertState&quot;:&quot;{{context.alertState}}&quot;,
    &quot;contextGroup&quot;:&quot;{{context.group}}&quot;,
    &quot;contextMetric&quot;:&quot;{{context.metric}}&quot;,
    &quot;contextReason&quot;:&quot;{{context.reason}}&quot;,
    &quot;contextThreshold&quot;:&quot;{{context.threshold}}&quot;,
    &quot;contextTimestamp&quot;:&quot;{{context.timestamp}}&quot;,
    &quot;contextValue&quot;:&quot;{{context.value}}&quot;,
    &quot;date&quot;:&quot;{{date}}&quot;,
    &quot;kibanaBaseUrl&quot;:&quot;{{kibanaBaseUrl}}&quot;,
    &quot;ruleId&quot;:&quot;{{rule.id}}&quot;,
    &quot;ruleName&quot;:&quot;{{rule.name}}&quot;,
    &quot;ruleSpaceId&quot;:&quot;{{rule.spaceId}}&quot;,
    &quot;ruleTags&quot;:&quot;{{rule.tags}}&quot;,
    &quot;ruleType&quot;:&quot;{{rule.type}}&quot;
}</code></pre><p>We created new index to write the alert and specified that with a connector as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2022/02/Screenshot-2022-02-13-at-14.31.37.png" class="kg-image" alt="Centralized Logging and Monitoring with Elastic Stack" loading="lazy" width="1222" height="1416" srcset="https://turkogluc.com/content/images/size/w600/2022/02/Screenshot-2022-02-13-at-14.31.37.png 600w, https://turkogluc.com/content/images/size/w1000/2022/02/Screenshot-2022-02-13-at-14.31.37.png 1000w, https://turkogluc.com/content/images/2022/02/Screenshot-2022-02-13-at-14.31.37.png 1222w" sizes="(min-width: 720px) 720px"></figure><p>If you check the alert-logs index we can see that triggered alerts are saved to this index.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2022/02/Screenshot-2022-02-13-at-14.34.00.png" class="kg-image" alt="Centralized Logging and Monitoring with Elastic Stack" loading="lazy" width="2000" height="1018" srcset="https://turkogluc.com/content/images/size/w600/2022/02/Screenshot-2022-02-13-at-14.34.00.png 600w, https://turkogluc.com/content/images/size/w1000/2022/02/Screenshot-2022-02-13-at-14.34.00.png 1000w, https://turkogluc.com/content/images/size/w1600/2022/02/Screenshot-2022-02-13-at-14.34.00.png 1600w, https://turkogluc.com/content/images/size/w2400/2022/02/Screenshot-2022-02-13-at-14.34.00.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>If you are using free license and want to receive email alerts, you can use the following tool that I was preparing to trigger email alerts. </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/turkogluc/elastic-email-alerts?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - turkogluc/elastic-email-alerts: Elasticsearch alerts email action</div><div class="kg-bookmark-description">Elasticsearch alerts email action. Contribute to turkogluc/elastic-email-alerts development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicons/favicon.svg" alt="Centralized Logging and Monitoring with Elastic Stack"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">turkogluc</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/fda4933ba1cc5f3db5ef30d7c12d38985833e1836d0c61f0f210232211020f6c/turkogluc/elastic-email-alerts" alt="Centralized Logging and Monitoring with Elastic Stack"></div></a></figure><p>There are few environment variables to configure and run it. It is very light-weighted tool and can be used to send email alerts from indexes. An example email looks like as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2022/02/email-example.png" class="kg-image" alt="Centralized Logging and Monitoring with Elastic Stack" loading="lazy" width="2000" height="1556" srcset="https://turkogluc.com/content/images/size/w600/2022/02/email-example.png 600w, https://turkogluc.com/content/images/size/w1000/2022/02/email-example.png 1000w, https://turkogluc.com/content/images/size/w1600/2022/02/email-example.png 1600w, https://turkogluc.com/content/images/2022/02/email-example.png 2196w" sizes="(min-width: 720px) 720px"></figure><h2 id="collecting-logs">Collecting Logs</h2><p><code>Metricbeat</code> is used to collect the metrics, and to collect the logs we can use <code>filebeat</code> as follows:</p><pre><code class="language-json">version: &apos;3.8&apos;
services:

  filebeat:
    image: docker.elastic.co/beats/filebeat:7.17.0
    user: root
    volumes:
      - ./filebeat.docker.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/log/custom:/var/log/custom:ro</code></pre><p>We can provide the configuration <code>filebeat.docker.yml</code> as follows:</p><pre><code class="language-yml">setup.dashboards.enabled: true
setup.dashboards.beat: filebeat

#=========================== Filebeat inputs =============================

filebeat.inputs:

- type: log
  enabled: true
  paths:
    - /var/log/custom/*.json

  json.keys_under_root: true
  json.overwrite_keys: true
  json.add_error_key: true
  json.expand_keys: true

  # Decode JSON options. Enable this if your logs are structured in JSON.
  # JSON key on which to apply the line filtering and multiline settings. This key
  # must be top level and its value must be string, otherwise it is ignored. If
  # no text key is defined, the line filtering and multiline features cannot be used.
  #json.message_key:

  # By default, the decoded JSON is placed under a &quot;json&quot; key in the output document.
  # If you enable this setting, the keys are copied top level in the output document.
  #json.keys_under_root: false

  # If keys_under_root and this setting are enabled, then the values from the decoded
  # JSON object overwrite the fields that Filebeat normally adds (type, source, offset, etc.)
  # in case of conflicts.
  #json.overwrite_keys: false

  # If this setting is enabled, then keys in the decoded JSON object will be recursively
  # de-dotted, and expanded into a hierarchical object structure.
  # For example, `{&quot;a.b.c&quot;: 123}` would be expanded into `{&quot;a&quot;:{&quot;b&quot;:{&quot;c&quot;:123}}}`.
  #json.expand_keys: false

  # If this setting is enabled, Filebeat adds a &quot;error.message&quot; and &quot;error.key: json&quot; key in case of JSON
  # unmarshaling errors or when a text key is defined in the configuration but cannot
  # be used.
  #json.add_error_key: false


# ---------------------------- Elasticsearch Output ----------------------------
output.elasticsearch:
  enabled: true
  hosts: [&quot;localhost:9200&quot;]
  username: &quot;elastic&quot;
  password: &quot;somethingsecret&quot;

setup.kibana:
  host: &quot;localhost:5601&quot;
  username: &quot;elastic&quot;
  password: &quot;somethingsecret&quot;</code></pre><p>We can run this on every host that we want to collect the logs and it will ship the logs. Note that logs are collected from <code>/var/log/custom</code> folder and only json files. The applications should write the logs within this folder.</p><h2 id="application-level-logging-for-java-application">Application Level Logging for Java Application</h2><p>For the spring or java applications we can configure the application logging compatible with log collectors and elasticsearch with <code>ecs-logging</code>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.elastic.co/guide/en/ecs-logging/java/current/setup.html?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Get started | ECS Logging Java Reference [1.x] | Elastic</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.elastic.co/android-chrome-192x192.png" alt="Centralized Logging and Monitoring with Elastic Stack"><span class="kg-bookmark-author">Elastic</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.elastic.co/static/images/elastic-logo-200.png" alt="Centralized Logging and Monitoring with Elastic Stack"></div></a></figure><p>We need to add the following dependency:</p><pre><code>implementation &apos;co.elastic.logging:logback-ecs-encoder:1.3.2&apos;</code></pre><p>And add the following <code>logback.xml</code> configuration:</p><pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;configuration&gt;
    &lt;property name=&quot;LOG_FILE&quot; value=&quot;${LOG_FILE:-spring.log}&quot;/&gt;
    &lt;include resource=&quot;org/springframework/boot/logging/logback/defaults.xml&quot;/&gt;
    &lt;include resource=&quot;org/springframework/boot/logging/logback/console-appender.xml&quot; /&gt;
    &lt;include resource=&quot;org/springframework/boot/logging/logback/file-appender.xml&quot; /&gt;
    &lt;include resource=&quot;co/elastic/logging/logback/boot/ecs-file-appender.xml&quot; /&gt;
    &lt;root level=&quot;INFO&quot;&gt;
        &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt;
        &lt;appender-ref ref=&quot;ECS_JSON_FILE&quot;/&gt;
        &lt;appender-ref ref=&quot;FILE&quot;/&gt;
    &lt;/root&gt;
&lt;/configuration&gt;
</code></pre><p>The <code>LOG_FILE</code> environment variable can be used to specify the path and file name. For example <code>export $LOG_FILE=/var/log/custom/app.log</code> and then there will be <code>app.log.json</code> file will be created and written as well. Therefore <code>filebeat</code> can collect the logs from the specific folder.</p>]]></content:encoded></item><item><title><![CDATA[SQL Window Functions Introduction]]></title><description><![CDATA[Understanding the window function in Postgres SQL. Introduction to partition by, order by, rank, first_value, last_value and aggregate functions]]></description><link>https://turkogluc.com/sql-window-functions/</link><guid isPermaLink="false">646f24c59b6311000195da65</guid><category><![CDATA[PostgreSQL]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Sat, 23 Oct 2021 11:45:20 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1586227740560-8cf2732c1531?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wxfDF8YWxsfDF8fHx8fHwyfHwxNjM0OTg5MzQ0&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1586227740560-8cf2732c1531?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wxfDF8YWxsfDF8fHx8fHwyfHwxNjM0OTg5MzQ0&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="SQL Window Functions Introduction"><p>Window functions perform a calculation over a set of rows that are connected in some way to the current row. It can be compared with <code>GROUP BY</code> and aggregate functions. However with window functions, rows keep their separate identities in the output instead of being grouped into a single output row. The window function is able access more than just the current row of the query result under the hood. Window definition has the following syntax:</p><pre><code class="language-sql">WINDOW window_name AS
(
	[ PARTITION BY expression [, ...] ]
	[ ORDER BY expression [ ASC | DESC ] [, ...] ]
)</code></pre><p>The <strong>ORDER</strong> and <strong>PARTITION</strong> define what is called the &quot;<strong>window</strong>&quot;, the ordered subset of data over which calculations are made.</p><h2 id="partition-by">PARTITION BY</h2><p>Let&apos;s focus on the partitioning concept. When we partition by a field, the table is divided to the partitions/groups and each row individually can access to the items from its own partition. For example:</p><pre><code class="language-sql">SELECT emp_no,
       department,
       salary,
       AVG(salary) OVER w
FROM emp_salary
    WINDOW w AS (PARTITION BY department)</code></pre><p>The aggregate function <code>AVG</code> is used <code>OVER</code> the window <code>w</code> and the result occurs to be:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-12.36.21.png" class="kg-image" alt="SQL Window Functions Introduction" loading="lazy" width="884" height="576" srcset="https://turkogluc.com/content/images/size/w600/2021/10/Screenshot-2021-10-23-at-12.36.21.png 600w, https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-12.36.21.png 884w" sizes="(min-width: 720px) 720px"><figcaption>partition by query result</figcaption></figure><p>Note that each color represents a partition.</p><p>The rows and the first 3 columns are the directly from the <code>emp_salary</code> table and we would see that exact part with <code>SELECT * FROM emp_salary</code> query, the last column comes with the aggregate function <code>AVG</code> that operates on the partition of the current row and calculates the average of salary of the partition.</p><blockquote>For each row, the window function is calculated over the rows that fall in the same partition as the current row.</blockquote><p>As you can see from the following explain analyse result, it is important to notice that the <code>PARTITION BY</code> statement first orders the table by the partitioned column:</p><pre><code>+--------------------------------------------------------------------------------------------------------------------+
|QUERY PLAN                                                                                                          |
+--------------------------------------------------------------------------------------------------------------------+
|WindowAgg  (cost=83.37..104.37 rows=1200 width=72) (actual time=0.589..0.879 rows=10 loops=1)                       |
|  -&gt;  Sort  (cost=83.37..86.37 rows=1200 width=40) (actual time=0.371..0.477 rows=10 loops=1)                       |
|        Sort Key: department                                                                                        |
|        Sort Method: quicksort  Memory: 25kB                                                                        |
|        -&gt;  Seq Scan on emp_salary  (cost=0.00..22.00 rows=1200 width=40) (actual time=0.037..0.129 rows=10 loops=1)|
|Planning Time: 0.068 ms                                                                                             |
|Execution Time: 1.053 ms                                                                                            |
+--------------------------------------------------------------------------------------------------------------------+
</code></pre><h2 id="order-by">ORDER BY</h2><p><code>ORDER BY</code> statement can be omitted as it is optional, however there is another important concept to understand the behaviour of using it. For each row, there is a set of rows within its partition called its <strong>window frame</strong>. </p><ul><li>When the <code>ORDER BY</code> is supplied then the frame consists of all rows from the start of the partition up through the current row. </li><li>When <code>ORDER BY</code> is omitted the default frame consists of all rows in the partition. </li></ul><p>If we compare result of the following query with the previous one we can see the <code>AVG</code> is calculated within the frame, which means from start to the current row:</p><pre><code class="language-sql">SELECT emp_no,
       department,
       salary,
       AVG(salary) OVER w
FROM emp_salary
    WINDOW w AS (PARTITION BY department ORDER BY salary)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-12.48.51.png" class="kg-image" alt="SQL Window Functions Introduction" loading="lazy" width="1684" height="942" srcset="https://turkogluc.com/content/images/size/w600/2021/10/Screenshot-2021-10-23-at-12.48.51.png 600w, https://turkogluc.com/content/images/size/w1000/2021/10/Screenshot-2021-10-23-at-12.48.51.png 1000w, https://turkogluc.com/content/images/size/w1600/2021/10/Screenshot-2021-10-23-at-12.48.51.png 1600w, https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-12.48.51.png 1684w" sizes="(min-width: 720px) 720px"><figcaption>order by query result</figcaption></figure><p>As we can see in the example <code>emp#5</code> has the average of the first 4 rows.</p><hr><h2 id="usual-aggregates-sum-count-and-avg">Usual Aggregates: SUM, COUNT, and AVG</h2><p>We can use the aggregate &#xA0;functions that are used normally without windows.</p><pre><code class="language-sql">SELECT emp_no,
       department,
       salary,
       AVG(salary) OVER w,
       SUM(salary) OVER w,
       COUNT(salary) OVER w
FROM emp_salary
    WINDOW w AS (PARTITION BY department ORDER BY salary)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-13.00.19.png" class="kg-image" alt="SQL Window Functions Introduction" loading="lazy" width="780" height="548" srcset="https://turkogluc.com/content/images/size/w600/2021/10/Screenshot-2021-10-23-at-13.00.19.png 600w, https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-13.00.19.png 780w" sizes="(min-width: 720px) 720px"><figcaption>sum, avg, count query result</figcaption></figure><p>Remember that the frame would be the whole partition if we did not use <code>ORDER BY</code> statement.</p><h2 id="rownumber">ROW_NUMBER</h2><p>As the name implies, it shows the number of the row within the partition, and it does not take any parameter.</p><pre><code class="language-sql">SELECT emp_no,
       department,
       salary,
       ROW_NUMBER() over w
FROM emp_salary
    WINDOW w AS (PARTITION BY department ORDER BY salary)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-13.05.38.png" class="kg-image" alt="SQL Window Functions Introduction" loading="lazy" width="600" height="556" srcset="https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-13.05.38.png 600w"><figcaption>row_number query result</figcaption></figure><h2 id="rank-and-denserank">RANK and DENSE_RANK</h2><p><code>Rank</code> is similar to the <code>row_number</code> however when 2 fields are having the same order (based on the order by clause) their rank is considered as the same, &#xA0;and the next rank is omitted, and +1 value is given for the upcoming row. &#xA0;<code>dense_rank</code> function does not skip the next rank and assigns it to the upcoming row.</p><pre><code class="language-sql">SELECT emp_no,
       department,
       salary,
       RANK() over w,
       DENSE_RANK() over w
FROM emp_salary
    WINDOW w AS (PARTITION BY department ORDER BY salary)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-13.13.30.png" class="kg-image" alt="SQL Window Functions Introduction" loading="lazy" width="764" height="554" srcset="https://turkogluc.com/content/images/size/w600/2021/10/Screenshot-2021-10-23-at-13.13.30.png 600w, https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-13.13.30.png 764w" sizes="(min-width: 720px) 720px"><figcaption>rank, dense_rank query result</figcaption></figure><h2 id="lag-and-lead">LAG and LEAD</h2><p>It can often be useful to compare rows with preceding or following rows, especially if you have the data in a meaningful order. <code>LAG</code> function can access data of the previous rows, and <code>LEAD</code> function can access the next rows relative to the current row. Both functions take offset as parameter to provide the number of rows to go backward of forward.</p><pre><code class="language-sql">SELECT emp_no,
       department,
       salary,
       LAG(salary, 1) over w as preceding_salary,
       LEAD(salary, 1) over w as following_salary
FROM emp_salary
    WINDOW w AS (PARTITION BY department ORDER BY salary ASC)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-13.23.47.png" class="kg-image" alt="SQL Window Functions Introduction" loading="lazy" width="808" height="552" srcset="https://turkogluc.com/content/images/size/w600/2021/10/Screenshot-2021-10-23-at-13.23.47.png 600w, https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-13.23.47.png 808w" sizes="(min-width: 720px) 720px"><figcaption>lag and lead query result</figcaption></figure><h2 id="firstvalue-and-last-value">FIRST_VALUE and LAST VALUE</h2><p>These functions return the first or last value of the window frame.</p><pre><code class="language-sql">SELECT emp_no,
       department,
       salary,
       first_value(salary) over w as smallest_salary,
       last_value(salary) over w  as biggest_salary
FROM emp_salary
    WINDOW w AS (PARTITION BY department ORDER BY salary ASC)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-13.23.47-1.png" class="kg-image" alt="SQL Window Functions Introduction" loading="lazy" width="808" height="552" srcset="https://turkogluc.com/content/images/size/w600/2021/10/Screenshot-2021-10-23-at-13.23.47-1.png 600w, https://turkogluc.com/content/images/2021/10/Screenshot-2021-10-23-at-13.23.47-1.png 808w" sizes="(min-width: 720px) 720px"><figcaption>first_value and last_value query result</figcaption></figure><hr><p>Please note as we can use the named windows, we could define it directly in the projection column as well:</p><pre><code class="language-sql">SELECT emp_no,
       department,
       salary,
       first_value(salary) over (PARTITION BY department ORDER BY salary) as smallest_salary,
       last_value(salary) over (PARTITION BY department ORDER BY salary)  as biggest_salary
FROM emp_salary</code></pre><h2 id="references">References</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.postgresql.org/docs/13/tutorial-window.html?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">3.5.&#xA0;Window Functions</div><div class="kg-bookmark-description">3.5.&amp;nbsp;Window Functions A window function performs a calculation across a set of table rows that are somehow related to the &#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.postgresql.org/favicon.ico" alt="SQL Window Functions Introduction"><span class="kg-bookmark-author">PostgreSQL Documentation</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.postgresql.org/media/img/about/press/elephant.png" alt="SQL Window Functions Introduction"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://mode.com/sql-tutorial/sql-window-functions/?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SQL Window Functions | Advanced SQL - Mode</div><div class="kg-bookmark-description">This lesson of the SQL tutorial for data analysis covers SQL windowing functions such as ROW_NUMBER(), NTILE, LAG, and LEAD.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://mode.com/resources/icons/icon-512x512.png?v=a44c4197f59d99da0fa553f91f81b9fd" alt="SQL Window Functions Introduction"><span class="kg-bookmark-author">Mode Resources</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://mode.com/resources/images/og-images/sql-facebook.png" alt="SQL Window Functions Introduction"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Server Sent Events with Spring Boot and ReactJS]]></title><description><![CDATA[Basics of the Server Sent Events.
Implementation examples for SSE with Spring boot MVC and ReactJs client.
Differences of Server-Sent Events vs. WebSockets]]></description><link>https://turkogluc.com/server-sent-events-with-spring-boot-and-reactjs/</link><guid isPermaLink="false">646f24c59b6311000195da64</guid><category><![CDATA[Spring]]></category><category><![CDATA[React]]></category><category><![CDATA[Event Handling]]></category><category><![CDATA[Java]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Sat, 16 Jan 2021 14:43:57 GMT</pubDate><media:content url="https://turkogluc.com/content/images/2021/01/Blank-diagram--1--1.png" medium="image"/><content:encoded><![CDATA[<img src="https://turkogluc.com/content/images/2021/01/Blank-diagram--1--1.png" alt="Server Sent Events with Spring Boot and ReactJS"><p>Server Sent Events (<strong>SSE</strong>) is an HTTP standart that provides the capability to servers to push streaming data to client. The flow is unidirectional from server to client and client receives updates when the server pushes some data. </p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2021/01/Blank-diagram--1-.png" class="kg-image" alt="Server Sent Events with Spring Boot and ReactJS" loading="lazy"></figure><p>SSE has an <a href="https://html.spec.whatwg.org/multipage/server-sent-events.html?ref=localhost#eventsource">EventSource</a> interface with a straightforward API in the client side:</p><pre><code class="language-javascript">var source = new EventSource(&apos;sse-endpoint-address&apos;);
source.onmessage = function (event) {
  console.log(event.data);
};</code></pre><p>The data sent is always decoded as UTF-8. The server sends the events in the <code><a href="https://html.spec.whatwg.org/multipage/iana.html?ref=localhost#text/event-stream">text/event-stream</a></code> MIME type, and the default event type is a <code>message</code> event. <code>onmessage</code> event handler captures these default messages:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2021/01/Screenshot-2021-01-16-at-12.07.43.png" class="kg-image" alt="Server Sent Events with Spring Boot and ReactJS" loading="lazy"></figure><p>The client API has 3 predefined event handlers:</p><ul><li><a href="https://developer.mozilla.org/en-US/docs/Web/API/EventSource/onopen?ref=localhost"><strong><code>EventSource.onopen</code></strong></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/API/EventSource/onmessage?ref=localhost"><strong><code>EventSource.onmessage</code></strong></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/API/EventSource/onerror?ref=localhost"><strong><code>EventSource.onerror</code></strong></a></li></ul><p>The server can also send custom event types and in that case client should register event listener for that event:</p><pre><code>event: add
data: 73857293

event: remove
data: 2153

event: add
data: 113411</code></pre><pre><code class="language-javascript">source.addEventListener(&apos;add&apos;, addHandler, false);
source.addEventListener(&apos;remove&apos;, removeHandler, false);</code></pre><h3 id="spring-mvc-server-sent-events">Spring MVC Server Sent Events</h3><p>Spring Boot provides a way to implement SSE by using Flux which is a reactive representation of a stream of events, however in this post I use Spring MVC which provides 3 important classes:</p><ul><li>ResponseBodyEmitter</li><li>SseEmitter</li><li>StreamingResponseBody</li></ul><p><code>ResponseBodyEmitter</code> is a parent class which handles async responses, and <code>SseEmitter</code> is a<strong><strong> </strong></strong>subclass of <code>ResponseBodyEmitter</code> and provides additional support for Server-Sent Events. Let us see some example implementations with <code>SseEmiter</code> in action.</p><h3 id="pushing-time-as-a-simple-message-event">Pushing Time As a Simple Message Event</h3><p>We can create a <strong>controller</strong> in the backend side as follows:</p><pre><code class="language-java">@RestController
public class Controller {

    private static final Logger LOGGER = LoggerFactory.getLogger(Controller.class);
    private final ExecutorService executor = Executors.newSingleThreadExecutor();

    @PostConstruct
    public void init() {
        Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; {
            executor.shutdown();
            try {
                executor.awaitTermination(1, TimeUnit.SECONDS);
            } catch (InterruptedException e) {
                LOGGER.error(e.toString());
            }
        }));
    }

    @GetMapping(&quot;/time&quot;)
    @CrossOrigin
    public SseEmitter streamDateTime() {

        SseEmitter sseEmitter = new SseEmitter(Long.MAX_VALUE);

        sseEmitter.onCompletion(() -&gt; LOGGER.info(&quot;SseEmitter is completed&quot;));

        sseEmitter.onTimeout(() -&gt; LOGGER.info(&quot;SseEmitter is timed out&quot;));

        sseEmitter.onError((ex) -&gt; LOGGER.info(&quot;SseEmitter got error:&quot;, ex));

        executor.execute(() -&gt; {
            for (int i = 0; i &lt; 15; i++) {
                try {
                    sseEmitter.send(LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;dd-MM-yyyy hh:mm:ss&quot;)));
                    sleep(1, sseEmitter);
                } catch (IOException e) {
                    e.printStackTrace();
                    sseEmitter.completeWithError(e);
                }
            }
            sseEmitter.complete();
        });

        LOGGER.info(&quot;Controller exits&quot;);
        return sseEmitter;
    }

    private void sleep(int seconds, SseEmitter sseEmitter) {
        try {
            Thread.sleep(seconds * 1000);
        } catch (InterruptedException e) {
            e.printStackTrace();
            sseEmitter.completeWithError(e);
        }
    }
}</code></pre><p>Note that <code>SseEmitter</code> instance is created and given to the thread pool to be used in async tasks, and also it is returned as the response to the REST call. Async task uses the <code>send</code> method to push data, and since only the data is being provided to the method, it pushes as default <code>message</code> event. So REST call immediately &#xA0;returns the emitter and the &quot;<strong>Controller exits</strong>&quot;, and whenever something is ready to push executor thread will do that.</p><p>At the client side, we can simply create a react project with <code>create-react-app</code> and use the <code>EventSource</code> interface to subscribe to an endpoint as follows:</p><pre><code class="language-javascript">function App() {

  const [listening, setListening] = useState(false);
  const [data, setData] = useState([]);
  let eventSource = undefined;

  useEffect(() =&gt; {
    if (!listening) {
      eventSource = new EventSource(&quot;http://localhost:8080/time&quot;);

      eventSource.onopen = (event) =&gt; {
        console.log(&quot;connection opened&quot;)
      }

      eventSource.onmessage = (event) =&gt; {
        console.log(&quot;result&quot;, event.data);
        setData(old =&gt; [...old, event.data])
      }

      eventSource.onerror = (event) =&gt; {
        console.log(event.target.readyState)
        if (event.target.readyState === EventSource.CLOSED) {
          console.log(&apos;eventsource closed (&apos; + event.target.readyState + &apos;)&apos;)
        }
        eventSource.close();
      }

      setListening(true);
    }

    return () =&gt; {
      eventSource.close();
      console.log(&quot;eventsource closed&quot;)
    }

  }, [])

  return (
    &lt;div className=&quot;App&quot;&gt;
      &lt;header className=&quot;App-header&quot;&gt;
        Received Data
        {data.map(d =&gt;
          &lt;span key={d}&gt;{d}&lt;/span&gt;
        )}
      &lt;/header&gt;
    &lt;/div&gt;
  );
}

export default App;</code></pre><p>So each received message is pushed to the data array and that array is displayed on the App page:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://turkogluc.com/content/images/2021/01/ss-vid2.gif" class="kg-image" alt="Server Sent Events with Spring Boot and ReactJS" loading="lazy"></figure><h3 id="pushing-custom-progress-event">Pushing Custom Progress Event</h3><p>This time we can use the <code>SseEventBuilder</code> to push a custom JSON data and give it an event name:</p><pre><code class="language-java">sseEmitter.send(SseEmitter.event().name(&quot;Progress&quot;).data(progress, MediaType.APPLICATION_JSON));</code></pre><p>We will push the instances of following class to represent the progress</p><pre><code class="language-java">@JsonInclude(JsonInclude.Include.NON_EMPTY)
public class ObservableProgress {
    private final int target;
    private final AtomicInteger value = new AtomicInteger(0);

    public ObservableProgress(int target) {
        this.target = target;
    }

    public ObservableProgress increment(int v){
        value.getAndAdd(v);
        return this;
    }

    public int getTarget() {
        return target;
    }

    public int getValue() {
        return value.get();
    }

    @Override
    public String toString() {
        return &quot;ObservableProgress{&quot; +
                &quot;target=&quot; + target +
                &quot;, value=&quot; + value +
                &apos;}&apos;;
    }
}</code></pre><p>In the controller I want to simulate handling a job with multiple steps and each step is an I/O Blocking operation. Any time we complete a step we push back to the client some progress points. It&apos;s implementation is as follows:</p><pre><code class="language-java">@RestController
public class Controller {

    private static final Logger LOGGER = LoggerFactory.getLogger(Controller.class);

    @GetMapping(&quot;/run&quot;)
    @CrossOrigin
    public SseEmitter doTheJob() {

        SseEmitter sseEmitter = new SseEmitter(Long.MAX_VALUE);

        sseEmitter.onCompletion(() -&gt; LOGGER.info(&quot;SseEmitter is completed&quot;));

        sseEmitter.onTimeout(() -&gt; LOGGER.info(&quot;SseEmitter is timed out&quot;));

        sseEmitter.onError((ex) -&gt; LOGGER.info(&quot;SseEmitter got error:&quot;, ex));

        ObservableProgress progress = new ObservableProgress(100);

        runAsync(() -&gt; {
            sleep(1, sseEmitter);
            pushProgress(sseEmitter, progress.increment(10));
        })
        .thenRunAsync(() -&gt; {
            sleep(1, sseEmitter);
            pushProgress(sseEmitter, progress.increment(20));
        })
        .thenRunAsync(() -&gt; {
            sleep(1, sseEmitter);
            pushProgress(sseEmitter, progress.increment(10));
        })
        .thenRunAsync(() -&gt; {
            sleep(1, sseEmitter);
            pushProgress(sseEmitter, progress.increment(20));
        })
        .thenRunAsync(() -&gt; {
            sleep(1, sseEmitter);
            pushProgress(sseEmitter, progress.increment(20));
        })
        .thenRunAsync(() -&gt; {
            sleep(1, sseEmitter);
            pushProgress(sseEmitter, progress.increment(20));
        })
        .thenRunAsync(sseEmitter::complete)
        .exceptionally(ex -&gt; {
            sseEmitter.completeWithError(ex);
            throw (CompletionException) ex;
        });

        LOGGER.info(&quot;Controller exits&quot;);
        return sseEmitter;
    }

    private void pushProgress(SseEmitter sseEmitter, ObservableProgress progress) {
        try {
            LOGGER.info(&quot;Pushing progress: {}&quot;, progress.toString());
            sseEmitter.send(SseEmitter.event().name(&quot;Progress&quot;).data(progress, MediaType.APPLICATION_JSON));
        } catch (IOException e) {
            LOGGER.error(&quot;An error occurred while emitting progress.&quot;, e);
        }
    }

    private void sleep(int seconds, SseEmitter sseEmitter) {
        try {
            Thread.sleep(seconds * 1000);
        } catch (InterruptedException e) {
            e.printStackTrace();
            sseEmitter.completeWithError(e);
        }
    }
}</code></pre><p>Note that I use <code>CompletableFuture</code> to simulate the handling async tasks.</p><p>In the client side we need to register a listener for our new custom event type:</p><pre><code class="language-javascript">eventSource.addEventListener(&quot;Progress&quot;, (event) =&gt; {
        const result = JSON.parse(event.data);
        console.log(&quot;received:&quot;, result);
        setData(result)
});</code></pre><p>So the <code>App.js</code> becomes as follows:</p><pre><code class="language-javascript">import React, {useEffect, useState} from &quot;react&quot;;
import {Card, Progress, Row} from &quot;antd&quot;;

function App() {

  const [listening, setListening] = useState(false);
  const [data, setData] = useState({value: 0, target: 100});
  let eventSource = undefined;

  useEffect(() =&gt; {
    if (!listening) {
      eventSource = new EventSource(&quot;http://localhost:8080/run&quot;);

      eventSource.addEventListener(&quot;Progress&quot;, (event) =&gt; {
        const result = JSON.parse(event.data);
        console.log(&quot;received:&quot;, result);
        setData(result)
      });

      eventSource.onerror = (event) =&gt; {
        console.log(event.target.readyState)
        if (event.target.readyState === EventSource.CLOSED) {
          console.log(&apos;SSE closed (&apos; + event.target.readyState + &apos;)&apos;)
        }
        eventSource.close();
      }

      eventSource.onopen = (event) =&gt; {
        console.log(&quot;connection opened&quot;)
      }
      setListening(true);
    }
    return () =&gt; {
      eventSource.close();
      console.log(&quot;event closed&quot;)
    }

  }, [])

  return (

    &lt;&gt;
      &lt;Card title=&quot;Progress Circle&quot;&gt;
        &lt;Row justify=&quot;center&quot;&gt;
          &lt;Progress type=&quot;circle&quot; percent={data.value / data.target * 100}/&gt;
        &lt;/Row&gt;
      &lt;/Card&gt;
      &lt;Card title=&quot;Progress Line&quot;&gt;
        &lt;Row justify=&quot;center&quot;&gt;
          &lt;Progress percent={data.value / data.target * 100} /&gt;
        &lt;/Row&gt;
      &lt;/Card&gt;
    &lt;/&gt;


  );
}</code></pre><p> Note that I use <code>Ant Design</code> Progress component to visualise the progress. </p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://turkogluc.com/content/images/2021/01/vid-rec-2.gif" class="kg-image" alt="Server Sent Events with Spring Boot and ReactJS" loading="lazy"></figure><h3 id="limitation-to-the-maximum-number-of-open-connections">Limitation to the maximum number of open connections</h3><p>Note that SSE has a drawback when <strong>not used over HTTP/2</strong>, a browser is not allowed to open more than 6 SSE connections for the same address (www.ex-adress.com). So in chrome and firefox we can maximum open 6 tabs to the same address which opens sse connection. When we are using over HTTP/2 there is no such low number limitation, by default up to 100 connections is allowed. </p><h3 id="server-sent-events-vs-websockets">Server-Sent Events vs. WebSockets</h3><p>Websockets and SSE (Server-Sent Events) are both capable of pushing data to browsers. </p><p>Websockets connections are bidirectional and can send data to the browser and receive data from the browser. The games, messaging apps, and the cases where you need near real-time updates in both directions are good examples for WebSocket usage.</p><p>SSE connections are unidirectional and can only push data to the browser. Stock tick data, pushing notifications, twitters updating timeline are good examples of an application that could benefit from SSE.</p><p>In practice since everything that can be done with SSE can also be done with Websockets and Websockets provides richer protocol. That&apos;s why WebSockets gets more attention and used more widely. However, it can be overkill for some types of applications, and the backend could be easier to implement with a protocol such as SSE.</p><p>References:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Using server-sent events - Web APIs | MDN</div><div class="kg-bookmark-description">Developing a web application that uses server-sent events is straightforward. You&#x2019;ll need a bit of code on the server to stream events to the front-end, but the client&#xA0;side code works almost&#xA0;identically to websockets in part of handling incoming events. This is one-way connection, so you can&#x2019;t send &#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://developer.mozilla.org/apple-touch-icon.png" alt="Server Sent Events with Spring Boot and ReactJS"><span class="kg-bookmark-author">MDN</span></div></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://stackoverflow.com/questions/5195452/websockets-vs-server-sent-events-eventsource/5326159?ref=localhost#5326159"><div class="kg-bookmark-content"><div class="kg-bookmark-title">WebSockets vs. Server-Sent events/EventSource</div><div class="kg-bookmark-description">Both WebSockets and Server-Sent Events are capable of pushing data to browsers. To me they seem to be competing technologies. What is the difference between them? When would you choose one over the...</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon.png?v=c78bd457575a" alt="Server Sent Events with Spring Boot and ReactJS"><span class="kg-bookmark-author">Stack Overflow</span><span class="kg-bookmark-publisher">Mads Mob&#xE6;k</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded" alt="Server Sent Events with Spring Boot and ReactJS"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.html5rocks.com/en/tutorials/eventsource/basics/?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Stream Updates with Server-Sent Events - HTML5 Rocks</div><div class="kg-bookmark-description">The EventSource API is designed for receiving push notifications from a server, removing the need for client-size XHR polling.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.html5rocks.com/static/images/identity/HTML5_Badge_64.png" alt="Server Sent Events with Spring Boot and ReactJS"><span class="kg-bookmark-author">HTML5 Rocks - A resource for open web HTML5 developers</span><span class="kg-bookmark-publisher">Eric Bidelman</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.html5rocks.com/static/images/profiles/ericbidelman.png" alt="Server Sent Events with Spring Boot and ReactJS"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Creating PDF Reports with iText 7 in Java]]></title><description><![CDATA[Creating and exporting pdf reports in java rest API using itext 7. Inserting image, table, page header, and logo. Learning Itext 7 basic java components.]]></description><link>https://turkogluc.com/java-creating-pdf-reports-with-itext/</link><guid isPermaLink="false">646f24c59b6311000195da63</guid><category><![CDATA[Java]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Fri, 27 Nov 2020 19:29:39 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1542216172-f356fdd22653?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1542216172-f356fdd22653?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Creating PDF Reports with iText 7 in Java"><p>I have been using <a href="https://react-pdf.org/?ref=localhost">React-pdf</a> for PDF report generation for quite sometime in one of the React project. It is a nice library for certain size of reports, as content is prepared as React components and styling becomes way easier. However if the page content is generated dynamically and there is no certainty in number of pages it might cause serious problems in the frontend side. I ended up getting my browser frozen or memory limit exceed errors. So for the huge reports I started looking for Java modules to handle it in the backend server, and it did not take me much to come across <a href="https://itextpdf.com/?ref=localhost">iText 7</a>.</p><p>First thing caught my attention was <code>html2pdf</code> module and I gave it a try. Html content can be provided as a string or a file.</p><pre><code class="language-java">public static void main(String[] args) throws FileNotFoundException {
    String html = &quot;&lt;h1&gt;Test&lt;/h1&gt;&quot; +
            &quot;&lt;p&gt;Hello World&lt;/p&gt;&quot;;

    String dest = &quot;hello.pdf&quot;;
    HtmlConverter.convertToPdf(html, new FileOutputStream(dest));
}</code></pre><p>Resulting PDF:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-27-at-16.59.17.png" class="kg-image" alt="Creating PDF Reports with iText 7 in Java" loading="lazy"></figure><p>If you have a certain design in Html or you would like to generate Html with <a href="thymeleaf.org">thymeleaf</a>, this module could be a good fit for your case. </p><p>In my scenario, each item is retrieved from database and inserted to the report, and its images are also placed. The length of the text in the fields and the number of images differs in each item. I decided to use iText core module and APIs to build up a report rather than generating html and converting to pdf. So let&apos;s have a look at the building blocks of core API.</p><h2 id="basic-components">Basic Components</h2><p>We can add the dependency as follows:</p><pre><code>implementation &apos;com.itextpdf:itext7-core:7.1.13&apos;</code></pre><h3 id="pdf-document">PDF Document</h3><p>PDF files are represented by <code>PdfDocument</code> class and it has a wrapper called <code>Document</code>. Instantiating <code>PdfDocument</code> class can be done by providing the <code>PdfReader</code>, or <code>PdfWriter</code> in the constructor. As we intent to write to file we can instantiate with a <code>PdfWriter</code> which wraps the <code>OutputStream</code> or destination <code>File</code>.</p><pre><code class="language-java">// Creating a PdfWriter
String dest = &quot;example.pdf&quot;;
PdfWriter writer = new PdfWriter(dest);

// Creating a PdfDocument
PdfDocument pdfDoc = new PdfDocument(writer);

// Adding a new page
pdfDoc.addNewPage();

// Creating a Document
Document document = new Document(pdfDoc);

// Closing the document
document.close();
System.out.println(&quot;PDF Created&quot;);</code></pre><p>We are going to add the components that we want to insert to the <code>Document</code> object by using the following method:</p><pre><code class="language-java">public Document add(IBlockElement element)</code></pre><h3 id="paragraph">Paragraph </h3><p><code>Paragraph</code> class is a container for textual information. It can be filled by sending string to constructor or added text by <code>add</code> method. It has many useful elements related to the text or the paragraph view.</p><pre><code class="language-java">String content = &quot;Lorem ipsum dolor sit amet...&quot;;
Paragraph paragraph = new Paragraph(content);
paragraph.setFontSize(14);
paragraph.setTextAlignment(TextAlignment.CENTER);
paragraph.setBorder(Border.NO_BORDER);
paragraph.setFirstLineIndent(20);
paragraph.setItalic();
paragraph.setBold();
paragraph.setBackgroundColor(new DeviceRgb(245, 245, 245));
paragraph.setMargin(10);
paragraph.setPaddingLeft(10);
paragraph.setPaddingRight(10);
paragraph.setWidth(1000);
paragraph.setHeight(100);
document.add(paragraph);

document.add(paragraph); // add second time</code></pre><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-27-at-17.58.14.png" class="kg-image" alt="Creating PDF Reports with iText 7 in Java" loading="lazy"></figure><h3 id="areabreak">AreaBreak</h3><p><code>AreaBreak</code> can be used when we would like to start some element at the beginning of the next page. So the remaining part of the current page will be left empty and following elements will start from the next page.</p><pre><code class="language-java">String content = &quot;Lorem ipsum dolor sit amet...&quot;;
document.add(new Paragraph(content));
document.add(new AreaBreak());
document.add(new Paragraph(&quot;This text will be located in the next pagee&quot;));</code></pre><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-27-at-17.40.37.png" class="kg-image" alt="Creating PDF Reports with iText 7 in Java" loading="lazy"></figure><h3 id="list">List</h3><p>We can insert a <code>List</code> as follows:</p><pre><code class="language-java">Paragraph paragraph = new Paragraph(&quot;Lorem ipsum dolor...&quot;);
document.add(paragraph);

List list = new List();
list.add(&quot;Java&quot;);
list.add(&quot;Go&quot;);
list.add(&quot;React&quot;);
list.add(&quot;Apache Kafka&quot;);
list.add(&quot;Jenkins&quot;);
list.add(&quot;Elastic Search&quot;);
document.add(list);</code></pre><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-27-at-18.04.39.png" class="kg-image" alt="Creating PDF Reports with iText 7 in Java" loading="lazy"></figure><h3 id="table">Table</h3><p><code>Table</code> class can be instantiated with providing either number of columns or array of column length. We can insert <code>Cell</code> objects to a table and a cell may contain any <code>IBlockElement</code> object.</p><pre><code class="language-java">float [] pointColumnWidths = {150F, 150F, 150F, 150F};
Table table = new Table(pointColumnWidths);
// Table table = new Table(4); init by number of columns

table.addCell(new Cell().add(new Paragraph(&quot;Id&quot;)));
table.addCell(new Cell().add(new Paragraph(&quot;Name&quot;)));
table.addCell(new Cell().add(new Paragraph(&quot;Location&quot;)));
table.addCell(new Cell().add(new Paragraph(&quot;Date&quot;)));

table.addCell(new Cell().add(new Paragraph(&quot;1000&quot;)));
table.addCell(new Cell().add(new Paragraph(&quot;Item-1&quot;)));
table.addCell(new Cell().add(new Paragraph(&quot;Istanbul&quot;)));
table.addCell(new Cell().add(new Paragraph(&quot;01/12/2020&quot;)));

table.addCell(new Cell().add(new Paragraph(&quot;1005&quot;)));
table.addCell(new Cell().add(new Paragraph(&quot;Item-2&quot;)));
table.addCell(new Cell().add(new Paragraph(&quot;Warsaw&quot;)));
table.addCell(new Cell().add(new Paragraph(&quot;05/12/2020&quot;)));</code></pre><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-27-at-18.10.44.png" class="kg-image" alt="Creating PDF Reports with iText 7 in Java" loading="lazy"></figure><p><code>Table</code> and <code>Cell</code> classes contains many methods related to styling such as borders, alignments, background options, margin/padding settings etc.</p><h3 id="image">Image</h3><p><code>Image</code> class is used to insert an image to pdf file. It might be created by a local file, remote url or stream. </p><pre><code class="language-java">String imFile = &quot;images/logo2.png&quot;;
ImageData data = ImageDataFactory.create(imFile);
Image image = new Image(data);
image.setPadding(20);
image.setMarginTop(20);
image.setWidth(200);
image.setMaxHeight(250);
image.setAutoScale(false);
document.add(image);</code></pre><p>ImageDataFactory class may create an image instance from local path or remote URL. <code>Image</code> class has many styling methods as other components.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-27-at-18.37.02.png" class="kg-image" alt="Creating PDF Reports with iText 7 in Java" loading="lazy"></figure><h2 id="exporting-reports-with-rest-apis">Exporting Reports With REST APIs</h2><p>As a solution to my scenario, first of all, I prepared some general styling for each report type so that I can use it for different entity types. I created an <code>AbstractPdfDocument</code> in order to re-use the common functionality, for example having title and logo at the top of each page, or some common functionality as inserting remote images, or tables. I achieved placing logo and title at the top of each page by registering an EventHandler to the Document class. So the components are listed as follows:</p><h3 id="imagedownloader">ImageDownloader</h3><p>As the images are going to be downloaded from remote URLs, I decided to create some <code>ImageDownloader</code> service to download images with a thread pool in parallel.</p><pre><code class="language-java">public class ImageDownloader {

    private static final Logger logger = LoggerFactory.getLogger(ImageDownloader.class);
    private final int NUMBER_OF_THREADS = 12;
    private final ExecutorService executorService = Executors.newFixedThreadPool(NUMBER_OF_THREADS);

    public ConcurrentHashMap &lt;String, Future &lt;Image&gt;&gt; downloadImagesInParallel(
            List &lt;String&gt; imageList, ConcurrentHashMap &lt;String, Future &lt;Image&gt;&gt; imageMap) {

        if (imageList != null &amp;&amp; imageList.size() &gt; 0) {
            imageList.forEach(image -&gt; startDownloadImageTask(imageMap, image));
        }
        return imageMap;
    }

    private void startDownloadImageTask(ConcurrentHashMap &lt;String, Future &lt;Image&gt;&gt; imageMap, String image) {
        Future &lt;Image&gt; imageFuture = executorService.submit(() -&gt; getImageObject(image));
        imageMap.put(image, imageFuture);
    }

    private Image getImageObject(String url) {
        try {
            return new Image(ImageDataFactory.create(url, false));
        } catch (MalformedURLException e) {
            logger.error(&quot;download image failed: {}&quot;, e.getMessage());
            return null;
        }
    }
}</code></pre><h3 id="tableheadereventhandler">TableHeaderEventHandler</h3><p>This event handler is used to add a table with logo and title at the top of each page and it is code is as follows:</p><pre><code class="language-java">class TableHeaderEventHandler implements IEventHandler {
    private Table table;
    private float tableHeight;
    private Document doc;

    public TableHeaderEventHandler(Document doc, String documentTitle) {
        this.doc = doc;
        // Calculate top margin to be sure that the table will fit the margin.
        initTable(documentTitle);

        TableRenderer renderer = (TableRenderer) table.createRendererSubTree();
        renderer.setParent(new DocumentRenderer(doc));

        // Simulate the positioning of the renderer to find out how much space the header table will occupy.
        LayoutResult result = renderer.layout(new LayoutContext(new LayoutArea(0, PageSize.A4)));
        tableHeight = result.getOccupiedArea().getBBox().getHeight();

        // set top margin
        float topMargin = 36 + getTableHeight();
        doc.setMargins(topMargin, 36, 36, 36);
    }

    @Override
    public void handleEvent(Event currentEvent) {
        PdfDocumentEvent docEvent = (PdfDocumentEvent) currentEvent;
        PdfDocument pdfDoc = docEvent.getDocument();
        PdfPage page = docEvent.getPage();
        PdfCanvas canvas = new PdfCanvas(page.newContentStreamBefore(), page.getResources(), pdfDoc);
        PageSize pageSize = pdfDoc.getDefaultPageSize();
        float coordX = pageSize.getX() + doc.getLeftMargin();
        float coordY = pageSize.getTop() - doc.getTopMargin();
        float width = pageSize.getWidth() - doc.getRightMargin() - doc.getLeftMargin();
        float height = getTableHeight();
        Rectangle rect = new Rectangle(coordX, coordY, width, height);

        new Canvas(canvas, rect)
                .add(table)
                .close();
    }

    public float getTableHeight() {
        return tableHeight;
    }

    private void initTable(String documentTitle) {
        table = new Table(new float[]{320F, 200F});
        table.useAllAvailableWidth();
        Cell title = new Cell();
        title.setBorder(Border.NO_BORDER);
        Paragraph movement_report = new Paragraph(documentTitle).setFontSize(17);
        title.add(movement_report);
        table.addCell(title);
        table.setMarginBottom(20);
        ImageData data = ImageDataFactory.create(&quot;images/rsz_logo10.png&quot;);
        Image img = new Image(data);
        img.setWidth(200);
        Cell logo = new Cell();
        logo.setBorder(Border.NO_BORDER);
        logo.add(img);
        table.addCell(logo);
    }
}</code></pre><h3 id="abstractpdfdocument">AbstractPdfDocument</h3><p><code>AbstractPdfDocument</code> contains the common functionality as adding header to pages, creation and destruction of <code>Document</code> object, and helper methods as inserting images, tables, titles etc.</p><pre><code class="language-java">public abstract class AbstractPdfDocument&lt;T&gt; {

    protected final int MAX_IMAGE_NUM = 4;
    protected final Color GRAY = new DeviceRgb(245, 245, 245);
    protected final Color GRAY_LINE = new DeviceRgb(212, 212, 212);
    protected final Color WHITE = new DeviceRgb(255, 255, 255);
    ConcurrentHashMap &lt;String, Future &lt;Image&gt;&gt; imageMap = new ConcurrentHashMap&lt;&gt;();

    private final ImageDownloader imageDownloader;
    protected final String documentTitle;

    AbstractPdfDocument(ImageDownloader imageDownloader, String documentTitle) {
        this.imageDownloader = imageDownloader;
        this.documentTitle = documentTitle;
    }

    public final byte[] generatePdf(List&lt;T&gt; data) {
        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
        PdfDocument pdfDocument = new PdfDocument(new PdfWriter(outputStream));
        Document document = new Document(pdfDocument);
        TableHeaderEventHandler handler = new TableHeaderEventHandler(document, documentTitle);
        pdfDocument.addEventHandler(PdfDocumentEvent.END_PAGE, handler);

        writeData(document, data);

        document.close();
        return outputStream.toByteArray();
    }

    public void startDownloadingImages(List &lt;String&gt; imageList) {
        if (imageList != null &amp;&amp; imageList.size() &gt; 0) {
            imageDownloader.downloadImagesInParallel(imageList, imageMap);
        }
    }

    protected void insertImageTable(Document document, List &lt;String&gt; imageList) {
        Table imageTable = new Table(4);

        imageList.forEach(image -&gt; {
            try {
                Image img = imageMap.get(image).get();
                imageTable.addCell(new Cell()
                        .setTextAlignment(TextAlignment.CENTER)
                        .setHorizontalAlignment(HorizontalAlignment.CENTER)
                        .setVerticalAlignment(VerticalAlignment.MIDDLE)
                        .setBorder(new SolidBorder(GRAY_LINE, 1))
                        .add(img.scaleToFit(114, 114))
                        .setPadding(7));
            } catch (InterruptedException | ExecutionException e) {
                e.printStackTrace();
            }
        });
        document.add(new Paragraph().setMarginTop(10));
        document.add(imageTable);
    }

    protected Table createTable(TableFields tableFields) {
        Table table = new Table(new float[]{220F, 300F});
        AtomicInteger rowCounter = new AtomicInteger(0);

        tableFields.fieldList.forEach(field -&gt;
                insertIfNotNull(field.displayName, field.value, table, rowCounter));
        return table;
    }

    protected Paragraph getBlockTitle(String title) {
        return new Paragraph(title)
                .setFontSize(13)
                .setBorderBottom(new SolidBorder(GRAY_LINE, 1))
                .setMarginTop(35);
    }

    protected void insertIfNotNull(String displayName, Object value, Table table, AtomicInteger rowCounter) {
        if (value != null) {
            Color color = rowCounter
                    .getAndIncrement() % 2 == 0 ?
                    GRAY :
                    WHITE;

            table.addCell(new Cell()
                    .setBorder(Border.NO_BORDER)
                    .setBackgroundColor(color)
                    .add(new Paragraph(displayName)));

            table.addCell(new Cell()
                    .setBorder(Border.NO_BORDER)
                    .setBackgroundColor(color)
                    .add(new Paragraph(String.valueOf(value))));
        }
    }

    protected static class TableField {
        public String displayName;
        public Object value;

        protected TableField(String displayName, Object value) {
            this.displayName = displayName;
            this.value = value;
        }

        @Override
        public boolean equals(Object o) {
            if (this == o) return true;
            if (!(o instanceof TableField)) return false;
            TableField that = (TableField) o;
            return Objects.equals(displayName, that.displayName) &amp;&amp;
                    Objects.equals(value, that.value);
        }

        @Override
        public int hashCode() {
            return Objects.hash(displayName, value);
        }
    }

    protected static class TableFields {
        private List &lt;TableField&gt; fieldList = new ArrayList &lt;&gt;();

        protected void add(String displayName, Object value) {
            fieldList.add(new TableField(displayName, value));
        }

        protected void add(TableField field) {
            fieldList.add(field);
        }
    }

    protected abstract void writeData(Document document, List&lt;T&gt; data);
}</code></pre><p>This class uses <code>Factory Method</code> design pattern with its <code>writeData</code> abstract method. The inheritors of this class has to implement this method to lead the insertion of actual content. For each entity the content is inserted differently, and by implementing this method the concrete classes should define how it should be inserted. And also, child classes might use the helper methods for common tasks.</p><h3 id="itempdfdocument">ItemPdfDocument</h3><p><code>ItemPdfDocument</code> is the concrete class which extends the <code>AbstractPdfDocument</code> and implements the <code>writeData</code> method. It handles inserting item details and after uses helper method <code>insertImageTable</code> of its parent class.</p><pre><code class="language-java">public class ItemPdfDocument extends AbstractPdfDocument&lt;Item&gt; {

    ItemPdfDocument(ImageDownloader imageDownloader) {
        super(imageDownloader, &quot;Item Report&quot;);
    }

    @Override
    protected void writeData(Document document, List &lt;Item&gt; items) {
        startDownloadingItemImages(items);
        items.forEach(item -&gt; {
            addItemFields(document, item);
            addItemImages(document, item);
        });

    }

    private void startDownloadingItemImages(List &lt;Item&gt; items) {
        if(items != null &amp;&amp; items.size() &gt; 0) {
            startDownloadingImages(
                    items.stream()
                            .map(Item::getImgList)
                            .flatMap(Collection::stream)
                            .collect(Collectors.toList())
            );
        }
    }

    private void addItemFields(Document document, Item item) {
        document.add(getBlockTitle(&quot;Report: &quot; + item.getId()).setMarginTop(0));

        TableFields movementFields = new TableFields();
        movementFields.add(&quot;Report Id&quot;, item.getId());
        movementFields.add(&quot;Item Name&quot;, item.getName());
        movementFields.add(&quot;Title&quot;, item.getTitle());
        movementFields.add(&quot;Description&quot;, item.getDescription());
        movementFields.add(&quot;Area&quot;, item.getArea());
        movementFields.add(&quot;Location&quot;, item.getLocation());
        Table movementDetails = createTable(movementFields);

        document.add(movementDetails);
    }

    private void addItemImages(Document document, Item item) {
        insertImageTable(document, item.getImgList());
        document.add(new Paragraph());
    }
}</code></pre><p>So introducing new reports for each entity type is easy, we can create a new concrete class and show how to insert the content in the <code>writeData</code> method.</p><p>An example report generated by this document looks like as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-27-at-20.14.53.png" class="kg-image" alt="Creating PDF Reports with iText 7 in Java" loading="lazy"></figure><h3 id="controller">Controller</h3><p>At the controller I create an endpoint to retrieve the generated pdf file. At the first attempt I returned the byte array from output stream however I realized that might cause corrupted PDF files at the client side. So I encode it with base64 and client should decode it before saving the file.</p><pre><code class="language-java">@PostMapping(&quot;/pdf&quot;)
public void exportPdf(@RequestParam(required = false) List&lt;Long&gt; idList, HttpServletResponse response) throws IOException {

    response.setHeader(&quot;Expires&quot;, &quot;0&quot;);
    response.setHeader(&quot;Cache-Control&quot;, &quot;must-revalidate, post-check=0, pre-check=0&quot;);
    response.setHeader(&quot;Pragma&quot;, &quot;public&quot;);
    response.setContentType(&quot;application/pdf&quot;);

    byte[] byteArray = itemService.exportPdf(idList);

    byte[] encodedBytes = Base64.getEncoder().encode(byteArray);
    response.setContentLength(encodedBytes.length);
    OutputStream os = response.getOutputStream();
    os.write(encodedBytes);
    os.flush();
    os.close();
}</code></pre>]]></content:encoded></item><item><title><![CDATA[Java Concurrency - Understanding the Executor Framework And Thread Pool Management]]></title><description><![CDATA[Understanding the Java Executor Framework, Executor Services, Thread Pools, Fork Join Pools, Thread Pool Executors, Scheduled Executors in Java Concurrency.]]></description><link>https://turkogluc.com/java-concurrency-executor-services/</link><guid isPermaLink="false">646f24c59b6311000195da60</guid><category><![CDATA[Java]]></category><category><![CDATA[Concurrency]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Mon, 02 Nov 2020 22:29:39 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1538935640135-844bd0bb1ca0?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1538935640135-844bd0bb1ca0?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Java Concurrency - Understanding the Executor Framework And Thread Pool Management"><p>In the previous post, I was writing about the <a href="https://turkogluc.com/java-concurrency-basics-of-threads/">Basics of Threads</a>. This post is going to focus on a higher-level abstraction of thread creation and management.</p><p>Concurrent programs generally run a large number of tasks. Creating thread on demand for each task is not a good approach in terms of performance and usage of the resource as the thread creation and threads itself is very expensive. There is also a limitation for the maximum number of threads a program can create, couple of thousands depending on your machine (this is going to be changed with Project Loom).</p><p>A call center is one of the good example given to illustrate parallelisation; you can have bounded number of customer representatives in the call center and if there will be more customer calling than your employees, customers wait in the queue until one representative will be available to take the next call. So, hiring a new representative on each call would not make sense.</p><p>Therefore it is a better idea to have a thread pool containing a number of threads that would execute the tasks we are sending. Thread pool may create the threads statically (at the time of the creation of the pool), or dynamically (on demand), but it should have a reasonable upper bound. If you like to see a simple thread pool implementation that is queuing the submitted tasks and using the threads from the pool to execute them please check the example in the <a href="https://turkogluc.com/java-concurrency-basics-of-threads/">previous post</a>. </p><p>Using the low-level Thread API is also hard and it requires very much attention each time we need to use it. Java Executor framework helps us in this manner by decoupling the creation and management of the Threads from the rest of the application.</p><p>In the following sections, I will try to explain consecutively, the <code>ExecutorService</code> interface and its methods, implementations of the <code>ExecutorService</code>, and using the factory methods of <code>Executors</code> to simplify creation of <code>ExecutorService</code>.</p><h2 id="the-executor-service">The Executor Service</h2><p>At the heart of the executor framework, there is the <code>Executor</code> interface which has the single <code>execute</code> method:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-01-at-22.10.39.png" class="kg-image" alt="Java Concurrency - Understanding the Executor Framework And Thread Pool Management" loading="lazy"></figure><p><code>ExecutorService</code> is the main interface extending the <code>Executor</code> and we are going to mostly interact with. It is an abstraction around a Thread Pool, and exposes the <code>submit</code> method that we use to send the tasks. It contains a number of threads in its pool depending its implementations which we will see in the following sections.</p><p>When we send the <code>Runnable</code> or <code>Callable</code> tasks by <code>submit</code> method, the threads from the pool are going to run them.</p><ul><li><strong>Runnable</strong>: So far we have mentioned only about runnable, which does not return anything or is not able to throw any exception.</li><li><strong>Callable</strong>: As similar to the Runnable, designed for classes whose<br>instances are potentially executed by another thread<em>,</em> &#xA0;but <em>returns a result</em> and <em>may throw exception</em>.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-01-at-23.28.44.png" class="kg-image" alt="Java Concurrency - Understanding the Executor Framework And Thread Pool Management" loading="lazy"><figcaption>Runnable vs Callable</figcaption></figure><p>Submit method returns a <code>Future</code> object that represents the result of an asynchronous computation. &#xA0;Future has methods to check if the task is complete, to wait for its completion, and to retrieve the result. Its <code>get</code> method returns the result but it is a <strong>blocking method</strong>, so we can postpone calling the get method as long as possible and do other operations. Once we need the result of the task, we call the get method and if the result is not ready the calling thread will be blocked and we need to wait the result. If we can not afford waiting for long time, we can call the method with a <strong>timeout</strong>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-01-at-22.55.06.png" class="kg-image" alt="Java Concurrency - Understanding the Executor Framework And Thread Pool Management" loading="lazy"><figcaption>Future interface</figcaption></figure><p>The <code>ExecutorService</code> also provides methods for sending collection of tasks all together. We can use <code>invokeAll</code> method to send multiple tasks, and it returns List of Futures. <code>InvokeAny</code> method can be used to run similar tasks, and it returns the fastest answer</p><p>Another important advantage executor service provides is that it has shutdown functionality to stop the pool and the threads. There is an important difference between <code>shutdown</code> and <code>shutdownNow</code> methods:</p><ul><li><strong>shutdown</strong>: Calling this method indicates that no new tasks will be accepted to the queue, and previously sent tasks are going to be waited to complete. Note that if the tasks are long running tasks (infinite loop) they will never complete.</li><li><strong>shutdownNow</strong>: This method interrupts all the active threads, stops the processing of new tasks from the queue and returns the list of those tasks that were waiting in the queue. </li></ul><p>Note that in order to stop the processing, we need to <em>handle the interruption</em> in our <code>Runnable/Callable</code> tasks. Otherwise <code>shutdownNow</code> will trigger interruption but no thread will show reaction to it, and it behaves same as <code>shutdown</code>.</p><p>We can also see the <code>ScheduledExecutorService</code> in the first diagram, as it extends the <code>ExecutorService</code> and provides methods to run scheduled tasks. It is an high-level abstraction for the <code>Timer</code>, and it is easier and better way to run periodic tasks.</p><h2 id="implementations-of-the-executorservice">Implementations of the ExecutorService</h2><p><code>ExecutorService</code> instances are mostly created by using <code>Executors</code> factory methods, and I will show it in the next section. <code>Executors</code> factory methods are easy way to generate Thread Pools, however before using that, I would like to show the important concrete classes that implements the <code>ExecutorService</code>, because the factory method internally retrieves one of these implementations, and I believe it is important to understand the internals. Knowing some of the concrete <code>ExecutorServices</code>, we can create custom pools in case we have specific needs.</p><p>If we look at the following diagram, we have <code>AbstractExecutorService</code> which provides default implementations of <code>submit</code>, <code>invokeAny</code> and <code>InvokeAll</code> methods of the <code>ExecutorService</code>. Concrete implementations overrides some of the implementation details.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-02-at-17.22.20.png" class="kg-image" alt="Java Concurrency - Understanding the Executor Framework And Thread Pool Management" loading="lazy"><figcaption>Implementations of Executor interface</figcaption></figure><h3 id="1-threadpoolexecutor">1- ThreadPoolExecutor</h3><p>The <code>ThreadPoolExecutor</code> is a one of the core implementation of the <code>ExecutorService</code> and it executes each submitted task using one of possibly several pooled threads. This class provides many adjustable parameters and extensibility for configuring and managing the pool. We can configure the following parameters in this class:</p><ul><li><strong>corePoolSize</strong>: minimum number of Threads in the pool.</li><li><strong>maximumPoolSize</strong>: it is self explanatory, the upper bound of pool size. By setting the <code>corePoolSize</code> and <code>maximumPoolSize</code> the same number, we simple create a fixed size pool.</li><li><strong>ThreadFactory</strong>: New threads are created by using a <code>ThreadFactory</code> which is by default <code>Executors#defaultThreadFactory</code> that creates threads to all be in the same <code>ThreadGroup</code>, with the same <code>priority</code> and <code>non-daemon</code> status. If you like to customise it, you can set a different <code>ThreadFactory</code>.</li><li><strong>keepAliveTime</strong>: When the pool has more than minumum number and the threads are idle, exceeding ones are terminated after the <code>keepAliveTime</code>.</li><li><strong>Queue</strong>: A <code>BlockingQueue</code> can be configured to keep the submitted tasks. Example queues and the queueing strategies are as follows:</li></ul><ol><li><code>SynchronousQueue</code>: It is provides a <strong>direct handoff strategy</strong>, which means that tasks are delivered directly to the workers without storing them in a queue. If no threads are available to take the received task, then a new thread will be constructed. If a maximumPoolSize is set and that limit is reached, task will be rejected.</li><li><code>LinkedBlockingQueue</code>: It provides <strong>unbounded queue strategy</strong> which means, using a queue without a predefined capacity. This will cause new tasks to wait in the queue when all corePoolSize threads are busy. So no more than corePoolSize threads will be created and the value of maximumPoolSize does not have any effect.</li><li><code>ArrayBlockingQueue</code>: It provides <strong>bounded queue strategy </strong>which means, using a queue with a predefined capacity. It has a limited space in its queue therefore there should be enough number of threads to consume the tasks rapidly. When the queue is not full tasks are added to the queue. When queue becomes full, and the number of threads are less than maximumPoolSize a new thread is created. Finally when number of threads reaches the limit, the task is rejected.</li></ol><p>We can configure parameters mentioned above at the construction time or later with the setter methods.</p><pre><code class="language-java">public ThreadPoolExecutor(int corePoolSize,
                          int maximumPoolSize,
                          long keepAliveTime,
                          TimeUnit unit,
                          BlockingQueue&lt;Runnable&gt; workQueue,
                          ThreadFactory threadFactory,
                          RejectedExecutionHandler handler)
</code></pre><p>Example of running multiple tasks by <code>ThreadPoolExecutor</code>:</p><pre><code class="language-java">public class Main {

    private static final int CORE_POOL_SIZE = 4;
    private static final int MAX_POOL_SIZE = 4;

    private static final AtomicInteger taskCounter = new AtomicInteger(0);
    private static final ThreadFactory threadFactory = (runnable) -&gt; new Thread(runnable,
        &quot;thread &quot; + taskCounter.incrementAndGet()); // name each thread

    public static void main(String[] args) throws InterruptedException {

        ThreadPoolExecutor pool = new ThreadPoolExecutor(CORE_POOL_SIZE,
            MAX_POOL_SIZE,
            0L, // No timeout.
            TimeUnit.MILLISECONDS,
            new LinkedBlockingQueue&lt;&gt;(),
            threadFactory);

        Collection&lt;Callable&lt;Long&gt;&gt; tasks = new ArrayList&lt;&gt;();
        for (int i = 0; i &lt; 10; i++) {
            int var = i;
            tasks.add(() -&gt; {
                System.out.println(&quot;[&quot; + Thread.currentThread().getName() + &quot;]&quot;
                    + &quot; running the task: &quot; + var);
                return Long.valueOf(var * var);
            });
        }

        List&lt;Future&lt;Long&gt;&gt; futures = pool.invokeAll(tasks);
        futures.forEach(longFuture -&gt; {
            try {
                Long result = longFuture.get();
                System.out.println(&quot;Result: &quot; + result);
            } catch (InterruptedException e) {
                e.printStackTrace();
            } catch (ExecutionException e) {
                e.printStackTrace();
            }
        });

        pool.shutdown();
        pool.awaitTermination(1, TimeUnit.SECONDS);
    }
}</code></pre><p>It would give the following result:</p><pre><code>[thread 4] running the task: 3
[thread 2] running the task: 1
[thread 3] running the task: 2
[thread 1] running the task: 0
[thread 4] running the task: 4
[thread 2] running the task: 5
[thread 1] running the task: 6
[thread 2] running the task: 7
[thread 1] running the task: 8
[thread 2] running the task: 9
Result: 0
Result: 1
Result: 4
Result: 9
Result: 16
Result: 25
Result: 36
Result: 49
Result: 64
Result: 81</code></pre><p>As you can see in the UML diagram, apart from its configuration, <code>ThreadPoolExecutor</code> class provides many more useful methods, related to the queue, tasks or threads. So it is obviously more verbose than having an Executor, or ExecutorService instance to interact with the pool.</p><h3 id="2-forkjoinpool">2- ForkJoinPool</h3><p>The fork/join framework is designed to recursively split a parallelizable task into smaller tasks and then combine the results of each subtask to produce the overall result. The way it works is quite different than the other <code>ExecutorServices</code>, as it is build upon an algorithm based on Divide and Conquer. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-02-at-20.29.37.png" class="kg-image" alt="Java Concurrency - Understanding the Executor Framework And Thread Pool Management" loading="lazy"><figcaption>ForkJoin Framework</figcaption></figure><p>As it can be seen in the above figure tasks should be divided into smaller tasks, each small task should be run separately and the results should be combined and merged. When the tasks does not have dependencies to each other and can be divided into smaller subtasks this framework can be used to process them in parallel.</p><p>ForkJoinPool contains a single <code>common queue</code> that contains the tasks that are sent form outside, and fixed number of worker threads. Each thread also contains a <code>deque</code> (Double ended queue). Once a thread takes a task from queue, the tasks is divided into smaller pieces and thread add the these smaller subtasks into its deque. The treads takes the subtasks from the front of the deque and processes. Therefore each thread contains the subtasks in its own queue. Once a thread does not have anymore a task in its deque, and also there are no more tasks waiting in the common queue, it starts to take tasks from the back of other threads deques. This behaviour is called <code>work stealing</code>. By this way, ForkJoin framework maximises the efficiency of each thread, decreases the competition between the threads to take task and provides improved parallelism.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-02-at-21.21.56.png" class="kg-image" alt="Java Concurrency - Understanding the Executor Framework And Thread Pool Management" loading="lazy"><figcaption>WorkStealing</figcaption></figure><p>In order to work with ForkJoin Framework API in Java, we need to create <code>RecursiveTask</code> instances which is used to define how to operate, divide etc. The API details is not going to be covered in this post.</p><p>ForkJoin Framework is used in many places in java, such as in <code>ExecutorServices</code> as we have seen, in <code>parallel streams</code>, in <code>CompletableFutures</code> and so on. Especially Streams API heavily uses ForkJoin pools, so it is important to understand how it works and how it is integrated with streams.</p><h3 id="3-scheduledthreadpoolexecutor">3- ScheduledThreadPoolExecutor</h3><p>This executor is used to run the task once or periodically multiple times in the future. It serves the similar purpose with <code>Timer</code> class, but it is higher level implementation. After creating an instance, we can simple use the one of the schedule methods:</p><pre><code class="language-java">public ScheduledFuture&lt;?&gt; scheduleAtFixedRate(Runnable command,
                                                  long initialDelay,
                                                  long period,
                                                  TimeUnit unit);</code></pre><p>An example logging task can be implemented as follows:</p><pre><code class="language-java">public static void main(String[] args) {
    Runnable runnable = () -&gt; {
        System.out.println(&quot;[&quot; + Thread.currentThread().getName() + &quot;]&quot;
            + &quot; running the scheduled task&quot;);
    };

    ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(1);
    executor.scheduleAtFixedRate(runnable, 1, 1, TimeUnit.SECONDS);

    Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; {
        executor.shutdown();
        try {
            executor.awaitTermination(1, TimeUnit.SECONDS);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }));
}</code></pre><h2 id="executors-factory-methods">Executors Factory Methods</h2><p>In the previous section we have seen the concrete implementations of <code>ExecutorServices</code>. It is good to know the way to create, configure and manage them. However, we can easily generate ExecutorServices by calling the static factory methods of the <code>Executors</code> class. The class provides the following methods to help creation of thread pools:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-02-at-22.58.48.png" class="kg-image" alt="Java Concurrency - Understanding the Executor Framework And Thread Pool Management" loading="lazy"></figure><p>So we can create most of the commonly used thread pools:</p><pre><code class="language-java">ExecutorService fixedPool = Executors.newFixedThreadPool(4);
ExecutorService workStealingPool = Executors.newWorkStealingPool();
ExecutorService singleThreadPool = Executors.newSingleThreadExecutor();
ExecutorService cachedThreadPool = Executors.newCachedThreadPool();
ScheduledExecutorService scheduledPool = Executors.newScheduledThreadPool(1);</code></pre><p>Executors class provides convenient and easy way to generate pools, and generally that&apos;s how we create them rather than using the <code>new</code> keyword and initiating the ExecutorService types. As mentioned before <code>Executors</code> class uses the concrete implementations of ExecutorService interface to generate these pools with reasonable default configurations. As a matter of fact, if we inline the method calls we can see the the details how they are being generated:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/11/Screenshot-2020-11-02-at-23.13.38-copy.png" class="kg-image" alt="Java Concurrency - Understanding the Executor Framework And Thread Pool Management" loading="lazy"></figure><h3 id="gracefully-closing-the-executor-services">Gracefully closing the Executor Services</h3><p>We need to remember to close the thread pools when we are done with them, in order to free the resources and also to not cause any problem. If we forget to close, the main process will not exit and hang as long as pool exists. If we need the pool for very specific purpose, we can immediately close them after using. However generally the common approach to have a the thread pools is keeping it in a Singleton class, so pools lives as long as the program is running, and does its job during its lifetime. In such cases we can add a shutdown hook in order to close the pool right before JVM exits.</p><pre><code class="language-java">Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; {
        executor.shutdown();
        try {
            executor.awaitTermination(1, TimeUnit.SECONDS);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
}));</code></pre>]]></content:encoded></item><item><title><![CDATA[Java Concurrency - Basics of Threads]]></title><description><![CDATA[Java concurrency basics of threads. Creating Threads, Stopping Threads, Cancelling Threads, ThreadLocal variable, Thread Groups, Thread Types, Daemon Thread]]></description><link>https://turkogluc.com/java-concurrency-basics-of-threads/</link><guid isPermaLink="false">646f24c59b6311000195da5f</guid><category><![CDATA[Java]]></category><category><![CDATA[Concurrency]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Fri, 30 Oct 2020 09:13:47 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1516996087931-5ae405802f9f?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1516996087931-5ae405802f9f?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Java Concurrency - Basics of Threads"><p>Java <code>Thread</code> objects allow us to run our code in separate threads. When an application starts JVM creates the initial thread named <code>main</code>. The main method is run on the main thread. Inside the application we can create new threads to execute other tasks in parallel with the main thread.</p><p>Java uses native operating system threads. So one java thread is mapped by one OS thread.</p><h3 id="creating-threads">Creating Threads</h3><p>The constructor of the <code>Thread</code> class takes a <code>Runnable</code> object. Runnable interface has an abstract <code>run</code> method which is called by <code>Thread#start()</code> method. It object can be instantiated by a lambda, anonymous class or a class which implements Runnable method. </p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-29-at-20.03.52.png" class="kg-image" alt="Java Concurrency - Basics of Threads" loading="lazy"></figure><p>Using lambdas are generally easier and more compact:</p><pre><code class="language-java">Thread thread = new Thread(() -&gt; {
    // content of run command
});
thread.start();</code></pre><p>Thread lives as long as the its run hook method has not returned. The scheduler can suspend and run the Thread many times. For a thread to execute forever, it needs an infinite loop that prevents it from returning. </p><p><code>Join</code> method allows one thread to wait for the completion of another. This is a simple form of barrier synchronisation.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-29-at-20.19.19.png" class="kg-image" alt="Java Concurrency - Basics of Threads" loading="lazy"></figure><h3 id="java-thread-types-user-and-daemon-threads">Java Thread Types: User and Daemon Threads</h3><p>When JVM start it contains a single User thread, named Main thread. The main difference between User and Daemon threads are what happens when they exit.</p><ul><li>A user thread continues its lifecycle even if the main thread exits.</li><li>However all Daemon threads terminates when all the user threads exits.</li><li>JVM itself exits when all the user threads has exited.</li></ul><p>Thread class contains boolean <code>daemon</code> field to specify whether the thread is daemon. It can be set at the time of creation by the constructor or by setter method.</p><pre><code class="language-java">Thread thread = new Thread(getRunnable());
thread.setDaemon(true);
thread.start();</code></pre><p>By default daemon field is false, so most of the Threads that we generate is a User Thread. Threads copy the <code>isDaemon</code> status of the parent threat if it is not specified. Java uses Daemon thread in some places such as <code>ForkJoinPool</code> and <code>Timer</code>. To illustrate we can use the following example:</p><pre><code class="language-java">public class Main {

    public static void main(String[] args) throws InterruptedException, ExecutionException {
//        runDeamonThread();
        runUserThread();
        System.out.println(getCurrentThreadName() + &quot; exits&quot;);
    }

    private static void runDeamonThread() throws ExecutionException, InterruptedException {
        ExecutorService executorService = Executors.newWorkStealingPool(10);
        executorService.execute(getRunnable());
    }

    private static void runUserThread() {
        Thread thread = new Thread(getRunnable());
        thread.start();
    }

    private static Runnable getRunnable() {
        return () -&gt; {
            for (int i = 0; i &lt;= 200; i++) {
                System.out.print(&quot;.&quot;);
                Thread.yield();
            }
            System.out.println(getCurrentThreadName() + &quot; exits. isDeamon: &quot; + isDaemon());
        };
    }

    private static boolean isDaemon() {
        return Thread.currentThread().isDaemon();
    }

    private static String getCurrentThreadName() {
        return Thread.currentThread().getName();
    }
}</code></pre><ul><li>When we invoke <code>runUserThread</code> method it show the following example output:</li></ul><pre><code>................................................
main exits
........................................................................................
Thread-0 exits. isDeamon: false</code></pre><ul><li>The second case is invoking the <code>runDeamonThread</code> which uses <code>ForkJoinPool</code> as an example of Daemon Threads. I could simply use <code>setDaemon(true)</code> method, but wanted to give an example usage. Output:</li></ul><pre><code>main exits</code></pre><p>So when the main method exits, all the user threads are terminated and JVM exits and kills all daemon threads, so that we did not even have a chance to see output from daemon threads.</p><h3 id="stopping-threads">Stopping Threads</h3><p>Compared to creating, stopping a thread is quite hard thing. Once thread starts running it diverges from the caller and it has it is own lifecycle anymore. It can either complete the task and exits or if it does a long running operation it can work forever. Java does not provides us a method (non-deprecated) to stop the thread voluntarily. </p><ol><li>A naive approach could be using a stop flag:</li></ol><pre><code class="language-java">volatile boolean isStopped = false;

public void test() {
    new Thread(() -&gt; {
        while (!isStopped) {
            System.out.print(&quot;.&quot;);
        }
        System.out.println(&quot;Child Exits&quot;);
    }).start();

    try {
        Thread.sleep(100);
    } catch (InterruptedException e) {
        e.printStackTrace();
    }
    isStopped = true;
    System.out.println(&quot;Main exits&quot;);
}</code></pre><p>Note that the flag is <code>volatile</code> in order to make its up-to-date value visible for both threads. However this approach fails if the thread is doing blocking operations such as <code>sleep</code>, <code>wait</code>, <code>join</code> or blocking I/O operations.</p><p>2. Another way to stop the tread is to use <code>interrupt()</code> method of the thread. </p><blockquote>An interrupt request to a thread is an indication that it should stop what it is doing and do something else. It is up to the programmer to decide exactly how a thread responds to an interrupt but it is very common for the tread to terminate.</blockquote><p>For the interrupt mechanism to work correctly, the interrupted thread must support its own interruption mechanism. There are 2 cases we can examine for interruption:</p><ul><li>Non Blocking and Long Running Tasks</li></ul><p>In this case calling the <code>thread.interrupt()</code> method will set the interrupt flag of the that thread but if the task itself does not check the status of the interrupted flag it will not have any impact. For example:</p><pre><code class="language-java">public void test() throws InterruptedException {
    Thread thread = new Thread(() -&gt; {
        System.out.println(&quot;Child Starts&quot;);
        while (true) {
            System.out.print(&quot;.&quot;);
        }
    });

    thread.start();
    thread.interrupt();

    thread.join();
    System.out.println(&quot;Main exits&quot;);
}</code></pre><p>In order for the thread to catch the interrupt, it should iteratively check the status of the interrupt flag so that it can understand if there are any pending interruption request and handle the request accordingly. </p><p>So we can check the flag in our while loop in if it is true we can return or break the loop. In the lambda expression it is not possible to throw an exception but in appropriate places we can throw <code>InterruptedException</code> as well.</p><pre><code class="language-java">public void test() throws InterruptedException {
    Thread thread = new Thread(() -&gt; {
        System.out.println(&quot;Child Starts&quot;);
        while (true) {
            if (Thread.interrupted()) {
                break;
            }
            System.out.print(&quot;.&quot;);
        }
        System.out.println(&quot;Child exits&quot;);
    });

    thread.start();
    thread.interrupt();

    thread.join();
    System.out.println(&quot;Main exits&quot;);
}</code></pre><p>Note the <code>Thread.interrupted()</code> method returns the value of the flag and clears it if it has been true. So if we want to keep the state of the Thread as interrupted for the upper level of stack, we can set it back with <code>Thread.currentThread().interrupt();</code> </p><ul><li>Blocking Tasks</li></ul><p>If a thread frequently calls the blocking methods such as <code>wait</code>, <code>join</code>, <code>sleep</code>, <code>blocking I/O</code> which are all run interruptively, these methods internally check if they have been interrupted and if so they automatically throw <code>InterruptedException</code>. This exception should be caught and handled in the appropriate context. The following example uses the interruption to break the loop in a blocking <code>sleep</code> operation:</p><pre><code class="language-java">public void test() throws InterruptedException {
    Thread thread = new Thread(() -&gt; {
        System.out.println(&quot;Child Starts&quot;);
        try {
            while (true) {
                Thread.sleep(10000);
            }
        } catch (InterruptedException e) {
            System.out.println(&quot;Thread interrupted: &quot; + e.getMessage());
        }
        System.out.println(&quot;Child Exits&quot;);
    });

    thread.start();
    thread.interrupt();

    thread.join();
    System.out.println(&quot;Main exits&quot;);
}</code></pre><p>There are patterns for dealing with Java <code>InterruptedException</code>:</p><ul><li>One approach is propagating the exception to the callers, so higher layer would be responsible.</li><li>Before re-throwing, we can do task specific clean up.</li><li>If it is not possible to re-throw, we can set the interrupted status to true again with <code>Thread.currentThread().interrupt()</code> to preserve the evidence if the higher layers want to check it.</li></ul><p>So as a conclusion if we want to implement cancellable tasks we need to periodically check the status of the interrupt status and handle the interruption in a way that thread will exit.</p><h3 id="thread-groups">Thread Groups</h3><p>In order to simplify thread management, multiple threads &#xA0;can be organised with <code>java.lang.ThreadGroup</code> objects that group related threads. Each Thread Group needs to have a parent group. In the hierarchy, there is the <code>Main</code> group which is the parent of the other groups or threads we create in the program. We can create <code>ThreadGroup</code> by calling its constructor with a parent group and/or name. To add the Threads in a group we need to specify the group in the Thread&apos;s constructor.</p><pre><code class="language-java">public void test() {
    ThreadGroup tg1 = new ThreadGroup(&quot;Thread-group-1&quot;);
    ThreadGroup tg2 = new ThreadGroup(tg1, &quot;Thread-group-2&quot;);

    Thread thread1 = new Thread(tg1,&quot;thread-1&quot;);
    Thread thread2 = new Thread(tg2,&quot;thread-2&quot;);
    Thread thread3 = new Thread(tg2,&quot;thread-3&quot;);

    thread1.start();
    thread2.start();
    thread3.start();

    Thread[] threads = new Thread[tg2.activeCount()];
    tg2.enumerate(threads);

    Arrays.asList(threads).forEach(t -&gt; System.out.println(t.getName()));
    tg1.list();
}</code></pre><p>We can iterate over the threads by calling the <code>enumerate</code> method, which fills the given array with the thread references of the group.</p><p>We can implement a Thread Pool by making use of Thread Groups:</p><pre><code class="language-java">public class ThreadPool {
    // Create a thread group field
    private final ThreadGroup group = new ThreadGroup(&quot;ThreadPoolGroup&quot;);
    // Create a LinkedList field containing Runnable
    private final List&lt;Runnable&gt; tasks = new LinkedList&lt;&gt;();

    public ThreadPool(int poolSize) {
        // create several Worker threads in the thread group
        for (int i = 0; i &lt; poolSize; i++) {
            var worker = new Worker(group, &quot;worker-&quot; + i);
            worker.start();
        }
    }

    private Runnable take() throws InterruptedException {
        synchronized (tasks) {
            // if the LinkedList is empty, we wait
            while (tasks.isEmpty()) tasks.wait();
            // remove the first job from the LinkedList and return it
            return tasks.remove(0);
        }
    }

    public void submit(Runnable job) {
        // Add the job to the LinkedList and notifyAll
        synchronized (tasks) {
            tasks.add(job);
            tasks.notifyAll();
        }
    }

    public int getRunQueueLength() {
        // return the length of the LinkedList
        // remember to also synchronize!
        synchronized (tasks) {
            return tasks.size();
        }
    }

    public void shutdown() {
        // this should stop all threads in the group
        group.interrupt();
    }

    private class Worker extends Thread {
        public Worker(ThreadGroup group, String name) {
            super(group, name);
        }

        public void run() {
            // we run in an infinite loop:
            while(true) {
                // remove the next job from the linked list using take()
                // we then call the run() method on the job
                try {
                    take().run();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                    break;
                }
            }
        }
    }
}</code></pre><h3 id="thread-local-variables">Thread Local Variables</h3><p>Java <code>ThreadLocal</code> class can be used to create variables whose value can be accessible by only the same thread. So, even if two threads are executing the same code, and the code has a reference to the same <code>ThreadLocal</code> variable, the two threads cannot see each other&apos;s <code>ThreadLocal</code> variables.</p><pre><code class="language-java">public class Main {

    public static class ThreadLocalStorage {

        private static final ThreadLocal&lt;String&gt; threadLocal = new ThreadLocal&lt;&gt;();

        public static void setName(String name) {
            threadLocal.set(name);
        }

        public static String getName() {
            return threadLocal.get();
        }
    }

    public static void main(String[] args) {

        ThreadLocalStorage.setName(&quot;Main thread&quot;);

        Runnable runnable = () -&gt; {
            ThreadLocalStorage.setName(getCurrentThreadName());
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
            System.out.println(&quot;Thread: [&quot; + getCurrentThreadName() + &quot;] &quot; +
                &quot;- value: [&quot; + ThreadLocalStorage.getName() + &quot;]&quot;);
        };

        Thread thread1 = new Thread(runnable);
        Thread thread2 = new Thread(runnable);

        thread1.start();
        thread2.start();

        System.out.println(&quot;Main exits&quot;);
    }

    private static String getCurrentThreadName() {
        return Thread.currentThread().getName();
    }
}</code></pre><p>If we run the code we can see that each thread has its own copy of the ThreadLocal object.</p><pre><code>Main exits
Thread: [Thread-0] - ThreadLocal value: [Thread-0]
Thread: [Thread-1] - ThreadLocal value: [Thread-1]</code></pre><p>Instead of each thread having its own value inside a <code>ThreadLocal</code>, the <code>InheritableThreadLocal</code> grants access to values to a thread and all child threads created by that thread.</p><h3 id="references">References</h3><ul><li><a href="https://www.ibm.com/developerworks/library/j-jtp05236/index.html?ref=localhost">IBM - Dealing with Interrupted Exception</a></li><li><a href="https://www.javaspecialists.eu/?ref=localhost">https://www.javaspecialists.eu/</a></li></ul>]]></content:encoded></item><item><title><![CDATA[Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-3)]]></title><description><![CDATA[Developing a Modern Admin Portal with React, Redux, and Ant Design. Step by step guide. Designing a Dashboard pages with bar, pie charts by using bizcharts.]]></description><link>https://turkogluc.com/developing-admin-portal-with-react-redux-and-ant-design-part-3/</link><guid isPermaLink="false">646f24c59b6311000195da5e</guid><category><![CDATA[React]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Sun, 18 Oct 2020 09:30:24 GMT</pubDate><media:content url="https://turkogluc.com/content/images/2020/10/reactjs-thumb-2.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://turkogluc.com/content/images/2020/10/reactjs-thumb-2.jpg" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-3)"><p>In the <a href="https://turkogluc.com/developing-react-admin-portal-with-redux-and-ant-design/">part-1</a>, I started with installing the React, dependencies and preparing the environment and showed &#xA0;the implementation of Main Layout of the Admin Panel. I have demonstrated routing in the react with the <code>SiderMenu</code> and <code>Content</code> in the Layout.</p><p>In the <a href="https://turkogluc.com/developing-admin-portal-with-react-redux-and-ant-design-part-2/">part-2</a>, I demonstrated using generic components with custom hooks, and created exemplary pages containing the tables in order to list the items, forms for create, update pages, and we designed the pages by invoking the reusable hooks.</p><p>In this part I would like show more visual components by designing the <code>Dashboard</code> page. For charting I use <a href="https://bizcharts.net/?ref=localhost">bizcharts</a>, that is a chart component library with wide variety of choice. Thanks to its <a href="https://bizcharts.net/product/BizCharts4/gallery?ref=localhost">Example Charts</a> gallery, it is easy to just copy the components and use in our pages. </p><h2 id="designing-the-dashboard">Designing the Dashboard</h2><p>We need to start with install the bizcharts library:</p><pre><code class="language-javascript">yarn add bizcharts</code></pre><h3 id="chartcard-component">ChartCard Component</h3><p>I would like have a Card component that will contain a summary info, and in the size of 1/4 of a row, so that I can add 4 of them in a row and display some tiny bit of information within. It is going to look like as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-18-at-10.57.12.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-3)" loading="lazy"></figure><p>The component code is as follows:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Card } from &apos;antd&apos;;
import &apos;./ChartCard.less&apos;;

function ChartCard(props) {
  const renderContent = () =&gt; {
    const {
      contentHeight,
      title,
      avatar,
      action,
      total,
      footer,
      children,
      loading,
    } = props;

    return (
      &lt;div className=&quot;chartCard&quot;&gt;
        &lt;div className=&quot;chartTop&quot;&gt;
          &lt;div className=&quot;avatar&quot;&gt;{avatar}&lt;/div&gt;
          &lt;div className=&quot;metaWrap&quot;&gt;
            &lt;div className=&quot;meta&quot;&gt;
              &lt;span className=&quot;title&quot;&gt;{title}&lt;/span&gt;
              &lt;span className=&quot;action&quot;&gt;{action}&lt;/span&gt;
            &lt;/div&gt;
            &lt;div className=&quot;total&quot;&gt;{total}&lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
        {children &amp;&amp; (
          &lt;div className=&quot;content&quot; style={{ height: contentHeight || &apos;auto&apos; }}&gt;
            &lt;div className=&quot;contentHeight&quot;&gt;{children}&lt;/div&gt;
          &lt;/div&gt;
        )}
        {footer &amp;&amp; &lt;div className=&quot;footer&quot;&gt;{footer}&lt;/div&gt;}
      &lt;/div&gt;
    );
  };
  return (
    &lt;Card loading={false} bodyStyle={{ padding: &apos;20px 24px 8px 24px&apos; }}&gt;
      {renderContent()}
    &lt;/Card&gt;
  );
}

export default ChartCard;
</code></pre><figcaption>ChartCard.js</figcaption></figure><p>And it has a styling file:</p><figure class="kg-card kg-code-card"><pre><code class="language-css">.chartCard {
  position: relative;
  .chartTop {
    position: relative;
    width: 100%;
    overflow: hidden;
  }
  .chartTopMargin {
    margin-bottom: 12px;
  }
  .chartTopHasMargin {
    margin-bottom: 20px;
  }
  .metaWrap {
    float: left;
  }
  .avatar {
    position: relative;
    top: 4px;
    float: left;
    margin-right: 20px;
    img {
      border-radius: 100%;
    }
  }
  .meta {
    height: 22px;
    color: fade(#000, 45%);
    font-size: 14px;
    line-height: 22px;
  }
  .action {
    position: absolute;
    top: 4px;
    right: 0;
    line-height: 1;
    cursor: pointer;
  }
  .total {
    height: 38px;
    margin-top: 4px;
    margin-bottom: 0;
    overflow: hidden;
    color: fade(#000, 85%);
    font-size: 30px;
    line-height: 38px;
    white-space: nowrap;
    text-overflow: ellipsis;
    word-break: break-all;
  }
  .content {
    position: relative;
    width: 100%;
    margin-bottom: 12px;
  }
  .contentFixed {
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
  }
  .footer {
    margin-top: 8px;
    padding-top: 9px;
    border-top: 1px solid hsv(0, 0, 91%);
    &amp; &gt; * {
      position: relative;
    }
  }
  .footerMargin {
    margin-top: 20px;
  }
  .trendText {
    margin-left: 8px;
    margin-right: 4px;
    color: fade(#000, 85%);
  }
  .boldText {
    color: fade(#000, 85%);
  }
}
.chartCard {
  position: relative;
  .chartTop {
    position: relative;
    width: 100%;
    overflow: hidden;
  }
  .chartTopMargin {
    margin-bottom: 12px;
  }
  .chartTopHasMargin {
    margin-bottom: 20px;
  }
  .metaWrap {
    float: left;
  }
  .avatar {
    position: relative;
    top: 4px;
    float: left;
    margin-right: 20px;
    img {
      border-radius: 100%;
    }
  }
  .meta {
    height: 22px;
    color: fade(#000, 45%);
    font-size: 14px;
    line-height: 22px;
  }
  .action {
    position: absolute;
    top: 4px;
    right: 0;
    line-height: 1;
    cursor: pointer;
  }
  .total {
    height: 38px;
    margin-top: 4px;
    margin-bottom: 0;
    overflow: hidden;
    color: fade(#000, 85%);
    font-size: 30px;
    line-height: 38px;
    white-space: nowrap;
    text-overflow: ellipsis;
    word-break: break-all;
  }
  .content {
    position: relative;
    width: 100%;
    margin-bottom: 12px;
  }
  .contentFixed {
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
  }
  .footer {
    margin-top: 8px;
    padding-top: 9px;
    border-top: 1px solid hsv(0, 0, 91%);
    &amp; &gt; * {
      position: relative;
    }
  }
  .footerMargin {
    margin-top: 20px;
  }
  .trendText {
    margin-left: 8px;
    margin-right: 4px;
    color: fade(#000, 85%);
  }
  .boldText {
    color: fade(#000, 85%);
  }
}
</code></pre><figcaption>ChartCard.less</figcaption></figure><h3 id="mini-charting-components-for-chartcard">Mini Charting Components for ChartCard</h3><p>I would like to add some bar/line charts within the <code>ChartCard</code> to display some visual summary info, but to be able to fit in such a small area, I need to use some styling and <code>autoHeight</code> method.</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;

function computeHeight(node) {
  const { style } = node;
  style.height = &apos;100%&apos;;
  const totalHeight = parseInt(`${getComputedStyle(node).height}`, 10);
  const padding =
    parseInt(`${getComputedStyle(node).paddingTop}`, 10) +
    parseInt(`${getComputedStyle(node).paddingBottom}`, 10);
  return totalHeight - padding;
}
function getAutoHeight(n) {
  if (!n) {
    return 0;
  }
  const node = n;
  let height = computeHeight(node);
  const { parentNode } = node;
  if (parentNode) {
    height = computeHeight(parentNode);
  }
  return height;
}
function autoHeight() {
  return WrappedComponent =&gt; {
    class AutoHeightComponent extends React.Component {
      constructor(props) {
        super(props);
        this.state = {
          computedHeight: 0,
        };
        this.root = undefined;
        this.handleRoot = node =&gt; {
          this.root = node;
        };
      }

      componentDidMount() {
        // eslint-disable-next-line react/prop-types
        const { height } = this.props;
        if (!height) {
          let h = getAutoHeight(this.root);
          this.setState({ computedHeight: h });
          if (h &lt; 1) {
            h = getAutoHeight(this.root);
            this.setState({ computedHeight: h });
          }
        }
      }

      render() {
        // eslint-disable-next-line react/prop-types
        const { height } = this.props;
        const { computedHeight } = this.state;
        const h = height || computedHeight;
        return (
          &lt;div ref={this.handleRoot}&gt;
            {/* eslint-disable-next-line react/jsx-props-no-spreading */}
            {h &gt; 0 &amp;&amp; &lt;WrappedComponent {...this.props} height={h} /&gt;}
          &lt;/div&gt;
        );
      }
    }
    return AutoHeightComponent;
  };
}

export default autoHeight;
</code></pre><figcaption>autoHeight.js</figcaption></figure><p>Some general styling:</p><figure class="kg-card kg-code-card"><pre><code class="language-css">.miniChart {
  position: relative;
  width: 100%;
  .chartContent {
    position: absolute;
    bottom: -28px;
    width: 100%;
    &gt; div {
      margin: 0 -5px;
      overflow: hidden;
    }
  }
  .chartLoading {
    position: absolute;
    top: 16px;
    left: 50%;
    margin-left: -7px;
  }
}
</code></pre><figcaption>chart.less</figcaption></figure><h3 id="miniarea-component">MiniArea Component</h3><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Axis, Chart, Geom, Tooltip } from &apos;bizcharts&apos;;
import &apos;./chart.less&apos;;
import autoHeight from &apos;./autoHeight&apos;;

function MiniArea(props) {
  const {
    height = 1,
    data = [],
    forceFit = true,
    color = &apos;rgba(24, 144, 255, 0.2)&apos;,
    borderColor = &apos;#1089ff&apos;,
    scale = { x: {}, y: {} },
    borderWidth = 2,
    line,
    xAxis,
    yAxis,
    animate = true,
  } = props;

  const padding = [36, 5, 30, 5];

  const scaleProps = {
    x: {
      type: &apos;cat&apos;,
      range: [0, 1],
      ...scale.x,
    },
    y: {
      min: 0,
      ...scale.y,
    },
  };

  const tooltip = [
    &apos;x*y&apos;,
    (x, y) =&gt; ({
      name: x,
      value: y,
    }),
  ];

  const chartHeight = height + 54;

  return (
    &lt;div className=&quot;miniChart&quot; style={{ height }}&gt;
      &lt;div className=&quot;chartContent&quot;&gt;
        {height &gt; 0 &amp;&amp; (
          &lt;Chart
            animate={animate}
            scale={scaleProps}
            height={chartHeight}
            forceFit={forceFit}
            data={data}
            padding={padding}
          &gt;
            &lt;Axis
              key=&quot;axis-x&quot;
              name=&quot;x&quot;
              label={null}
              line={null}
              tickLine={null}
              grid={null}
              {...xAxis}
            /&gt;
            &lt;Axis
              key=&quot;axis-y&quot;
              name=&quot;y&quot;
              label={null}
              line={null}
              tickLine={null}
              grid={null}
              {...yAxis}
            /&gt;
            &lt;Tooltip showTitle={false} crosshairs={false} /&gt;
            &lt;Geom
              type=&quot;area&quot;
              position=&quot;x*y&quot;
              color={color}
              tooltip={tooltip}
              shape=&quot;smooth&quot;
              style={{
                fillOpacity: 1,
              }}
            /&gt;
            {line ? (
              &lt;Geom
                type=&quot;line&quot;
                position=&quot;x*y&quot;
                shape=&quot;smooth&quot;
                color={borderColor}
                size={borderWidth}
                tooltip={false}
              /&gt;
            ) : (
              &lt;span style={{ display: &apos;none&apos; }} /&gt;
            )}
          &lt;/Chart&gt;
        )}
      &lt;/div&gt;
    &lt;/div&gt;
  );
}

export default autoHeight()(MiniArea);
</code></pre><figcaption>MiniArea.js</figcaption></figure><p>We will use <code>MiniArea</code> component in <code>ChartCard</code> and it looks like as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-18-at-11.26.43.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-3)" loading="lazy"></figure><h3 id="minibar-component">MiniBar Component</h3><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Chart, Interval, Interaction } from &apos;bizcharts&apos;;
import &apos;./chart.less&apos;;
import autoHeight from &apos;./autoHeight&apos;;

function MiniBar(props) {
  const data = [
    { year: &apos;1951 year&apos;, sales: 38 },
    { year: &apos;1952 year&apos;, sales: 52 },
    { year: &apos;1956 year&apos;, sales: 61 },
    { year: &apos;1957 year&apos;, sales: 45 },
    { year: &apos;1958 year&apos;, sales: 48 },
    { year: &apos;1959 year&apos;, sales: 38 },
    { year: &apos;1960 year&apos;, sales: 38 },
    { year: &apos;1962 year&apos;, sales: 38 },
    { year: &apos;1963 year&apos;, sales: 10 },
    { year: &apos;1965 year&apos;, sales: 90 },
    { year: &apos;1966 year&apos;, sales: 80 },
    { year: &apos;1967 year&apos;, sales: 20 },
    { year: &apos;1968 year&apos;, sales: 80 },
    { year: &apos;1970 year&apos;, sales: 50 },
  ];

  return (
    &lt;div style={{ paddingTop: &apos;20px&apos; }}&gt;
      &lt;Chart autoFit pure data={data}&gt;
        &lt;Interval position=&quot;year*sales&quot; /&gt;
        &lt;Interaction type=&quot;element-highlight&quot; /&gt;
        &lt;Interaction type=&quot;active-region&quot; /&gt;
      &lt;/Chart&gt;
    &lt;/div&gt;
  );
}

export default MiniBar;
</code></pre><figcaption>Minibar.js</figcaption></figure><p>We will use <code>MiniBar</code> component in <code>ChartCard</code> and it looks like as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-18-at-11.26.48.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-3)" loading="lazy"></figure><h3 id="adding-chartcards-to-dashboard">Adding ChartCards to Dashboard</h3><p>We can display similar information or visual charts in the ChartCards and we can display them in the Dashboard as follows:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Card, Col, Row, Layout, Tooltip } from &apos;antd&apos;;
import { InfoCircleFilled, CaretUpFilled } from &apos;@ant-design/icons&apos;;
import ChartCard from &apos;../../component/chart/ChartCard&apos;;
import MiniArea from &apos;../../component/chart/MiniArea&apos;;
import MiniBar from &apos;../../component/chart/MiniBar&apos;;
import MiniProgress from &apos;../../component/chart/MiniProgress&apos;;
import { movementSummary, visitSummary } from &apos;./Constants&apos;;
import ProductBarChart from &apos;../../component/chart/ProductBarChart&apos;;
import ProductPieChart from &apos;../../component/chart/ProductPieChart&apos;;

function Dashboard() {
  const topColResponsiveProps = {
    xs: 24,
    sm: 12,
    md: 12,
    lg: 12,
    xl: 6,
    style: { marginBottom: 24 },
  };

  return (
    &lt;&gt;
      &lt;Row gutter={24} type=&quot;flex&quot;&gt;
        &lt;Col {...topColResponsiveProps}&gt;
          &lt;ChartCard
            bordered={false}
            title=&quot;Total Items&quot;
            action={
              &lt;Tooltip title=&quot;Total number of items&quot;&gt;
                &lt;InfoCircleFilled /&gt;
              &lt;/Tooltip&gt;
            }
            loading={false}
            total={12}
            footer={
              &lt;&gt;
                &lt;span className=&quot;boldText&quot;&gt;{13}&lt;/span&gt; Items added in the last{&apos; &apos;}
                &lt;span className=&quot;boldText&quot;&gt;7&lt;/span&gt; days
              &lt;/&gt;
            }
            contentHeight={46}
          &gt;
            &lt;div style={{ position: &apos;absolute&apos;, bottom: 0, left: 0 }}&gt;
              Weekly Changes
              &lt;span className=&quot;trendText&quot;&gt;{14}%&lt;/span&gt;
              &lt;CaretUpFilled style={{ color: &apos;#52c41a&apos; }} /&gt;
            &lt;/div&gt;
          &lt;/ChartCard&gt;
        &lt;/Col&gt;
        &lt;Col {...topColResponsiveProps}&gt;
          &lt;ChartCard
            bordered={false}
            title=&quot;Portal Visits&quot;
            action={
              &lt;Tooltip title=&quot;Total number of active users in the last month.&quot;&gt;
                &lt;InfoCircleFilled /&gt;
              &lt;/Tooltip&gt;
            }
            loading={false}
            total={10}
            footer={
              &lt;&gt;
                &lt;span className=&quot;boldText&quot;&gt;{12}&lt;/span&gt; Average daily visits per
                day
              &lt;/&gt;
            }
            contentHeight={46}
          &gt;
            &lt;MiniArea color=&quot;#975FE4&quot; data={visitSummary} /&gt;
          &lt;/ChartCard&gt;
        &lt;/Col&gt;
        &lt;Col {...topColResponsiveProps}&gt;
          &lt;ChartCard
            bordered={false}
            title=&quot;Items Moved&quot;
            action={
              &lt;Tooltip title=&quot;Item movement in the last year.&quot;&gt;
                &lt;InfoCircleFilled /&gt;
              &lt;/Tooltip&gt;
            }
            loading={false}
            total={124}
            footer={
              &lt;&gt;
                &lt;span className=&quot;boldText&quot;&gt;{123}&lt;/span&gt; Items moved in the last
                month
              &lt;/&gt;
            }
            contentHeight={46}
          &gt;
            &lt;MiniBar data={movementSummary} /&gt;
          &lt;/ChartCard&gt;
        &lt;/Col&gt;
        &lt;Col {...topColResponsiveProps}&gt;
          &lt;ChartCard
            bordered={false}
            title=&quot;Item Returns&quot;
            action={
              &lt;Tooltip title=&quot;Percentage of returned items.&quot;&gt;
                &lt;InfoCircleFilled /&gt;
              &lt;/Tooltip&gt;
            }
            loading={false}
            total={10 + &apos; %&apos;}
            footer={
              &lt;&gt;
                &lt;span className=&quot;boldText&quot;&gt;{12}&lt;/span&gt; Items in the last year
              &lt;/&gt;
            }
            contentHeight={46}
          &gt;
            &lt;MiniProgress
              percent={10}
              strokeWidth={16}
              color=&quot;#13C2C2&quot;
              target={100}
            /&gt;
          &lt;/ChartCard&gt;
        &lt;/Col&gt;
      &lt;/Row&gt;
    &lt;/&gt;
  );
}

export default Dashboard;
</code></pre><figcaption>Dashboard.js</figcaption></figure><p>Now the Dashboard page looks like as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-18-at-11.16.17.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-3)" loading="lazy"></figure><h3 id="barchart-component">BarChart Component</h3><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Chart, Interval, Tooltip } from &apos;bizcharts&apos;;
import { Card } from &apos;antd&apos;;

const barData = [
  { x: &apos;W-1&apos;, y: 44 },
  { x: &apos;W-2&apos;, y: 201 },
  { x: &apos;W-3&apos;, y: 41 },
  { x: &apos;W-4&apos;, y: 197 },
  { x: &apos;W-5&apos;, y: 173 },
  { x: &apos;W-6&apos;, y: 184 },
  { x: &apos;W-7&apos;, y: 109 },
  { x: &apos;W-8&apos;, y: 55 },
  { x: &apos;W-9&apos;, y: 28 },
  { x: &apos;W-10&apos;, y: 153 },
  { x: &apos;W-11&apos;, y: 76 },
  { x: &apos;W-12&apos;, y: 27 },
];

function ProductBarChart() {
  return (
    &lt;Card bordered={false}&gt;
      &lt;Chart
        height={250}
        autoFit
        data={barData}
        interactions={[&apos;active-region&apos;]}
      &gt;
        &lt;Interval position=&quot;x*y&quot; /&gt;
        &lt;Tooltip shared /&gt;
      &lt;/Chart&gt;
    &lt;/Card&gt;
  );
}

export default ProductBarChart;</code></pre><figcaption>ProductBarChart.js</figcaption></figure><h3 id="piechart-component">PieChart Component</h3><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Interaction, PieChart } from &apos;bizcharts&apos;;
import { Card } from &apos;antd&apos;;

const pieData = [
  {
    type: &apos;home&apos;,
    value: 27,
  },
  {
    type: &apos;living&apos;,
    value: 25,
  },
  {
    type: &apos;accessories&apos;,
    value: 18,
  },
  {
    type: &apos;jewellery&apos;,
    value: 15,
  },
  {
    type: &apos;clothing&apos;,
    value: 10,
  },
  {
    type: &apos;handmade&apos;,
    value: 5,
  },
];

function ProductPieChart() {
  return (
    &lt;Card bordered={false}&gt;
      &lt;PieChart
        forceFit
        height={250}
        data={pieData}
        radius={0.8}
        angleField=&quot;value&quot;
        colorField=&quot;type&quot;
        label={{
          visible: true,
          type: &apos;outer&apos;,
          offset: 20,
          formatter: val =&gt; `${val}%`,
        }}
      &gt;
        &lt;Interaction type=&quot;element-single-selected&quot; /&gt;
      &lt;/PieChart&gt;
    &lt;/Card&gt;
  );
}

export default ProductPieChart;
</code></pre><figcaption>ProductPieChart.js</figcaption></figure><p>These components are just some example usage of chart components of Bizchart. We can display them in the dashboard with a 1/2 row size card. Right after the first row in the Dashboard page we can add these components as the second row:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">&lt;Row gutter={24} type=&quot;flex&quot;&gt;
  &lt;Col span={12}&gt;
    &lt;Card title=&quot;Weekly Sale Report&quot;&gt;
      &lt;ProductBarChart /&gt;
    &lt;/Card&gt;
  &lt;/Col&gt;
  &lt;Col span={12}&gt;
    &lt;Card title=&quot;Sale Summary&quot;&gt;
      &lt;ProductPieChart /&gt;
    &lt;/Card&gt;
  &lt;/Col&gt;
&lt;/Row&gt;</code></pre><figcaption>Dashboard.js</figcaption></figure><p>Now the Dashboard page looks like as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-17-at-23.47.59.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-3)" loading="lazy"></figure><p>See the commit for the changes: <a href="https://github.com/turkogluc/react-admin-portal/commit/22f27985acaa184e17178ddcdf02723e1ed11ad0?ref=localhost">22f2798</a>.</p>]]></content:encoded></item><item><title><![CDATA[Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-2)]]></title><description><![CDATA[Developing a Modern Admin Portal with React, Redux, and Ant Design. Step by Step Guide. Using generic custom hooks for tables, menus and forms. React Router]]></description><link>https://turkogluc.com/developing-admin-portal-with-react-redux-and-ant-design-part-2/</link><guid isPermaLink="false">646f24c59b6311000195da5d</guid><category><![CDATA[React]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Sun, 18 Oct 2020 09:30:16 GMT</pubDate><media:content url="https://turkogluc.com/content/images/2020/10/reactjs-thumb-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://turkogluc.com/content/images/2020/10/reactjs-thumb-1.jpg" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-2)"><p>In the <a href="https://turkogluc.com/developing-react-admin-portal-with-redux-and-ant-design/">part-1</a>, I demonstrated the implementation of Main Layout of the Admin Panel. In this part I will show some generic components that can be used in the content part of the layout, such as tables, forms, charts.</p><h3 id="header-component">Header Component</h3><p>We can define a generic component that can be used as the header above the table views. It contains a <code>search bar</code>, <code>add new</code> and <code>delete</code> buttons and looks like as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-17-at-20.36.13.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-2)" loading="lazy"></figure><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Button, Col, Divider, Input, Popconfirm, Row } from &apos;antd&apos;;
import {
  DeleteOutlined,
  PlusOutlined,
  QuestionCircleOutlined,
} from &apos;@ant-design/icons&apos;;
import { useHistory } from &apos;react-router-dom&apos;;

const { Search } = Input;

function Header({ addNewPath, hasSelected, handleSearch }) {
  const history = useHistory();

  const handleAddNew = () =&gt; {
    history.push(&apos;/&apos; + addNewPath);
  };

  return (
    &lt;&gt;
      &lt;Row&gt;
        &lt;Col&gt;
          &lt;Search
            placeholder=&quot;Search&quot;
            onSearch={handleSearch}
            allowClear
            style={{ float: &apos;left&apos;, width: 350 }}
          /&gt;
        &lt;/Col&gt;
        &lt;Col flex=&quot;auto&quot;&gt;
          &lt;Button
            icon={&lt;PlusOutlined /&gt;}
            type=&quot;primary&quot;
            style={{ float: &apos;right&apos; }}
            onClick={handleAddNew}
          &gt;
            Add New
          &lt;/Button&gt;

          &lt;Button
            icon={&lt;DeleteOutlined /&gt;}
            disabled={!hasSelected}
            style={{ float: &apos;right&apos;, marginRight: 12 }}
          &gt;
            &lt;Popconfirm
              title=&quot;Sure to delete?&quot;
              icon={&lt;QuestionCircleOutlined style={{ color: &apos;red&apos; }} /&gt;}
              onConfirm={() =&gt; {}}
            &gt;
              Delete
            &lt;/Popconfirm&gt;
          &lt;/Button&gt;
        &lt;/Col&gt;
      &lt;/Row&gt;
      &lt;Divider /&gt;
    &lt;/&gt;
  );
}

export default Header;</code></pre><figcaption>Header.js</figcaption></figure><p>Now we can add the Header component in the pages, for example in the product page:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React, { useState } from &apos;react&apos;;
import Header from &apos;../../component/Header&apos;;

function ShowProducts() {
  const [hasSelected, setHasSelected] = useState(false);

  return (
    &lt;&gt;
      &lt;Header addNewPath=&quot;add-product&quot; hasSelected={hasSelected} /&gt;
    &lt;/&gt;
  );
}

export default ShowProducts;</code></pre><figcaption>ShowProducts.js</figcaption></figure><p>See the commit for changes: <strong><a href="https://github.com/turkogluc/react-admin-portal/commit/5c6f738e5cae4f3f28c148ed411bc7fb7b43acbf?ref=localhost">5c6f738</a></strong></p><h3 id="datatable-component">DataTable Component</h3><p>Ant design has a <a href="https://ant.design/components/table/?ref=localhost">Table</a> component with a wide variety of features as selectable rows, pagination, rendering custom columns, handling user events on the table. However, using it in each page causes duplicate code. Therefore we can create our custom ho0k in order to use tables. So the table hook is as follows:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React, { useState } from &apos;react&apos;;
import { Table } from &apos;antd&apos;;
import useActionMenu from &apos;./ActionMenu&apos;;

const DEFAULT_PAGE_SIZE = 10;
const DEFAULT_PAGE_NUMBER = 0;

function useDataTable({ columns, dataSource, updateEntityPath }) {
  const [selectedRowKeys, setSelectedRowKeys] = useState([]);
  const [selectedRow, setSelectedRow] = useState(null);
  const [currentPage, setCurrentPage] = useState(DEFAULT_PAGE_NUMBER);
  const [pageSize, setPageSize] = useState(DEFAULT_PAGE_SIZE);
  const [actionColumnView] = useActionMenu({ selectedRow, updateEntityPath });

  const hasSelected = selectedRowKeys.length &gt; 0;

  const rowSelection = {
    selectedRowKeys,
    onChange: selected =&gt; {
      setSelectedRowKeys(selected);
    },
  };

  const updatedColumns = [
    ...columns,
    {
      title: &apos;Action&apos;,
      key: &apos;action&apos;,
      render: () =&gt; actionColumnView,
    },
  ];

  const handleSingleDelete = () =&gt; {
    console.log(&apos;handleSingleDelete, selected:&apos;, selectedRow);
  };

  const resetPagination = () =&gt; {
    setCurrentPage(DEFAULT_PAGE_NUMBER);
  };

  const handleTableChange = pagination =&gt; {
    console.log(&apos;pagination:&apos;, pagination);
    setCurrentPage(pagination.current - 1);
  };

  const DataTable = () =&gt; (
    &lt;Table
      rowKey={record =&gt; record.id}
      rowSelection={rowSelection}
      columns={updatedColumns}
      dataSource={dataSource.content}
      onRow={record =&gt; {
        return {
          onClick: () =&gt; {
            setSelectedRow(record);
          },
        };
      }}
      onChange={handleTableChange}
      pagination={{
        pageSize: DEFAULT_PAGE_SIZE,
        current: currentPage + 1,
        total: dataSource.totalElements,
        showTotal: (total, range) =&gt; {
          return `${range[0]}-${range[1]} of ${total} items`;
        },
      }}
    /&gt;
  );

  return {
    DataTable,
    hasSelected,
    selectedRow,
    selectedRowKeys,
    currentPage,
    pageSize,
    resetPagination,
  };
}

export default useDataTable;
</code></pre><figcaption>DataTable.js</figcaption></figure><p>It returns:</p><ul><li><code>DataTable</code> component that represents the table</li><li><code>hasSelected</code> boolean value for any columns selected</li><li><code>selectedRow</code> is the single row which is at last selected</li><li><code>selectedRowKeys</code> is the array containing multiple selected keys</li><li><code>currentPage</code>, <code>pageSize</code> and <code>resetPagination</code> for the pagination</li></ul><p>These values can be used in the component that is wrapping the table. We are also adding an action column at the end of column list to display <code>update</code> and <code>delete</code> actions. This view is also implemented as a custom hook in the <code>ActionMenu</code> file as follows:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Dropdown, Menu, Popconfirm } from &apos;antd&apos;;
import {
  DeleteOutlined,
  DownOutlined,
  EditOutlined,
  QuestionCircleOutlined,
} from &apos;@ant-design/icons&apos;;
import { useHistory } from &apos;react-router-dom&apos;;

function useActionMenu({ selectedRow, updateEntityPath }) {
  const history = useHistory();

  const handleMenuClick = (action) =&gt; {
    if (action.key === &apos;edit&apos;) {
      const updatePath = &apos;/&apos; + updateEntityPath + &apos;/&apos; + selectedRow.id;
      history.push(updatePath);
    }
  };

  const handleSingleDelete = () =&gt; {
    console.log(&apos;handleSingleDelete, selected:&apos;, selectedRow);
  };

  const actionMenu = (
    &lt;Menu onClick={handleMenuClick}&gt;
      &lt;Menu.Item key=&quot;edit&quot;&gt;
        &lt;EditOutlined /&gt;
        Update
      &lt;/Menu.Item&gt;
      &lt;Menu.Item key=&quot;delete&quot;&gt;
        &lt;Popconfirm
          title=&quot;Sure to delete?&quot;
          placement=&quot;left&quot;
          icon={&lt;QuestionCircleOutlined style={{ color: &apos;red&apos; }} /&gt;}
          onConfirm={handleSingleDelete}
        &gt;
          &lt;DeleteOutlined type=&quot;delete&quot; /&gt;
          Delete
        &lt;/Popconfirm&gt;
      &lt;/Menu.Item&gt;
    &lt;/Menu&gt;
  );

  const actionColumnView = (
    &lt;span&gt;
      &lt;Dropdown overlay={actionMenu} trigger={[&apos;click&apos;]}&gt;
        &lt;a className=&quot;ant-dropdown-link&quot; href=&quot;#&quot;&gt;
          Actions &lt;DownOutlined /&gt;
        &lt;/a&gt;
      &lt;/Dropdown&gt;
    &lt;/span&gt;
  );

  return [actionColumnView];
}

export default useActionMenu;
</code></pre><figcaption>ActionMenu.js</figcaption></figure><p>Now we can use the table hook in the pages for example in product page:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import Header from &apos;../../component/Header&apos;;
import useDataTable from &apos;../../component/DataTable&apos;;
import * as constants from &apos;./Constants&apos;;

function ShowProducts() {
  const {
    DataTable,
    hasSelected,
    currentPage,
    pageSize,
    resetPagination,
  } = useDataTable({
    columns: constants.columns,
    dataSource: constants.data,
    updateEntityPath: &apos;update-product&apos;,
  });

  return (
    &lt;&gt;
      &lt;Header addNewPath=&quot;add-product&quot; hasSelected={hasSelected} /&gt;
      &lt;DataTable /&gt;
    &lt;/&gt;
  );
}

export default ShowProducts;
</code></pre><figcaption>ShowProducts.js</figcaption></figure><p>Columns and DataSource to table is defined in a constant file as follows:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Tag } from &apos;antd&apos;;

export const columns = [
  {
    title: &apos;Id&apos;,
    dataIndex: &apos;key&apos;,
    key: &apos;key&apos;,
  },
  {
    title: &apos;Name&apos;,
    dataIndex: &apos;name&apos;,
    key: &apos;name&apos;,
    render: text =&gt; &lt;a&gt;{text}&lt;/a&gt;,
  },
  {
    title: &apos;Description&apos;,
    dataIndex: &apos;description&apos;,
    key: &apos;description&apos;,
  },
  {
    title: &apos;Quantity&apos;,
    dataIndex: &apos;qty&apos;,
    key: &apos;qty&apos;,
  },
  {
    title: &apos;owner&apos;,
    dataIndex: &apos;owner&apos;,
    key: &apos;owner&apos;,
  },
  {
    title: &apos;Category&apos;,
    key: &apos;category&apos;,
    dataIndex: &apos;category&apos;,
    render: tags =&gt; (
      &lt;&gt;
        {tags.map(tag =&gt; {
          let color = &apos;blue&apos;;
          if (tag === &apos;accessory&apos;) {
            color = &apos;volcano&apos;;
          } else if (tag === &apos;clothing&apos;) {
            color = &apos;geekblue&apos;;
          } else if (tag === &apos;jewellery&apos;) {
            color = &apos;green&apos;;
          }
          return (
            &lt;Tag color={color} key={tag}&gt;
              {tag.toUpperCase()}
            &lt;/Tag&gt;
          );
        })}
      &lt;/&gt;
    ),
  },
];

export const data = {
  totalElements: 8,
  content: [
    {
      key: &apos;1&apos;,
      name: &apos;Personalized Bar Bracelet&apos;,
      description: &apos;This is a metal bracelet&apos;,
      qty: 32,
      owner: &apos;John Brown&apos;,
      category: [&apos;jewellery&apos;, &apos;accessory&apos;],
    },
    {
      key: &apos;2&apos;,
      name: &apos;Handcraft Boots&apos;,
      description: &apos;Vegan-friendly leather&apos;,
      qty: 12,
      owner: &apos;John Green&apos;,
      category: [&apos;clothing&apos;, &apos;living&apos;],
    },
    {
      key: &apos;3&apos;,
      name: &apos;Personalized Bar Bracelet&apos;,
      description: &apos;This is a metal bracelet&apos;,
      qty: 32,
      owner: &apos;John Brown&apos;,
      category: [&apos;jewellery&apos;, &apos;clothing&apos;],
    },
    // ...
  ],
};
</code></pre><figcaption>Constants.js</figcaption></figure><p>Of course the <code>data</code> variable is a mock here and in real life applications it is supposed to be retrieved from backend by API calls. So the table view becomes as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-17-at-21.40.44.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-2)" loading="lazy"></figure><p>See the commit for changes: <strong><a href="https://github.com/turkogluc/react-admin-portal/commit/1694602ebb2a7246c4d51dc3672a70ad1899df84?ref=localhost">1694602</a></strong></p><h3 id="add-new-item-with-forms">Add New Item with Forms</h3><p>Ant Design has the feature rich <code>Form</code> component that handles most of the work and design hassle for us. I will demonstrate a save product page using <code>Antd Form</code>.</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import {
  Switch,
  Card,
  Form,
  Input,
  Row,
  Col,
  Select,
  Divider,
  Button,
  InputNumber,
} from &apos;antd&apos;;
import { CheckOutlined, CloseOutlined } from &apos;@ant-design/icons&apos;;

const { Option } = Select;

function AddProduct() {
  const [form] = Form.useForm();

  const handleSave = values =&gt; {
    console.log(&apos;onFinish&apos;, values);
    // call save API
  };

  const requiredFieldRule = [{ required: true, message: &apos;Required Field&apos; }];

  const ownerArray = [
    {
      id: 1,
      value: &apos;John Nash&apos;,
    },
    {
      id: 2,
      value: &apos;Leonhard Euler&apos;,
    },
    {
      id: 3,
      value: &apos;Alan Turing&apos;,
    },
  ];

  const categoryArray = [
    {
      id: 1,
      value: &apos;Clothing&apos;,
    },
    {
      id: 2,
      value: &apos;Jewelery&apos;,
    },
    {
      id: 3,
      value: &apos;Accessory&apos;,
    },
  ];

  return (
    &lt;Card title=&quot;Add Product&quot; loading={false}&gt;
      &lt;Row justify=&quot;center&quot;&gt;
        &lt;Col span={12}&gt;
          &lt;Form
            labelCol={{ span: 4 }}
            wrapperCol={{ span: 16 }}
            form={form}
            name=&quot;product-form&quot;
            onFinish={handleSave}
          &gt;
            &lt;Form.Item label=&quot;Name&quot; name=&quot;name&quot; rules={requiredFieldRule}&gt;
              &lt;Input /&gt;
            &lt;/Form.Item&gt;
            &lt;Form.Item label=&quot;Description&quot; name=&quot;description&quot;&gt;
              &lt;Input /&gt;
            &lt;/Form.Item&gt;
            &lt;Form.Item label=&quot;Owner&quot; name=&quot;owner&quot;&gt;
              &lt;Select&gt;
                {ownerArray.map(item =&gt; (
                  &lt;Option key={item.id} value={item.id}&gt;
                    {item.value}
                  &lt;/Option&gt;
                ))}
              &lt;/Select&gt;
            &lt;/Form.Item&gt;
            &lt;Form.Item label=&quot;Category&quot; name=&quot;category&quot;&gt;
              &lt;Select&gt;
                {categoryArray.map(item =&gt; (
                  &lt;Option key={item.id} value={item.id}&gt;
                    {item.value}
                  &lt;/Option&gt;
                ))}
              &lt;/Select&gt;
            &lt;/Form.Item&gt;
            &lt;Form.Item label=&quot;Quantity&quot; name=&quot;qty&quot;&gt;
              &lt;InputNumber /&gt;
            &lt;/Form.Item&gt;
            &lt;Form.Item
              label=&quot;Status&quot;
              name=&quot;active&quot;
              valuePropName=&quot;checked&quot;
              initialValue={false}
            &gt;
              &lt;Switch
                checkedChildren={&lt;CheckOutlined /&gt;}
                unCheckedChildren={&lt;CloseOutlined /&gt;}
              /&gt;
            &lt;/Form.Item&gt;
            &lt;Divider /&gt;
            &lt;Row justify=&quot;center&quot;&gt;
              &lt;Button type=&quot;primary&quot; htmlType=&quot;submit&quot;&gt;
                Save
              &lt;/Button&gt;
            &lt;/Row&gt;
          &lt;/Form&gt;
        &lt;/Col&gt;
      &lt;/Row&gt;
    &lt;/Card&gt;
  );
}

export default AddProduct;
</code></pre><figcaption>AddProduct.js</figcaption></figure><p>We need to add the new component in the <code>RoutingList</code> as a new route. The <code>Add New</code> button in the Show Product page forwards to the <code>/add-product</code> path.</p><pre><code class="language-javascript">const routes = [
  // ...
  {
    path: &apos;/add-product&apos;,
    component: AddProduct,
    key: &apos;/add-product&apos;,
  },
];</code></pre><p>The view looks like as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-17-at-22.22.36.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-2)" loading="lazy"></figure><p>See the commit for the changes: <strong><a href="https://github.com/turkogluc/react-admin-portal/commit/790615d1d046b78637b61ba45239c8a36dd71ed8?ref=localhost">790615</a></strong>.</p><p><code>handleSave</code> method is called when the <code>Save</code> button is clicked and the values parameter contains the <code>json</code> that can be sent to backend save API.</p><p>What is Next</p><ul><li><a href="https://turkogluc.com/developing-admin-portal-with-react-redux-and-ant-design-part-3/">Developing Admin Portal with React, Redux, and Ant Design (Part-3)</a></li></ul>]]></content:encoded></item><item><title><![CDATA[Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-1)]]></title><description><![CDATA[Developing a Modern Admin Portal with React, Redux, and Ant Design, step by step guide, using custom reusable hooks. Installing Ant Design, Creating layout, React Routing]]></description><link>https://turkogluc.com/developing-react-admin-portal-with-redux-and-ant-design/</link><guid isPermaLink="false">646f24c59b6311000195da5c</guid><category><![CDATA[React]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Sun, 18 Oct 2020 09:30:05 GMT</pubDate><media:content url="https://turkogluc.com/content/images/2020/10/reactjs-thumb.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="getting-started">Getting Started</h2><img src="https://turkogluc.com/content/images/2020/10/reactjs-thumb.jpg" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-1)"><p>In this series I would like to share a step by step guide for developing high quality admin portals with <code>React</code> and <code>Ant Design</code>. I will share reusable generic components that will fasten the ground up development. In order to focus on the certain aspects, I divided it to multiple parts.</p><p>In this part I present the steps for preparing React and necessary modules, and then, gradually show the implementation of each separate component to build up the Admin Portal. If you like to see the complete code I committed in the following public repository:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/turkogluc/react-admin-portal?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">turkogluc/react-admin-portal</div><div class="kg-bookmark-description">Contribute to turkogluc/react-admin-portal development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicons/favicon.svg" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-1)"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">turkogluc</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://avatars3.githubusercontent.com/u/15093288?s=400&amp;v=4" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-1)"></div></a></figure><p>I use the <code><a href="https://ant.design/?ref=localhost">Ant Design</a></code> as the user interface design framework as it contains a set of high quality components and ready to use demos for building rich, interactive user interfaces. The list of Ant Design react components can be found in the following link:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://ant.design/components/overview/?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Components Overview - Ant Design</div><div class="kg-bookmark-description">antd provides plenty of UI components to enrich your web applications, and we will improve components experience consistently. We also recommand some great Third-Party Libraries additionally.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://gw.alipayobjects.com/zos/rmsportal/rlpTLlbMzTNYuZGGCVYM.png" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-1)"><span class="kg-bookmark-author">Ant Design</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://gw.alipayobjects.com/zos/rmsportal/rlpTLlbMzTNYuZGGCVYM.png" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-1)"></div></a></figure><h3 id="initializing-the-react-project">Initializing the React project</h3><p>We can use the following boilerplate code as it already contains <code>React 16</code>, <code>Webpack 4</code> with <code>babel 7</code>, the <code>webpack-dev-server</code>, <code>react-hot-loader</code> and <code>CSS-Modules</code>:</p><ul><li><a href="https://github.com/HashemKhalifa/webpack-react-boilerplate?ref=localhost">webpack-react-boilerplate </a></li></ul><pre><code class="language-bash">git clone https://github.com/HashemKhalifa/webpack-react-boilerplate.git
mv webpack-react-boilerplate react-admin-portal
cd react-admin-portal
yarn install
yarn start</code></pre><p>This repository is maintained continuously and the dependencies are upgraded. So it is good starting point as the most of the environment is already prepared and the technologies are up to date. </p><p>The web application starts at <code>http://localhost:8080</code> address.</p><h3 id="installing-ant-design">Installing Ant Design</h3><pre><code>yarn add antd
yarn add babel-plugin-import
yarn add less-loader
yarn add less</code></pre><p>While using <code>antd</code>, we can either import the complete style file in our root file, or add styling only for the used components which is better in terms of performance. That&apos;s why we added <code>babel-plugin-import</code> module and we will update the <code>webpack</code> configuration to import <code>less</code> styles as follows:</p><p>Add <code>antd</code> library option in <code>babel-loader</code> &#xA0;rule in <code>webpack-common.js</code>:</p><figure class="kg-card kg-code-card"><pre><code class="language-json">{
  test: /\.(js|jsx)$/,
  loader: &apos;babel-loader&apos;,
  exclude: /(node_modules)/,
  options: {
    presets: [&apos;@babel/react&apos;],
    plugins: [[&apos;import&apos;, { libraryName: &apos;antd&apos;, style: true }]],
  },
},</code></pre><figcaption>webpack-common.js</figcaption></figure><p>In the same file add also <code>.less</code> to the extensions:</p><pre><code>extensions: [&apos;*&apos;, &apos;.js&apos;, &apos;.jsx&apos;, &apos;.css&apos;, &apos;.scss&apos;, &apos;.less&apos;],</code></pre><p>Add <code>less-loader</code> to the <code>webpack-dev.js</code>:</p><figure class="kg-card kg-code-card"><pre><code class="language-json">{
	test: /\.less$/,
	use: [
		&apos;style-loader&apos;,
		&apos;css-loader&apos;,
		&apos;sass-loader&apos;,
		{
            loader: &apos;less-loader&apos;,
            options: {
              lessOptions: {
                javascriptEnabled: true,
              },
            },
		},
	],
},</code></pre><figcaption>webpack-dev.js</figcaption></figure><p>You can find these changes in the commit <a href="https://github.com/turkogluc/react-admin-portal/commit/6f0001f3db28392e10ffdcdbaf085fee05d079f5?ref=localhost"><strong>6f0001f</strong></a>.</p><h3 id="installing-redux">Installing Redux</h3><pre><code>yarn add @reduxjs/toolkit
yarn add react-redux
yarn add redux-logger</code></pre><p>Let&apos;s create our first reducer just as an example:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">const INITIAL_STATE = {};

export default (state = INITIAL_STATE, action) =&gt; {
  switch (action.type) {
    default:
      return state;
  }
};</code></pre><figcaption>initReducer.js</figcaption></figure><p>And add this reducer to global reducers list:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import { combineReducers } from &apos;redux&apos;;
import initReducer from &apos;./initReducer&apos;;

export default combineReducers({
  initReducer,
});</code></pre><figcaption>reducer/index.js</figcaption></figure><p>Now we can create our <code>store</code>:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import { createLogger } from &apos;redux-logger&apos;;
import { applyMiddleware, createStore } from &apos;redux&apos;;
import thunkMiddleware from &apos;redux-thunk&apos;;
import reducer from &apos;./reducer&apos;;

const loggerMiddleware = createLogger();

export const store = createStore(
  reducer,
  applyMiddleware(thunkMiddleware, loggerMiddleware)
);</code></pre><figcaption>store.js</figcaption></figure><p>And wrap the <code>App</code> component with <code>Provider</code>:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">function App() {
  return (
    &lt;Provider store={store}&gt;
      Hello world
    &lt;/Provider&gt;
  );
}</code></pre><figcaption>App.js</figcaption></figure><p>See the commit for changes: <a href="https://github.com/turkogluc/react-admin-portal/commit/98dd23bb90f8d2c14bdf2e408982747a41f489dc?ref=localhost"><strong>98dd23b</strong></a>.</p><h3 id="installing-react-router">Installing React Router</h3><pre><code class="language-javascript">yarn add react-router-dom</code></pre><p>Let&apos;s create browser history:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import { createBrowserHistory } from &apos;history&apos;;

export default createBrowserHistory();</code></pre><figcaption>history.js</figcaption></figure><p>We can create an empty <code>dashboard</code> page as the initial page in the routing list.</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;

function Dashboard() {
  return &lt;div&gt;Dashboard Page&lt;/div&gt;;
}

export default Dashboard;</code></pre><figcaption>Dashboard.js</figcaption></figure><p>Now we can create Routing list that will contain the path and the component mapping. <code>routes</code> variable contains the list of path component mapping. Dashboard Page is be added as the first path and the root path (<code>/</code>). </p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Route } from &apos;react-router-dom&apos;;
import Dashboard from &apos;../page/dashboard/Dashboard&apos;;

const routes = [
  {
    path: &apos;/&apos;,
    component: Dashboard,
    key: &apos;/&apos;,
  },
];

function RoutingList() {
  return routes.map(item =&gt; {
    if (item.path.split(&apos;/&apos;).length === 2) {
      return (
        &lt;Route
          exact
          path={item.path}
          component={item.component}
          key={item.key}
        /&gt;
      );
    }
    return &lt;Route path={item.path} component={item.component} key={item.key} /&gt;;
  });
}

export default RoutingList;</code></pre><figcaption>RoutingList.js</figcaption></figure><p>And we can wrap the <code>App</code> component with <code>BrowserRouter</code>. So the App component becomes as follows:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { hot } from &apos;react-hot-loader/root&apos;;
import { Provider } from &apos;react-redux&apos;;
import { BrowserRouter, Switch } from &apos;react-router-dom&apos;;
import { store } from &apos;./redux/store&apos;;
import history from &apos;./router/history&apos;;
import MainLayout from &apos;./page/layout/MainLayout&apos;;

function App() {
  return (
    &lt;Provider store={store}&gt;
      &lt;BrowserRouter history={history}&gt;
        &lt;Switch&gt;
          &lt;MainLayout /&gt;
        &lt;/Switch&gt;
      &lt;/BrowserRouter&gt;
    &lt;/Provider&gt;
  );
}
export default hot(App);</code></pre><figcaption>App.js</figcaption></figure><p>I have added another empty component which is <code>MainLayout</code>. It is the component that will contain the structure of the layout.</p><p>See the commit for changes: <strong><a href="https://github.com/turkogluc/react-admin-portal/commit/7afcd0c28ab9d4f0307d2806212b4c2ad5aa235b?ref=localhost">7afcd0c</a></strong>.</p><hr><h2 id="designing-the-layout">Designing the Layout</h2><p><code>MainLayout</code> component contains sider menu on the left header at the top and the content in the middle. Content part will be controller by the Router. We will place <code>&lt;RoutingList /&gt;</code> component in the content place so that we can to change the content by routing different paths. See the structures of the mentioned components in the next picture.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-16-at-23.14.47.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-1)" loading="lazy"></figure><p>Let&apos;s see the implementation of each component in details.</p><h3 id="user-avatar-component">User Avatar Component</h3><p>I use Ant Design <code><a href="https://ant.design/components/avatar/?ref=localhost">Avatar</a></code> component in order to represent User Avatar based on the users first name. </p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Avatar } from &apos;antd&apos;;

function getColor(username) {
  const colors = [
    &apos;#ffa38a&apos;,
    &apos;#a9a7e0&apos;,
    &apos;#D686D4&apos;,
    &apos;#96CE56&apos;,
    &apos;#4A90E2&apos;,
    &apos;#62b3d0&apos;,
    &apos;#ef7676&apos;,
  ];
  const firstChar = username.charCodeAt(0);
  const secondChar = username.charCodeAt(1);
  const thirdChar = username.charCodeAt(2);

  return colors[(firstChar + secondChar + thirdChar) % 7];
}

export const getUsernameAvatar = (username, size = &apos;large&apos;) =&gt; {
  return (
    &lt;div&gt;
      &lt;Avatar
        style={{
          backgroundColor: getColor(username),
          verticalAlign: &apos;middle&apos;,
        }}
        size={size}
      &gt;
        {username ? username.charAt(0).toUpperCase() : &apos;&apos;}
      &lt;/Avatar&gt;
    &lt;/div&gt;
  );
};</code></pre><figcaption>UserAvatar.js</figcaption></figure><p>It looks as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-16-at-23.04.11.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-1)" loading="lazy"></figure><h3 id="layout-banner-component">Layout Banner Component</h3><p>It is the header component in the layout that contains &#xA0;some horizontal <code><a href="https://ant.design/components/menu/?ref=localhost">menu</a></code>s from Ant Design such as user menu, language switcher etc.</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import {
  MenuUnfoldOutlined,
  MenuFoldOutlined,
  QuestionCircleOutlined,
  GlobalOutlined,
  BellOutlined,
  UserOutlined,
  LogoutOutlined,
} from &apos;@ant-design/icons&apos;;
import { Layout, Menu, Badge } from &apos;antd&apos;;
import &apos;./Style.less&apos;;
import { getUsernameAvatar } from &apos;../../component/UserAvatar&apos;;

const { Header } = Layout;
const { SubMenu } = Menu;

function LayoutBanner({ collapsed, handleOnCollapse }) {
  const getCollapseIcon = () =&gt; {
    if (collapsed) {
      return (
        &lt;MenuUnfoldOutlined onClick={handleOnCollapse} className=&quot;trigger&quot; /&gt;
      );
    }
    return &lt;MenuFoldOutlined onClick={handleOnCollapse} className=&quot;trigger&quot; /&gt;;
  };

  const handleLanguageMenuClick = () =&gt; {};
  const handleSettingMenuClick = () =&gt; {};
  const handleLogout = () =&gt; {};

  return (
    &lt;Header className=&quot;header&quot; style={{ background: &apos;#fff&apos;, padding: 0 }}&gt;
      &lt;div
        style={{
          float: &apos;left&apos;,
          width: &apos;100%&apos;,
          alignSelf: &apos;center&apos;,
          display: &apos;flex&apos;,
        }}
      &gt;
        {window.innerWidth &gt; 992 &amp;&amp; getCollapseIcon()}
      &lt;/div&gt;
      &lt;Menu
        // onClick={this.handleLanguageMenuClick}
        mode=&quot;horizontal&quot;
        className=&quot;menu&quot;
      &gt;
        &lt;SubMenu title={&lt;QuestionCircleOutlined /&gt;} /&gt;
      &lt;/Menu&gt;
      &lt;Menu
        // onClick={this.handleLanguageMenuClick}
        mode=&quot;horizontal&quot;
        className=&quot;menu&quot;
      &gt;
        &lt;SubMenu
          title={
            &lt;Badge dot&gt;
              &lt;BellOutlined /&gt;
            &lt;/Badge&gt;
          }
        /&gt;
      &lt;/Menu&gt;
      &lt;Menu
        onClick={handleLanguageMenuClick}
        mode=&quot;horizontal&quot;
        className=&quot;menu&quot;
      &gt;
        &lt;SubMenu title={&lt;GlobalOutlined /&gt;}&gt;
          &lt;Menu.Item key=&quot;en&quot;&gt;
            &lt;span role=&quot;img&quot; aria-label=&quot;English&quot;&gt;
              &#x1F1FA;&#x1F1F8; English
            &lt;/span&gt;
          &lt;/Menu.Item&gt;
          &lt;Menu.Item key=&quot;it&quot;&gt;
            &lt;span role=&quot;img&quot; aria-label=&quot;Italian&quot;&gt;
              &#x1F1EE;&#x1F1F9; Italian
            &lt;/span&gt;
          &lt;/Menu.Item&gt;
        &lt;/SubMenu&gt;
      &lt;/Menu&gt;
      &lt;Menu onClick={handleSettingMenuClick} mode=&quot;horizontal&quot; className=&quot;menu&quot;&gt;
        &lt;SubMenu title={getUsernameAvatar(&apos;Cemal&apos;)}&gt;
          &lt;Menu.Item key=&quot;setting:1&quot;&gt;
            &lt;span&gt;
              &lt;UserOutlined /&gt;
              Profile
            &lt;/span&gt;
          &lt;/Menu.Item&gt;
          &lt;Menu.Item key=&quot;setting:2&quot;&gt;
            &lt;span&gt;
              &lt;LogoutOutlined onClick={handleLogout} /&gt;
              Logout
            &lt;/span&gt;
          &lt;/Menu.Item&gt;
        &lt;/SubMenu&gt;
      &lt;/Menu&gt;
    &lt;/Header&gt;
  );
}

export default LayoutBanner;
</code></pre><figcaption>LayoutBanner.js</figcaption></figure><p>Adding styles:</p><figure class="kg-card kg-code-card"><pre><code class="language-css">.header {
  display: flex;
}

.trigger {
  margin-left: 16px;
  margin-right: 16px;
  align-self: center;

}

.menu {
  .ant-menu-horizontal {
    &amp; &gt; .ant-menu-submenu {
      float: right;
    }
    border: none;
  }
  box-shadow: #e4ecef;
  position: relative;
  .ant-menu-submenu-title {
    width: 64px;
    height: 64px;
    text-align: center;
    padding-top: 8px;
  }
}</code></pre><figcaption>Sytle.less</figcaption></figure><p>So it looks as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-16-at-23.11.36.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-1)" loading="lazy"></figure><h3 id="sider-menu-component">Sider Menu Component</h3><p>This component uses <code><a href="https://ant.design/components/layout/?ref=localhost#components-layout-demo-side">Sider</a></code>, <code><a href="https://ant.design/components/menu/?ref=localhost">Menu</a></code>, <code><a href="https://ant.design/components/icon/?ref=localhost">Icon</a></code> components of <a href="https://ant.design/components/overview/?ref=localhost">Ant Design</a>.</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React from &apos;react&apos;;
import { Layout, Menu } from &apos;antd&apos;;
import { useHistory } from &apos;react-router-dom&apos;;
import {
  DashboardOutlined,
  FundProjectionScreenOutlined,
  PartitionOutlined,
  SettingOutlined,
  TeamOutlined,
} from &apos;@ant-design/icons&apos;;
import &apos;./Style.less&apos;;

const { SubMenu } = Menu;

const { Sider } = Layout;

function SiderMenu({ handleOnCollapse, collapsed }) {
  const theme = &apos;light&apos;;

  const history = useHistory();

  const handleSiderMenuClick = action =&gt; {
    console.log(&apos;menu:&apos;, action);
    switch (action.key) {
      case &apos;dashboard&apos;:
        history.push(&apos;/&apos;);
        break;
      case &apos;showProducts&apos;:
        history.push(&apos;/products&apos;);
        break;
      case &apos;addProduct&apos;:
        history.push(&apos;/add-product&apos;);
        break;
      case &apos;showCustomers&apos;:
        history.push(&apos;/customers&apos;);
        break;
      case &apos;addCustomer&apos;:
        history.push(&apos;/add-customer&apos;);
        break;
      default:
        history.push(&apos;/&apos;);
    }
  };

  return (
    &lt;Sider
      breakpoint=&quot;lg&quot;
      collapsedWidth=&quot;80&quot;
      onCollapse={handleOnCollapse}
      collapsed={collapsed}
      width=&quot;256&quot;
      theme={theme}
    &gt;
      &lt;a&gt;
        &lt;div className=&quot;menu-logo&quot; /&gt;
      &lt;/a&gt;
      &lt;Menu mode=&quot;inline&quot; theme={theme} onClick={handleSiderMenuClick}&gt;
        &lt;Menu.Item key=&quot;dashboard&quot;&gt;
          &lt;DashboardOutlined /&gt;
          &lt;span className=&quot;nav-text&quot;&gt;Dashboard&lt;/span&gt;
        &lt;/Menu.Item&gt;
        &lt;SubMenu
          key=&quot;products&quot;
          title={
            &lt;span&gt;
              &lt;PartitionOutlined /&gt;
              &lt;span&gt;Products&lt;/span&gt;
            &lt;/span&gt;
          }
        &gt;
          &lt;Menu.Item key=&quot;showProducts&quot;&gt;
            &lt;span className=&quot;nav-text&quot;&gt;Show Products&lt;/span&gt;
          &lt;/Menu.Item&gt;
          &lt;Menu.Item key=&quot;addProduct&quot;&gt;
            &lt;span className=&quot;nav-text&quot;&gt;Add Product&lt;/span&gt;
          &lt;/Menu.Item&gt;
        &lt;/SubMenu&gt;
        &lt;SubMenu
          key=&quot;customers&quot;
          title={
            &lt;span&gt;
              &lt;TeamOutlined /&gt;
              &lt;span&gt;Customers&lt;/span&gt;
            &lt;/span&gt;
          }
        &gt;
          &lt;Menu.Item key=&quot;showCustomers&quot;&gt;
            &lt;span className=&quot;nav-text&quot;&gt;Show Customers&lt;/span&gt;
          &lt;/Menu.Item&gt;
          &lt;Menu.Item key=&quot;addCustomer&quot;&gt;
            &lt;span className=&quot;nav-text&quot;&gt;Add Customer&lt;/span&gt;
          &lt;/Menu.Item&gt;
        &lt;/SubMenu&gt;
        &lt;Menu.Item key=&quot;settings&quot;&gt;
          &lt;SettingOutlined /&gt;
          &lt;span className=&quot;nav-text&quot;&gt;Settings&lt;/span&gt;
        &lt;/Menu.Item&gt;
        &lt;Menu.Item key=&quot;reports&quot;&gt;
          &lt;FundProjectionScreenOutlined /&gt;
          &lt;span className=&quot;nav-text&quot;&gt;Reports&lt;/span&gt;
        &lt;/Menu.Item&gt;
      &lt;/Menu&gt;
    &lt;/Sider&gt;
  );
}

export default SiderMenu;</code></pre><figcaption>SiderMenu.js</figcaption></figure><p>We can add a logo above the sider menu, within the less style file.</p><figure class="kg-card kg-code-card"><pre><code class="language-css">.menu-logo {
  background-image: url(&apos;../../../public/icon.png&apos;);
  background-repeat: no-repeat;
  background-position: center;
  height: 35px;
  background-size: 100%;
  margin: 20px;
  color: #ffffff;
}</code></pre><figcaption>Sytle.less</figcaption></figure><h3 id="main-layout-component">Main Layout Component</h3><p>Main Layout component contains the <code><a href="https://ant.design/components/layout/?ref=localhost">Layout</a></code> and <code><a href="https://ant.design/components/layout/?ref=localhost#components-layout-demo-top">Content</a></code> of Ant Design. Layout wraps the entire body, our custom <code>SiderMenu</code> is placed on the left and <code>LayoutBanner</code> stands above the content. <code>RoutingList</code> component is added within the <code>Content</code>. React Router will render the route at this place.</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">import React, { useState } from &apos;react&apos;;
import { Layout } from &apos;antd&apos;;
import SiderMenu from &apos;./SiderMenu&apos;;
import LayoutBanner from &apos;./LayoutBanner&apos;;
import &apos;./Style.less&apos;;
import RoutingList from &apos;../../router/RoutingList&apos;;

const { Content } = Layout;

function MainLayout() {
  const [collapsed, setCollapsed] = useState(false);

  const handleOnCollapse = () =&gt; {
    setCollapsed(prevState =&gt; !prevState);
  };

  return (
    &lt;Layout style={{ minHeight: &apos;100vh&apos; }}&gt;
      &lt;SiderMenu collapsed={collapsed} handleOnCollapse={handleOnCollapse} /&gt;
      &lt;Layout&gt;
        &lt;LayoutBanner
          collapsed={collapsed}
          handleOnCollapse={handleOnCollapse}
        /&gt;
        &lt;Content style={{ margin: &apos;24px 16px 0&apos; }}&gt;
          &lt;div style={{ padding: 24, background: &apos;#fff&apos;, minHeight: 20 }}&gt;
            &lt;RoutingList /&gt;
          &lt;/div&gt;
        &lt;/Content&gt;
      &lt;/Layout&gt;
    &lt;/Layout&gt;
  );
}

export default MainLayout;</code></pre><figcaption>MainLayout.js</figcaption></figure><p>See the commit for changes: <strong><a href="https://github.com/turkogluc/react-admin-portal/commit/229b7762a78a8d222240186330d8f93d9189ce39?ref=localhost">229b776</a></strong>.</p><hr><h2 id="adding-new-pages-routes">Adding New Pages/Routes</h2><p>We create new components, with a simple content:</p><pre><code class="language-javascript">function ShowCustomers() {
  return &lt;div&gt;Customer Page&lt;/div&gt;;
}

function ShowProducts() {
  return &lt;div&gt;Product Page&lt;/div&gt;;
}</code></pre><p>And we can add new routes to the <code>RoutingList</code> component as follows:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">const routes = [
  {
    path: &apos;/&apos;,
    component: Dashboard,
    key: &apos;/&apos;,
  },
  {
    path: &apos;/customers&apos;,
    component: ShowCustomers,
    key: &apos;/customers&apos;,
  },
  {
    path: &apos;/products&apos;,
    component: ShowProducts,
    key: &apos;/products&apos;,
  },
];</code></pre><figcaption>RoutingList.js</figcaption></figure><p>In the <code>SiderMenu</code> click action sends these paths for the clicked keys:</p><figure class="kg-card kg-code-card"><pre><code class="language-javascript">case &apos;showProducts&apos;:
	history.push(&apos;/products&apos;);
	break;

case &apos;showCustomers&apos;:
    history.push(&apos;/customers&apos;);
    break;</code></pre><figcaption>SiderMenu.js</figcaption></figure><p>Now clicking <code>Show Products</code> renders the <code>ShowProducts</code> component within the content area.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/10/Screenshot-2020-10-17-at-00.35.56.png" class="kg-image" alt="Developing a Modern Admin Portal with React, Redux, and Ant Design (Part-1)" loading="lazy"></figure><p>See the commit for changes: <strong><a href="https://github.com/turkogluc/react-admin-portal/commit/4c52d5b901a5c34c0779fb0ea92fc8dc5c94e97d?ref=localhost">4c52d5b</a></strong>.</p><h3 id="what-is-next">What is Next?</h3><ul><li><a href="https://turkogluc.com/developing-admin-portal-with-react-redux-and-ant-design-part-2/">Developing Admin Portal with React, Redux, and Ant Design (Part-2)</a></li><li><a href="https://turkogluc.com/developing-admin-portal-with-react-redux-and-ant-design-part-3/">Developing Admin Portal with React, Redux, and Ant Design (Part-3)</a></li></ul>]]></content:encoded></item><item><title><![CDATA[Making Sense of Change Data Capture Pipelines for Postgres with Debezium Kafka Connector]]></title><description><![CDATA[Making Sense of Change Data Capture for Postgres with Debezium Kafka Connector. Understanding PostgreSQL Logical Replication and Write Ahead Log mechanism.]]></description><link>https://turkogluc.com/postgresql-capture-data-change-with-debezium/</link><guid isPermaLink="false">646f24c59b6311000195da52</guid><category><![CDATA[Apache Kafka]]></category><category><![CDATA[PostgreSQL]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Sun, 27 Sep 2020 13:39:38 GMT</pubDate><media:content url="https://turkogluc.com/content/images/2020/09/4cff1d7bfcc6fe58267f8b8cccb6b372-2.png" medium="image"/><content:encoded><![CDATA[<img src="https://turkogluc.com/content/images/2020/09/4cff1d7bfcc6fe58267f8b8cccb6b372-2.png" alt="Making Sense of Change Data Capture Pipelines for Postgres with Debezium Kafka Connector"><p>Should you need to get familiar with <a href="https://turkogluc.com/apache-kafka-connect-introduction/">Kafka Connect Basics</a> or <a href="https://turkogluc.com/kafka-connect-jdbc-source-connector/">Kafka JDBC Connector</a><strong> </strong>check out the previous post. This post focuses on PostgreSQL backup-replication mechanism and streaming data from database to Kafka with using Debezium connector.</p><p>There are basically 3 major methods to perform backups or replication in PostgreSQL:</p><ul><li>Logical dumps (Extracting SQL script that represents the data, for example with using <a href="https://www.postgresql.org/docs/12/app-pgdump.html?ref=localhost">pg_dump</a>)</li><li>Transactional Log Shipping (Using Write-Ahead Log)</li><li>Logical Decoding</li></ul><p>The first method is relatively harder to maintain and creates more load on the database. <code>Log shipping</code> and <code>Logical Decoding</code> is low level solutions that make use of transaction logs and have the major advantage of efficiency.</p><h2 id="understanding-the-transactional-log">Understanding the Transactional Log</h2><p>Transactional Log is the essential part of the modern relational database systems. It is basically <strong>history log of all actions and changes</strong> applied on the database. The database stores the data eventually in the filesystem, however this I/O operation is relatively costly and if any failure interrupts the writing process the file becomes inconsistent and it would not be easy to recover. </p><p>Therefore database writes the data to a log which is called <strong>Write-Ahead Log</strong> (WAL) directly and transaction is completed. When the data is logged in the WAL, it is considered to be successfully stored even though it is not written to file system yet. So even if system crashes or some failure arises, it will be read from the log when system restarts. </p><p>A process constantly reads the WAL, sets a <code>checkpoint</code> as point in time and writes the changes appended since the last checkpoint time to the actual database file system (every 5 mins by default). As the disk space is limited, the WAL files that are already processed are either archived or recycled (old ones are removed) in order to clean up disk space.</p><p>Each transaction log is divided into 16 MB file that is called <strong>WAL segment</strong> that consist of <strong>records</strong>. Each record has an identifier named <strong>Log Sequence Number</strong> (LSN) that shows the physical location of the record. Processing the transactions and checkpointing could be demonstrated as follows:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-27-at-01.51.59.png" class="kg-image" alt="Making Sense of Change Data Capture Pipelines for Postgres with Debezium Kafka Connector" loading="lazy"></figure><h2 id="it-s-all-about-wal-">It&apos;s All About WAL!</h2><p>The WAL has important role in replication as it contains the details about transactions, and these records are very important and useful in terms of representing the database state. Note that the WAL segments are not stored forever, once it is consumed it is removed from the disk. So one way to backup locally is copying segment files to a different directory (default dir: <code>pg_wal</code>). Sharing these records with remote servers gives the opportunity to create replicas efficiently.</p><h3 id="transactional-log-shipping">Transactional Log Shipping</h3><p>It is a solution to replicate the database in different servers by shipping the WAL segments. Replica server operates on recovery mode all the time and replays the WAL records that it receives to be consistent with primary server. This solution could be implemented by sending the complete 16MB WAL segments files or streaming individual records as they are written. Streaming is a better solution in terms of durability. See the details of psychical replication implementation details: <a href="https://www.postgresql.org/docs/13/warm-standby.html?ref=localhost">Log Shipping</a>.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-27-at-10.36.31.png" class="kg-image" alt="Making Sense of Change Data Capture Pipelines for Postgres with Debezium Kafka Connector" loading="lazy"></figure><p>In Log Shipping method the segment files which contains binary data copied physically (byte-by-byte) to another server. So it can be also called <strong>physical replication</strong>. This approach has some limitations and drawbacks as it is not possible to replicate between different versions of Postgres or different operating systems, and also it can not replicate part of a database. Logical replication is introduced to address these limitations.</p><h3 id="logical-replication">Logical Replication</h3><p>Logical replication is a method that decodes the binary WAL records into a more understandable format and sends this decoded data to remote server. It uses <code>publish</code> and <code>subscribe</code> model with one or more subscribers receiving data from publishers. In contrast to physical replication, there is no Primary and Standby servers but data can flow both sides, but from publishers to subscribers. Publisher creates a <a href="https://www.postgresql.org/docs/13/sql-createpublication.html?ref=localhost"><code>PUBLICATION</code></a>:</p><pre><code class="language-sql">CREATE PUBLICATION mypublication FOR TABLE users, departments;
CREATE PUBLICATION alltables FOR ALL TABLES;</code></pre><p>And Subscribers creates <code><a href="https://www.postgresql.org/docs/13/sql-createsubscription.html?ref=localhost">SUBSCRIPTION</a></code>:</p><pre><code class="language-sql">CREATE SUBSCRIPTION mysubcribtion
         CONNECTION &apos;host=localhost port=5432 user=foo dbname=foodb&apos;
        PUBLICATION mypublication, alltables;</code></pre><p>The changes send by publisher are replicated in the subscriber database to ensure that the two databases remain in sync. It is important to remember that after some time WAL segments are deleted. So if a subscriber stops for some and the the WAL record is deleted in the publisher it causes a FATAL error. </p><p><strong><a href="https://www.postgresql.org/docs/13/logicaldecoding-explanation.html?ref=localhost">Replication slots</a></strong> solves this problem for us. By assigning a subscriber to a replication slot we can guarantee that the WAL records that it related with the subscription are not going to be removed until the subscriber consumes them. <code>CREATE SUBSCRIPTION</code> command automatically generates a replication slot in the publisher side for us so we do not need to create it manually.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-27-at-12.14.55.png" class="kg-image" alt="Making Sense of Change Data Capture Pipelines for Postgres with Debezium Kafka Connector" loading="lazy"></figure><p>Now having the necessary knowledge of PostgreSQL replication concepts we can proceed to configure and manage the Debezium Connector.</p><h2 id="debezium-connector">Debezium Connector</h2><p>Debezium is an open source <strong>Change Data Capture</strong> platform that turns the existing database into event streams. Debezium Kafka Connector captures each row level change in the database and sends them to Kafka topics.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/debezium-architecture.png" class="kg-image" alt="Making Sense of Change Data Capture Pipelines for Postgres with Debezium Kafka Connector" loading="lazy"></figure><p>Debezium uses the <code>logical replication</code> feature of PostgreSQL in order to capture the transaction records from the WAL. The connector acts the subscriber role for the changes published from tables. It handles all the low level configuration for us, as creating publisher, subscribers replication slots etc.</p><p>In order to use the connector we need an <code>output plugin</code> installed in the PostgreSQL server so that it can decode the WAL records. Debezium connector supports number of output plugins:</p><ul><li><a href="https://github.com/debezium/postgres-decoderbufs?ref=localhost"><code>decoderbufs</code></a>: based on Protobuf and maintened by Debezium Community</li><li><a href="https://github.com/eulerto/wal2json/blob/master/README.md?ref=localhost"><code>wal2json</code></a>: based on JSON</li><li><code>pgoutput</code>: is the standard logical decoding output plug-in in PostgreSQL, also supported by Debezium Community.</li></ul><p>I personally recommend using <code>pgoutput</code> as it is the native plugin and it does not require any additional installation. If you would like to use another plugin you need to install it in the database server. See the <a href="https://debezium.io/documentation/reference/1.2/postgres-plugins.html?ref=localhost">documentation</a> for installing plugins. We can setup the all architecture with the following steps</p><h3 id="1-running-kafka-cluster">1-Running Kafka Cluster</h3><p>We can use the following docker-compose file to get Kafka cluster with a single broker up and running.</p><pre><code class="language-yml">version: &apos;2&apos;
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - &quot;2181:2181&quot;
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:latest
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - &quot;9092:9092&quot;
      - &quot;29092:29092&quot;
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  control-center:
    image: confluentinc/cp-enterprise-control-center:5.5.1
    hostname: control-center
    container_name: control-center
    depends_on:
      - zookeeper
      - kafka
    ports:
      - &quot;9021:9021&quot;
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: &apos;kafka:9092&apos;
      CONTROL_CENTER_ZOOKEEPER_CONNECT: &apos;zookeeper:2181&apos;
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      PORT: 9021

  postgresql:
    image: postgresql:12
    hostname: postgresql
    container_name: postgresql
    ports:
      - &quot;5432:5432&quot;
    environment:
      POSTGRES_DB: demo
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: root
</code></pre><p>Note that confluent center is optional and used only as the user interface for Kafka broker.</p><h3 id="2-preparing-debezium-connector-plugin">2- Preparing Debezium Connector Plugin</h3><p>Download the <a href="https://debezium.io/releases/1.2/?ref=localhost">Debezium PostgreSQL Connector</a> plugin and extract the zip file to the Kafka Connect&apos;s plugins path. While we start Kafka Connector we can specify a plugin path that will be used to access the plugin libraries. For example <code>plugin.path=/usr/local/share/kafka/plugins</code>. Check <a href="https://docs.confluent.io/current/connect/managing/install.html?ref=localhost#connect-install-connectors">Install Connector Manually</a> documentation for details.</p><h3 id="3-running-kafka-connect"><strong>3- Running Kafka Connect</strong></h3><p>We can run the Kafka Connect with <code>connect-distributed.sh</code> script that is located inside the kafka <code>bin</code> directory. We need to provide a properties file while running this script for configuring the worker properties.</p><p>We can create create <code>connect-distributed.properties</code> file to specify the worker properties as follows:</p><pre><code># A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
bootstrap.servers=localhost:29092

# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs
group.id=connect-cluster

# The converters specify the format of data in Kafka and how to translate it into Connect data.
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true

# Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.
offset.storage.topic=connect-offsets
offset.storage.replication.factor=1

# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,
config.storage.topic=connect-configs
config.storage.replication.factor=1

# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.
status.storage.topic=connect-status
status.storage.replication.factor=1

# Flush much faster than normal, which is useful for testing/debugging
offset.flush.interval.ms=10000

plugin.path=/Users/cemalturkoglu/kafka/plugins</code></pre><p>Note that the <code>plugin.path</code> is the path that we need to place the plugin library that we downloaded.</p><p>After running the connector we can confirm that connector&apos;s REST endpoint is accessible, and we can confirm that JDBC connector is in the plugin list by calling <a href="http://localhost:8083/connector-plugins?ref=localhost"><code>http://localhost:8083/connector-plugins</code></a></p><h3 id="4-preparing-postgresql-server-for-logical-replication">4- Preparing PostgreSQL Server for Logical Replication</h3><p>The first thing we need to configure in PostgreSQL is <code>wal_level</code> that can be one of <code>minimal</code>, <code>replica</code> and <code>logical</code>. This configuration specifies how much information should be written to WAL, and the default value is <code>replica</code>, we need to set it as <code>logical</code> to be able to stream records with logical replication.</p><p>The default <code>wal_level</code> can be seen with <code>SHOW wal_level;</code> command</p><pre><code class="language-bash">postgres=# SHOW wal_level;
 wal_level
-----------
 replica
(1 row)</code></pre><p>This configuration is in the <code>postgresql.conf</code> file which is generally located at <code>/var/lib/postgresql/data/pgdata/postgresql.conf</code>. To find the config location you can use <code>SHOW config_file;</code> command. In the PosgreSQL server go to this location and edit the file:</p><pre><code class="language-bash">~ docker exec -it 62ce968539a6 /bin/bash
root@db:/# nano /var/lib/postgresql/data/pgdata/postgresql.conf</code></pre><p>Set the <code>wal_level</code> to <code>logical</code> and save it. This change requires restarting the postgreSQL server. This is the bare minimum connection required to run the connector.</p><p>Also optionally, you can create another user who is not a super user for security reasons. This user need to have LOGIN and REPLICATION roles.</p><pre><code class="language-sql">CREATE ROLE name REPLICATION LOGIN;</code></pre><p>And this user should be added in <code>pg_hba.conf</code><em> </em>file to have authentication. This config may have <code>host all all</code> line then it accepts all connections and there is no need add the new user.</p><h3 id="5-starting-the-connector">5- Starting the Connector</h3><p>We run the connectors by calling REST endpoints with the configuration JSON. We can specify the configuration payload from a file for <code>curl</code> command. The following command starts the connector.</p><pre><code class="language-bash">curl -d @&quot;debezium-config.json&quot; \
-H &quot;Content-Type: application/json&quot; \
-X POST http://localhost:8083/connectors</code></pre><p>The configuration for the plugin is stored in <code>debezium-config.json</code> file can be as follows:</p><pre><code class="language-json">{
  &quot;name&quot;: &quot;debezium-connector&quot;,
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.debezium.connector.postgresql.PostgresConnector&quot;,
    &quot;database.hostname&quot;: &quot;localhost&quot;,
    &quot;database.port&quot;: &quot;5432&quot;,
    &quot;database.user&quot;: &quot;postgres&quot;,
    &quot;database.password&quot;: &quot;root&quot;,
    &quot;database.dbname&quot; : &quot;demo&quot;,
    &quot;plugin.name&quot;: &quot;pgoutput&quot;,
    &quot;database.server.name&quot;: &quot;demo-server&quot;
  }
}</code></pre><ul><li>The connector connets to the database with the connection string.</li><li>Reads the current position of the server&apos;s transaction log.</li><li>It starts by performing an initial consistent<em> </em>snapshot of each of the database schemas. It sends messages with <code>READ</code> event for each row encountered during the snapshot.</li><li>After snapshot is completed, it starts streaming changes as <code>CREATE</code>, <code>UPDATE</code> and <code>DELETE</code> events from the transaction log starting from the saved position. So if new records are added during the snapshot they will be captured from WAL and send to Kafka.</li></ul><p>We can see that 4 tables from my demo database are shipped to Kafka topics. Topic names are generated as: <code>&lt;server-name&gt;.&lt;schema-name&gt;.&lt;table-name&gt;</code></p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-27-at-13.58.32.png" class="kg-image" alt="Making Sense of Change Data Capture Pipelines for Postgres with Debezium Kafka Connector" loading="lazy"></figure><p>And each row in the tables are loaded as a message:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-27-at-14.04.06.png" class="kg-image" alt="Making Sense of Change Data Capture Pipelines for Postgres with Debezium Kafka Connector" loading="lazy"></figure><p>Each message contains <code>schema</code> and <code>payload</code> fields. Payload has the <code>before</code> and <code>after</code> fields specifying the object that is being changed. The payload also contains <code>op</code> field which stands for the operation and maps as follows:</p><ul><li><code>c</code>: create</li><li><code>r</code>: read</li><li><code>u</code>: update</li><li><code>d</code>: delete</li></ul><pre><code class="language-json">{
  &quot;schema&quot;: {
    &quot;type&quot;: &quot;struct&quot;,
    &quot;fields&quot;: [
      {
        &quot;type&quot;: &quot;struct&quot;,
        &quot;fields&quot;: [
          {
            &quot;type&quot;: &quot;int64&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;id&quot;
          },
          // ...
        ],
        &quot;optional&quot;: true,
        &quot;name&quot;: &quot;demo_server.public.address.Value&quot;,
        &quot;field&quot;: &quot;after&quot;
      },
    ],
    &quot;optional&quot;: false,
    &quot;name&quot;: &quot;demo_server.public.address.Envelope&quot;
  },
  &quot;payload&quot;: {
    &quot;before&quot;: null,
    &quot;after&quot;: {
      &quot;id&quot;: 5,
      &quot;flat&quot;: &quot;flat-1&quot;,
      &quot;postal_code&quot;: &quot;ABC&quot;,
      &quot;street&quot;: &quot;street&quot;,
      &quot;title&quot;: &quot;new address&quot;,
      &quot;city_id&quot;: 7,
      &quot;user_id&quot;: 2
    },
    &quot;source&quot;: {
      &quot;version&quot;: &quot;1.2.5.Final&quot;,
      &quot;connector&quot;: &quot;postgresql&quot;,
      &quot;name&quot;: &quot;demo-server&quot;,
      &quot;ts_ms&quot;: 1601157566900,
      &quot;snapshot&quot;: &quot;false&quot;,
      &quot;db&quot;: &quot;demo&quot;,
      &quot;schema&quot;: &quot;public&quot;,
      &quot;table&quot;: &quot;address&quot;,
      &quot;txId&quot;: 848,
      &quot;lsn&quot;: 24940072,
      &quot;xmin&quot;: null
    },
    &quot;op&quot;: &quot;c&quot;,
    &quot;ts_ms&quot;: 1601157567132,
    &quot;transaction&quot;: null
  }
}</code></pre><p>In this example <code>payload.op</code> is <code>c</code> and the <code>payload.before</code> field is null as this is a create operation.</p><h3 id="filtering-schema-and-tables">Filtering Schema and Tables</h3><ul><li><a href="https://debezium.io/documentation/reference/1.2/connectors/postgresql.html?ref=localhost#postgresql-property-schema-whitelist"><code><code>schema.whitelist</code></code></a> and <a href="https://debezium.io/documentation/reference/1.2/connectors/postgresql.html?ref=localhost#postgresql-property-schema-blacklist"><code><code>schema.blacklist</code></code></a> configuration properties can be used to choose the schemas to be subscribed to.</li><li><a href="https://debezium.io/documentation/reference/1.2/connectors/postgresql.html?ref=localhost#postgresql-property-table-whitelist"><code><code>table.whitelist</code></code></a> and <a href="https://debezium.io/documentation/reference/1.2/connectors/postgresql.html?ref=localhost#postgresql-property-table-blacklist"><code>table.blacklist</code></a> configuration properties can be used to choose the tables to be subscribed to. Table list should be comma separated list of <code>&lt;schema-name&gt;.&lt;table-name&gt;</code> format.</li><li><code>column.whitelist</code> and <code>column.blacklist</code> &#xA0;configuration properties can be used to choose the subset of columns.</li></ul><h3 id="replica-identity">Replica Identity</h3><p>Replica Identity is a table level PostgreSQL setting that is used to determine the amount of the information to be written in the WAL for update events. In the <code>UPDATE</code> and <code>DELETE</code> events there is a <code>payload.before</code> field which contains the previous object and some of its details. For example in a delete event:</p><pre><code class="language-json">{
   &quot;payload&quot;: {
      &quot;before&quot;: {
         &quot;id&quot;: 5,
         &quot;flat&quot;: null,
         &quot;postal_code&quot;: null,
         &quot;street&quot;: null,
         &quot;title&quot;: null,
         &quot;city_id&quot;: null,
         &quot;user_id&quot;: null
      },
      &quot;after&quot;: null,
      &quot;op&quot;: &quot;d&quot;,
      &quot;ts_ms&quot;: 1601157654097,
      &quot;transaction&quot;: null
   }
}</code></pre><p><code>after</code> is null as it is removed, and before contains only primary key of the object. The details of this object depends on the <code>REPLICA IDENTITY</code> of the table. It has the following options:</p><ul><li><code>DEFAULT</code>: contains only primary keys</li><li><code>FULL</code>: contains all of the columns</li><li><code>NOTHING</code>: contains no information</li></ul><p>We can set this config at the source table with the following command:</p><pre><code class="language-sql">ALTER TABLE table_name REPLICA IDENTITY FULL;</code></pre><p>After this change on <code>address</code> table if I update a column I get the following message with the payload containing full details of the before object:</p><pre><code class="language-json">{
   &quot;payload&quot;: {
      &quot;before&quot;: {
         &quot;id&quot;: 4,
         &quot;flat&quot;: &quot;10/1&quot;,
         &quot;postal_code&quot;: &quot;KLM123&quot;,
         &quot;street&quot;: &quot;Street3&quot;,
         &quot;title&quot;: &quot;office address&quot;,
         &quot;city_id&quot;: 8,
         &quot;user_id&quot;: 3
      },
      &quot;after&quot;: {
         &quot;id&quot;: 4,
         &quot;flat&quot;: &quot;10/1&quot;,
         &quot;postal_code&quot;: &quot;KLM02&quot;,
         &quot;street&quot;: &quot;St 15&quot;,
         &quot;title&quot;: &quot;office alternative address&quot;,
         &quot;city_id&quot;: 8,
         &quot;user_id&quot;: 3
      },
      &quot;op&quot;: &quot;u&quot;,
      &quot;ts_ms&quot;: 1601212592317,
      &quot;transaction&quot;: null
   }
}</code></pre><h3 id="final-words"><strong>Final words</strong></h3><p>In the <a href="https://turkogluc.com/kafka-connect-jdbc-source-connector/">Kafka JDBC Connector</a> post high level implementation of copying data from relational database to Kafka is discusses. JDBC connector uses SQL queries to retrieve data from database so it creates some load on the server. It is also not possible to retrieve DELETED rows in this solution.</p><p>Although it is easy to start and setup JDBC connectors, it has pitfalls. Transactional Log based Change Data Capture pipelines are better way to stream every single event from database to Kafka. CDC pipelines are more complex to set up at first than JDBC Connector, however as it directly interacts with the low level transaction log it is way more efficient. It does not generate load on the database.</p><p>One drawback of this approach is that it is not possible to get schema changes as events. If the schema changes in the source database, the destination client should be adjusted manually.</p><h3 id="references">References</h3><ul><li><a href="https://debezium.io/?ref=localhost">https://debezium.io/</a></li><li><a href="https://www.postgresql.org/docs/13/high-availability.html?ref=localhost">https://www.postgresql.org/docs/13/high-availability.html</a></li><li>Hans-Jurgen Schonig, Mastering PostgreSQL 12 Third Edition</li><li><a href="http://www.interdb.jp/pg/pgsql09.html?ref=localhost">http://www.interdb.jp/pg/pgsql09.html</a></li><li><a href="https://www.confluent.io/blog/no-more-silos-how-to-integrate-your-databases-with-apache-kafka-and-cdc/?ref=localhost">How to Integrate Your Databases with Apache Kafka and CDC</a>, by Robin Moffatt</li></ul>]]></content:encoded></item><item><title><![CDATA[Kafka Connect JDBC Source Connector]]></title><description><![CDATA[Kafka Connect JDBC Source Connector Configuration Examples, Working Modes, Moving Data from Postgresql]]></description><link>https://turkogluc.com/kafka-connect-jdbc-source-connector/</link><guid isPermaLink="false">646f24c59b6311000195da5b</guid><category><![CDATA[Apache Kafka]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Tue, 22 Sep 2020 15:00:40 GMT</pubDate><media:content url="https://turkogluc.com/content/images/2020/09/4cff1d7bfcc6fe58267f8b8cccb6b372-1.png" medium="image"/><content:encoded><![CDATA[<img src="https://turkogluc.com/content/images/2020/09/4cff1d7bfcc6fe58267f8b8cccb6b372-1.png" alt="Kafka Connect JDBC Source Connector"><p>Getting data from database to Apache Kafka is certainly one of the most popular use case of Kafka Connect. Kafka Connect provides scalable and reliable way to move the data in and out of Kafka. As it uses plugins for specific plugins for connectors and it is run by only configuration (without writing code) it is an easy integration point. </p><p>Visit the <a href="https://turkogluc.com/apache-kafka-connect-introduction/">Kafka Connect Basics</a> post if you would like to get an introduction. </p><h3 id="1-running-kafka-cluster">1- Running Kafka Cluster</h3><p>We can use the following docker-compose file to get Kafka cluster with a single broker up and running.</p><pre><code class="language-yaml">version: &apos;2&apos;
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - &quot;2181:2181&quot;
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:latest
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - &quot;9092:9092&quot;
      - &quot;29092:29092&quot;
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1</code></pre><p>If you would like to use a user interface rather than console tools to manage the Kafka, <a href="https://www.confluent.io/product/confluent-platform/gui-driven-management-and-monitoring/?ref=localhost">Confluent Control Center</a> is one of the best choice. It is commercial tool but it comes with 30 days licence. There are also <a href="https://lenses.io/?ref=localhost">Landoop UI</a> which has Kafka Connect management interface as well. If you would like to use Confluent Control Center you can add it as a service to the docker-compose file as follows:</p><pre><code class="language-yaml">control-center:
    image: confluentinc/cp-enterprise-control-center:5.5.1
    hostname: control-center
    container_name: control-center
    depends_on:
      - zookeeper
      - kafka
    ports:
      - &quot;9021:9021&quot;
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: &apos;kafka:9092&apos;
      CONTROL_CENTER_ZOOKEEPER_CONNECT: &apos;zookeeper:2181&apos;
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      PORT: 9021</code></pre><h3 id="2-preparing-connector-library">2- Preparing Connector Library</h3><p>Download the <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc?ref=localhost">Kafka Connect JDBC</a> plugin from Confluent hub and extract the zip file to the Kafka Connect&apos;s plugins path. While we start Kafka Connector we can specify a plugin path that will be used to access the plugin libraries. For example <code>plugin.path=/usr/local/share/kafka/plugins</code>. Check <a href="https://docs.confluent.io/current/connect/managing/install.html?ref=localhost#connect-install-connectors">Install Connector Manually</a> documentation for details. </p><p>We also need JDBC 4.0 driver as it will be used by the connector to communicate with the database. &#xA0;<code>Postgresql</code> and <code>sqlite</code> drivers are already shipped with JDBC connector plugin. If you like to connect to another database system add the driver to the same folder with <code>kafka-connect-jdbc</code> jar file. See <a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/index.html?ref=localhost#installing-jdbc-drivers">Installing JDBC Driver</a> Manual.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-21-at-19.32.36.png" class="kg-image" alt="Kafka Connect JDBC Source Connector" loading="lazy"></figure><h3 id="3-running-kafka-connect">3- Running Kafka Connect</h3><p>We can run the Kafka Connect with <code>connect-distributed.sh</code> script that is located inside the kafka <code>bin</code> directory. We need to provide a properties file while running this script for configuring the worker properties.</p><pre><code class="language-bash">connect-distributed.sh &lt;worker properties file&gt;</code></pre><p>We can create create <code>connect-distributed.properties</code> file to specify the worker properties as follows:</p><pre><code># A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
bootstrap.servers=localhost:29092

# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs
group.id=connect-cluster

# The converters specify the format of data in Kafka and how to translate it into Connect data.
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true

# Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.
offset.storage.topic=connect-offsets
offset.storage.replication.factor=1

# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,
config.storage.topic=connect-configs
config.storage.replication.factor=1

# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.
status.storage.topic=connect-status
status.storage.replication.factor=1

# Flush much faster than normal, which is useful for testing/debugging
offset.flush.interval.ms=10000

plugin.path=/Users/cemalturkoglu/kafka/plugins</code></pre><p>Note that the <code>plugin.path</code> is the path that we need to place the library that we downloaded.</p><p>After running the connector we can confirm that connector&apos;s REST endpoint is accessible, and we can confirm that JDBC connector is in the plugin list by calling <a href="http://localhost:8083/connector-plugins?ref=localhost"><code>http://localhost:8083/connector-plugins</code></a></p><pre><code class="language-json">[{&quot;class&quot;:&quot;io.confluent.connect.jdbc.JdbcSinkConnector&quot;,&quot;type&quot;:&quot;sink&quot;,&quot;version&quot;:&quot;5.5.1&quot;},{&quot;class&quot;:&quot;io.confluent.connect.jdbc.JdbcSourceConnector&quot;,&quot;type&quot;:&quot;source&quot;,&quot;version&quot;:&quot;5.5.1&quot;},{&quot;class&quot;:&quot;org.apache.kafka.connect.file.FileStreamSinkConnector&quot;,&quot;type&quot;:&quot;sink&quot;,&quot;version&quot;:&quot;2.6.0&quot;},{&quot;class&quot;:&quot;org.apache.kafka.connect.file.FileStreamSourceConnector&quot;,&quot;type&quot;:&quot;source&quot;,&quot;version&quot;:&quot;2.6.0&quot;},{&quot;class&quot;:&quot;org.apache.kafka.connect.mirror.MirrorCheckpointConnector&quot;,&quot;type&quot;:&quot;source&quot;,&quot;version&quot;:&quot;1&quot;},{&quot;class&quot;:&quot;org.apache.kafka.connect.mirror.MirrorHeartbeatConnector&quot;,&quot;type&quot;:&quot;source&quot;,&quot;version&quot;:&quot;1&quot;},{&quot;class&quot;:&quot;org.apache.kafka.connect.mirror.MirrorSourceConnector&quot;,&quot;type&quot;:&quot;source&quot;,&quot;version&quot;:&quot;1&quot;}]</code></pre><h3 id="4-starting-the-jdbc-connector">4. Starting the JDBC Connector</h3><p>As we operate on distributed mode we run the connectors by calling REST endpoints with the configuration JSON. We can specify the configuration payload from a file for <code>curl</code> command. The following command starts the connector.</p><pre><code class="language-bash">curl -d @&quot;jdbc-source.json&quot; \
-H &quot;Content-Type: application/json&quot; \
-X POST http://localhost:8083/connectors</code></pre><p>The configuration for the plugin is stored in <code>jdbc-source.json</code> file can be as follows:</p><pre><code class="language-json">{
    &quot;name&quot;: &quot;jdbc_source_connector_postgresql_01&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;io.confluent.connect.jdbc.JdbcSourceConnector&quot;,
        &quot;connection.url&quot;: &quot;jdbc:postgresql://localhost:5432/demo&quot;,
        &quot;connection.user&quot;: &quot;postgres&quot;,
        &quot;connection.password&quot;: &quot;root&quot;,
        &quot;topic.prefix&quot;: &quot;postgres-01-&quot;,
        &quot;poll.interval.ms&quot; : 3600000,
        &quot;mode&quot;:&quot;bulk&quot;
    }
}</code></pre><ul><li>The connector connects to the database with using the JDBC URL and connection credentials. </li><li>It will create kafka topic per table. Topics are named with the <code>topic.prefix</code> + &lt;table_name&gt;</li><li>The data is retrieved from database with the interval specified by <code>poll.interval.ms</code> config.</li><li>The <code>mode</code> configuration is to specify the working mode which will be discussed below. Bulk mode is used to load all the data.</li></ul><p>We can see that my demo database with 4 tables are loaded to the 4 kafka topics:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-21-at-20.44.28.png" class="kg-image" alt="Kafka Connect JDBC Source Connector" loading="lazy"></figure><p>And each row in the tables are loaded as a message.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-21-at-20.45.30.png" class="kg-image" alt="Kafka Connect JDBC Source Connector" loading="lazy"></figure><p>The message contains the following fields:</p><pre><code class="language-json">{
  &quot;schema&quot;: {
    &quot;type&quot;: &quot;struct&quot;,
    &quot;fields&quot;: [
      {
        &quot;type&quot;: &quot;int64&quot;,
        &quot;optional&quot;: false,
        &quot;field&quot;: &quot;id&quot;
      },
      {
        &quot;type&quot;: &quot;string&quot;,
        &quot;optional&quot;: true,
        &quot;field&quot;: &quot;flat&quot;
      },
      {
        &quot;type&quot;: &quot;string&quot;,
        &quot;optional&quot;: true,
        &quot;field&quot;: &quot;postal_code&quot;
      },
      {
        &quot;type&quot;: &quot;string&quot;,
        &quot;optional&quot;: true,
        &quot;field&quot;: &quot;street&quot;
      },
      {
        &quot;type&quot;: &quot;string&quot;,
        &quot;optional&quot;: true,
        &quot;field&quot;: &quot;title&quot;
      },
      {
        &quot;type&quot;: &quot;int64&quot;,
        &quot;optional&quot;: true,
        &quot;field&quot;: &quot;city_id&quot;
      },
      {
        &quot;type&quot;: &quot;int64&quot;,
        &quot;optional&quot;: true,
        &quot;field&quot;: &quot;user_id&quot;
      }
    ],
    &quot;optional&quot;: false,
    &quot;name&quot;: &quot;address&quot;
  },
  &quot;payload&quot;: {
    &quot;id&quot;: 3,
    &quot;flat&quot;: &quot;3/1B&quot;,
    &quot;postal_code&quot;: &quot;ABX501&quot;,
    &quot;street&quot;: &quot;Street2&quot;,
    &quot;title&quot;: &quot;work address&quot;,
    &quot;city_id&quot;: 7,
    &quot;user_id&quot;: 3
  }
}</code></pre><p>Note that it contains the <code>fields</code> attribute with the information about the fields and <code>payload</code> with the actual data.</p><h3 id="selecting-schema-and-tables-to-copy">Selecting Schema and Tables To Copy</h3><p>We can use <code>catalog.pattern</code> or <code>schema.pattern</code> to filter the schemas to be copied.</p><p>By default all tables are queried to be copied. However we include or exclude the list of tables in copying by <code>table.whitelist</code> and <code>table.blacklist</code> configurations. We can use either blacklist or whitelist at the same time.</p><p><code>table.whitelist:&quot;Users,Address,City&quot;</code></p><p><code>table.blacklist:&quot;Groups&quot;</code></p><h3 id="query-modes">Query Modes</h3><p>There are alternative incremental query modes to <code>bulk</code> mode which is used in the above demonstration. Incremental modes can be used to load the data only if there is a change. Certain columns are used to detect if there is a change in the table or row.</p><p><code><strong>bulk</strong></code>: In this mode connector will load <strong>all</strong> the selected tables <strong>in each iteration</strong>. If the iteration interval is set to some small number (5 seconds default) it wont make much sense to load all the data as there will be duplicate data. It can be useful if a periodical backup, or dumping the entire database.</p><p><strong><code>incrementing</code></strong>: This mode uses a <strong>single column</strong> that is <strong>unique</strong> for each row, ideally auto incremented primary keys to detect the changes in the table. If new row with new ID is added it will be copied to Kafka. However this mode lacks the capability of catching <strong>update</strong> operation on the row as it will not change the ID. <code>incrementing.column.name</code> is used to configure the column name.</p><p><strong><code>timestamp</code></strong>: Uses a single column that shows <strong>the last modification timestamp</strong> and in each iteration queries only for rows that have been modified since that time. As timestamp is not unique field, it can miss some updates which have the same timestamp. <code>timestamp.column.name</code> is used to configure the column name.</p><p><code><strong>timestamp+incrementing</strong></code>: Most robust and accurate mode that uses both a unique incrementing ID and timestamp. Only drawback is that it is needed to add modification timestamp column on legacy tables.</p><p><strong><code>query</code></strong>: The connector supports using custom queries to fetch data in each iteration. It is not very flexible in terms of incremental changes. It can be useful to fetch only necessary columns from a very wide table, or to fetch a view containing multiple joined tables. If the query gets complex, the load and the performance impact on the database increases.</p><h3 id="incremental-querying-with-timestamp">Incremental Querying with Timestamp</h3><p>Using only unique ID or timestamp has pitfalls as mentioned above. It is better approach to use them together. The following configuration shows an example of <code>timestamp+incrementing</code> mode:</p><pre><code class="language-json">{
    &quot;name&quot;: &quot;jdbc_source_connector_postgresql_02&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;io.confluent.connect.jdbc.JdbcSourceConnector&quot;,
        &quot;connection.url&quot;: &quot;jdbc:postgresql://localhost:5432/demo-db&quot;,
        &quot;connection.user&quot;: &quot;postgres&quot;,
        &quot;connection.password&quot;: &quot;root&quot;,
        &quot;topic.prefix&quot;: &quot;postgres-02-&quot;,
        &quot;table.whitelist&quot;: &quot;store,tag,category,address,city&quot;,
        &quot;mode&quot;:&quot;timestamp+incrementing&quot;,
        &quot;timestamp.column.name&quot;: &quot;last_modified_date&quot;,
        &quot;validate.non.null&quot;: false,
        &quot;db.timezone&quot;: &quot;Europe/Warsaw&quot;
    }
}</code></pre><p>Note the <code>validate.non.null</code> is used because connector requires the timestamp column to be <code>NOT NULL</code>, we can either set these columns NOT NULL or we can disable this validation with setting <code>validate.not.null</code> false.</p><p>While using the timestamp column timezone of the database system matters. There might be different behaviour because of time mismatches so it can be configure by <code>db.timezone</code>.</p><p><code>table.whitelist</code> configuration is used to limit the tables to given list. So these 5 tables are copied to Kafka topics.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-22-at-16.18.46.png" class="kg-image" alt="Kafka Connect JDBC Source Connector" loading="lazy"></figure><p>It is mentioned above that using <code>incrementing</code> mode without timestamp causes not capturing the <code>UPDATE</code> operations on the table. With the <code>timestamp+incrementing</code> mode update operations are captured as well.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-22-at-16.40.39.png" class="kg-image" alt="Kafka Connect JDBC Source Connector" loading="lazy"></figure><h3 id="final-words">Final words</h3><p>JDBC Connector is great way to start for shipping data from relational databases to Kafka. It is easy to setup and use, only it is needed to configure few properties to get you data streamed out. However there are some drawbacks of JDBC connector as well. Some of the drawbacks can be listed as:</p><ul><li>It needs to constantly run queries, so it generates some load on the physical database. To not cause performance impacts, queries should be kept simple, and scalability should not be used heavily.</li><li>As the incremental timestamp is mostly needed, working on legacy datastore would need extra work to add columns. There can be also cases that it is not possible to update the schema.</li><li>JDBC Connector <strong>can not fetch DELETE operations </strong>as it uses SELECT queries to retrieve data and there is no sophisticated mechanism to detect the deleted rows. You can implement your solution to overcome this problem.</li></ul><h3 id="references">References</h3><ul><li><a href="https://www.confluent.io/blog/kafka-connect-deep-dive-jdbc-source-connector/?_ga=2.242664011.856618672.1600704303-1221623284.1596107588&amp;ref=localhost">Kafka Connect Deep Dive &#x2013; JDBC Source Connector</a> by <a href="https://rmoff.net/?ref=localhost">Robin Moffatt</a></li><li><a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/source_config_options.html?ref=localhost">JDBC Connector Source Connector Configuration Properties</a></li></ul>]]></content:encoded></item><item><title><![CDATA[Spring Data JPA Auditing]]></title><description><![CDATA[Spring Data auditing, keep track of the entity state changes as created by, modified by, created date, last modified date]]></description><link>https://turkogluc.com/spring-data-jpa-auditing/</link><guid isPermaLink="false">646f24c59b6311000195da5a</guid><category><![CDATA[JPA]]></category><category><![CDATA[Spring]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Sun, 20 Sep 2020 13:54:53 GMT</pubDate><media:content url="https://turkogluc.com/content/images/2020/09/5368464-thumb.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://turkogluc.com/content/images/2020/09/5368464-thumb.jpg" alt="Spring Data JPA Auditing"><p>Spring Data provides a great support to keep track of the persistence layer changes. By using auditing, we can store or log the information about the changes on the entity such as who created or changed the entity and when the change is made.</p><p>We can make use of the annotations like <code>@CreatedBy</code>, <code>@CreatedDate</code>, <code>@LastModifiedDate</code>, <code>@LastModifiedBy</code> annotation on the entity fields to instruct the Spring JPA to transparently fill these fields. We can use the annotations as follows:</p><pre><code class="language-java">@Entity
public class Category {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private Long title;

    @CreatedBy
    private Long createdBy;

    @CreatedDate
    private LocalDateTime createdDate;

    @LastModifiedBy
    private Long lastModifiedBy;

    @LastModifiedDate
    private LocalDateTime lastModifiedDate;

    // getters and setter..
}</code></pre><p>Auditing feature is generally needed in most of the entities, so it is a better approach to create and abstract class <code>Auditable</code> class that contains the auditing fields and extend the abstract class by the entities that need auditing. By this way we will avoid duplicating the same fields in all entities.</p><h3 id="creating-abstract-auditable-class">Creating Abstract Auditable Class</h3><p>We can create an Abstract class to contain the audit related fields as follows:</p><pre><code class="language-java">@MappedSuperclass
@EntityListeners(AuditingEntityListener.class)
public abstract class Auditable {

    @CreatedBy
    @Column(columnDefinition = &quot;bigint default 1&quot;, updatable = false)
    protected Long createdBy;

    @CreatedDate
    @Column(columnDefinition = &quot;timestamp default &apos;2020-04-10 20:47:05.967394&apos;&quot;, updatable = false)
    protected LocalDateTime createdDate;

    @LastModifiedBy
    @Column(columnDefinition = &quot;bigint default 1&quot;)
    protected Long lastModifiedBy;

    @LastModifiedDate
    @Column(columnDefinition = &quot;timestamp default &apos;2020-04-10 20:47:05.967394&apos;&quot;)
    protected LocalDateTime lastModifiedDate;

    public Long getCreatedBy() {
        return createdBy;
    }

    public void setCreatedBy(Long createdBy) {
        this.createdBy = createdBy;
    }

    public LocalDateTime getCreatedDate() {
        return createdDate;
    }

    public void setCreatedDate(LocalDateTime createdDate) {
        this.createdDate = createdDate;
    }

    public Long getLastModifiedBy() {
        return lastModifiedBy;
    }

    public void setLastModifiedBy(Long lastModifiedBy) {
        this.lastModifiedBy = lastModifiedBy;
    }

    public LocalDateTime getLastModifiedDate() {
        return lastModifiedDate;
    }

    public void setLastModifiedDate(LocalDateTime lastModifiedDate) {
        this.lastModifiedDate = lastModifiedDate;
    }
}</code></pre><p><code>@MappedSuperclass</code> annotation is used to specify that the class itself is not an entity but its attributes can be<em> </em>mapped in the same way as an entity, however this mappings will apply only to its subclasses. So each class inherits the abstract <code>Auditable</code> class will contain these attributes.</p><p><code>@EntityListeners</code> annotation is used to configure <code>AuditingEntityListener</code> which contains the <code>@PrePersist</code> and <code>@PreUpdate</code> methods in order to capture auditing information</p><h3 id="enable-auditing-feature">Enable Auditing Feature</h3><p>In order to enable the auditing feature in Spring we need to use <code>@EnableJpaAuditing</code> annotation.</p><pre><code class="language-java">@SpringBootApplication
@EnableJpaAuditing
public class BackendApplication {
	public static void main(String[] args) {
		SpringApplication.run(BackendApplication.class, args);
	}
}</code></pre><h3 id="provide-auditor">Provide Auditor</h3><p><code>createdDate</code> and <code>lastModifiedDate</code> fields are filled according to the current time. Besides, <code>createdBy</code> and <code>lastModifiedBy</code> annotations needs a way to get the user who is performing the action. In order to provide this information we need to implement the <code>AuditorAware</code> interface.</p><pre><code class="language-java">@Component
public class AuditAwareImpl implements AuditorAware &lt;Long&gt; {

    @Override
    public Optional &lt;Long&gt; getCurrentAuditor() {
        ApplicationUser principal = (ApplicationUser) SecurityContextHolder.getContext().getAuthentication().getPrincipal();
        return Optional.of(principal.getId());
    }
}</code></pre><p>We added the implementation of <code>getCurrentAuditor</code> method because it is invoked to retrieve the user who is performing the operation.</p><h3 id="extend-the-auditable-class-in-entities">Extend the Auditable Class in Entities</h3><p>Now we can extend the <code>Auditable</code> class in the entities that we want to use auditing. For example:</p><pre><code class="language-java">@Entity
@Data
public class Category extends Auditable {

    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Id
    private Long id;
    private String title;

    // ..
}</code></pre><p>And the auditing fields will be filled automatically:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-20-at-15.49.34.png" class="kg-image" alt="Spring Data JPA Auditing" loading="lazy"></figure>]]></content:encoded></item><item><title><![CDATA[Understanding the effective data fetching with JPA Entity Graphs (Part-2)]]></title><description><![CDATA[JPA effective data fetching with Named Entity Graphs, fetch strategies, subgraphs]]></description><link>https://turkogluc.com/understanding-the-effective-data-fetching-with-jpa-entity-graphs-part-2/</link><guid isPermaLink="false">646f24c59b6311000195da58</guid><category><![CDATA[JPA]]></category><category><![CDATA[Spring]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Fri, 18 Sep 2020 19:30:14 GMT</pubDate><media:content url="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-18-at-21.38.30.png" medium="image"/><content:encoded><![CDATA[<h3 id="part-2-the-solution">Part-2: The Solution</h3><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-18-at-21.38.30.png" alt="Understanding the effective data fetching with JPA Entity Graphs (Part-2)"><p>In the <a href="https://turkogluc.com/understanding-jpa-entity-graphs/">previous post</a> I tried to demonstrate JPA fetching strategies. The are number of problems using the <code>FetchType.Lazy</code> or <code>FetchType.Eager</code> when it is the only configuration for the entity for fetching plan. </p><ul><li>It causes <code>N+1 problem</code> which runs many unnecessary queries and affect the data access layer performance very badly. </li><li>It does not give the flexibility to choose different fetch strategies for different scenarios, so it is limiting.</li><li>Very likely to put you in trouble with JPA lazy initialisation exception that is caused by accessing the proxy objects that are not fetched from the outside of transaction context.</li></ul><p>What is needed to improve this behaviour is that using SQL <code>JOIN</code> s when it is intended to retrieve the referenced objects. One of the solution could be writing your own queries for more predictable and better performing queries.</p><pre><code class="language-java">@Query(
value = &quot;SELECT a FROM Address AS a &quot; +
	&quot;JOIN FETCH a.user user &quot; +
    &quot;JOIN FETCH a.city city &quot;
)
List &lt;Address&gt; findAllAddresses();</code></pre><p>Although it is good enough to perform join queries when it is necessary, in a big scale enterprise project most probably number of such methods will increase a lot and there might me lots of almost duplicate SQL queries needed. So for such cases Entity Graphs seems to be a better fit.</p><h3 id="entity-graphs">Entity Graphs</h3><p>Entity graphs are introduced in <a href="https://download.oracle.com/otndocs/jcp/persistence-2_1-fr-eval-spec/index.html?ref=localhost">JPA 2.1</a> and used to allow partial or specified fetching of objects. When an entity has references to other entities we can specify a fetch plan by entity graphs in order to determine which fields or properties should be fetched together. We can describe a fetch plan with its paths and boundaries with <code>@NamedEntityGraph</code> annotation in the entity class.</p><pre><code class="language-java">@NamedEntityGraph(
        name = &quot;address-city-user-graph&quot;,
        attributeNodes = {
                @NamedAttributeNode(&quot;city&quot;),
                @NamedAttributeNode(&quot;user&quot;)
        }
)
@Entity
public class Address {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    private String title;
    private String street;
    private String flat;
    private String postalCode;

    @OneToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = &quot;city_id&quot;)
    private City city;

    @ManyToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = &quot;user_id&quot;)
    private User user;

    // getters and setters

}</code></pre><p>I switched the fetch type to be lazy, and defined and entity graph named &#xA0;<code>address-city-user-graph</code>. Entity graph contains the city and user attributes so that those objects will be retrieved all together. So for example when I want to retrieve only the address data, I can use <code>findAll</code> method and children object will not be queries because of lazy fetch strategy. However if I would like to retrieve details of users and city as well I can use the <code>address-city-user-graph</code> entity graph that I defined. To assign a defined NamedEntityGraph in the queries we use <code>@EntityGraph</code> annotation on the repository methods. </p><pre><code class="language-java">@Repository
public interface AddressRepository extends JpaRepository&lt;Address, Long&gt; {

    @EntityGraph(&quot;address-city-user-graph&quot;)
    List &lt;Address&gt; findByUserId(Long userId);

    @EntityGraph(&quot;address-city-user-graph&quot;)
    List&lt;Address&gt; findByCityId(Long cityId);
}</code></pre><p>Entity graph annotation takes the name of <code>NamedEntityGraph</code> which is only a user defined String as an identifier. So we can define this fetch plan by entity graph and use it multiple times. If I were to write SQL queries I would have to write duplicate JOIN queries multiple times. Now when I call the <code>findByUserId</code> method it runs a single query with JOINs:</p><pre><code class="language-sql">-- hibernate
    select
        address0_.id as id1_0_0_,
        user1_.id as id1_2_1_,
        city2_.id as id1_1_2_,
        address0_.city_id as city_id6_0_0_,
        address0_.flat as flat2_0_0_,
        address0_.postal_code as postal_c3_0_0_,
        address0_.street as street4_0_0_,
        address0_.title as title5_0_0_,
        address0_.user_id as user_id7_0_0_,
        user1_.email as email2_2_1_,
        user1_.name as name3_2_1_,
        user1_.password as password4_2_1_,
        user1_.phone as phone5_2_1_,
        city2_.name as name2_1_2_
    from
        address address0_
    left outer join
        users user1_
            on address0_.user_id=user1_.id
    left outer join
        city city2_
            on address0_.city_id=city2_.id
    where
        user1_.id=?</code></pre><p>We can define multiple entity graphs in an entity within <code>@NamedEntityGraphs</code> annotation:</p><pre><code class="language-java">@NamedEntityGraphs({
        @NamedEntityGraph(
                name = Address.WITH_USER_GRAPH,
                attributeNodes = {
                        @NamedAttributeNode(&quot;user&quot;)
                }
        ),
        @NamedEntityGraph(
                name = Address.WITH_CITY_GRAPH,
                attributeNodes = {
                        @NamedAttributeNode(&quot;city&quot;),
                }
        ),
        @NamedEntityGraph(
                name = Address.WITH_USER_AND_CITY_GRAPH,
                attributeNodes = {
                        @NamedAttributeNode(&quot;user&quot;),
                        @NamedAttributeNode(&quot;city&quot;)
                }
        )
})
@Entity
public class Address {

    public static final String WITH_USER_GRAPH = &quot;address-with-user-graph&quot;;
    public static final String WITH_CITY_GRAPH = &quot;address-with-city-graph&quot;;
    public static final String WITH_USER_AND_CITY_GRAPH = &quot;address-with-user-and-city-graph&quot;;

    // fields..

}</code></pre><p>And we can assign each entity graph to repository methods as we&apos;d like:</p><pre><code class="language-java">@Repository
public interface AddressRepository extends JpaRepository&lt;Address, Long&gt; {

    @EntityGraph(Address.WITH_USER_AND_CITY_GRAPH)
    List&lt;Address&gt; findAll();

    @EntityGraph(Address.WITH_USER_GRAPH)
    List &lt;Address&gt; findByUserId(Long userId);

    @EntityGraph(Address.WITH_CITY_GRAPH)
    List&lt;Address&gt; findByCityId(Long cityId);

    Optional&lt;Address&gt; findById(Long id);

}</code></pre><p>In the example there is no Entity Graph assigned to <code>findById</code> method so it will use FetchTypes from the entity which is defined lazy so it will only retrieve the fields of address but not children. If I run the <code>findByCity</code> method thanks to its Entity Graph it returns the city field of the address in the same query as well.</p><pre><code class="language-sql">Hibernate:
    select
        address0_.id as id1_0_0_,
        city1_.id as id1_1_1_,
        address0_.city_id as city_id6_0_0_,
        address0_.flat as flat2_0_0_,
        address0_.postal_code as postal_c3_0_0_,
        address0_.street as street4_0_0_,
        address0_.title as title5_0_0_,
        address0_.user_id as user_id7_0_0_,
        city1_.name as name2_1_1_
    from
        address address0_
    left outer join
        city city1_
            on address0_.city_id=city1_.id
    where
        city1_.id=?</code></pre><h3 id="types-of-entity-graph">Types of Entity Graph</h3><p><code>@EntityGraph</code> annotation takes a <code>type</code> parameter with 2 values:</p><ul><li><code>FETCH</code>: It is the default graph type. When it is selected the attributes that are specified by attribute nodes of the entity graph are treated as FetchType.EAGER and attributes that are not specified are treated as FetchType.LAZY.</li><li><code>LOAD</code>: When this type is selected the attributes that are specified by attribute nodes of the entity graph are treated as FetchType.EAGER and attributes that are not specified are treated according to their specified or default FetchType.</li></ul><p>Selecting LOAD type example:</p><pre><code class="language-java">@EntityGraph(value = Address.WITH_USER_AND_CITY_GRAPH, type = EntityGraph.EntityGraphType.LOAD)
List&lt;Address&gt; findAll();</code></pre><h3 id="subgraphs">Subgraphs</h3><p>Subgraphs can be defined in a <code>NamedEntityGraph</code> in order to specify fetch plan of the fields that belongs to the child.</p><p>In the <a href="https://turkogluc.com/understanding-jpa-entity-graphs/">previous post</a> User entity did not have reference for Address relation, so the relation was mapped with unidirectional <code>@ManyToOne</code> on the Address entity. To demonstrate the the subgraphs I will make this relation bidirectional with adding <code>@OneToMany</code> to the User entity, and also I will create another entity named <code>Group</code> that will be parent entity of User. </p><pre><code class="language-java">@Entity
@Table(name = &quot;users&quot;)
public class User {

	// ..

	@OneToMany(fetch = FetchType.LAZY, mappedBy = &quot;user&quot;)
    private Set &lt;Address&gt; addressList = new HashSet &lt;&gt;();

    @ManyToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = &quot;group_id&quot;)
    private Group group;

}</code></pre><p>And the group entity:</p><pre><code class="language-java">@Entity
public class Group {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private String name;

    @OneToMany(fetch = FetchType.LAZY, mappedBy = &quot;group&quot;)
    private Set &lt;User&gt; userList = new HashSet &lt;&gt;();

    // getter and setters..

}</code></pre><p>Now I will define an entity graph in order to retrieve group with its users that contains its children addresses, and addresses contain city information. So I would like to see such a tree from parent to child in 1 query.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-18-at-21.20.27.png" class="kg-image" alt="Understanding the effective data fetching with JPA Entity Graphs (Part-2)" loading="lazy"></figure><p>The entity graph contains the subgraph and subgraphs are defined with <code>@NamedSubgraph</code> annotation. This annotation also takes <code>@NamedAttributeNode</code> to specify the attributes to be fetched.</p><pre><code class="language-java">@NamedEntityGraphs(
        @NamedEntityGraph(
                name = Group.WITH_USER_AND_SUB_GRAPH,
                attributeNodes = {
                        @NamedAttributeNode( value = &quot;userList&quot;, subgraph = &quot;userSubGraph&quot;) },
                subgraphs = {
                        @NamedSubgraph(
                                name = &quot;userSubGraph&quot;,
                                attributeNodes = { @NamedAttributeNode( value = &quot;addressList&quot;, subgraph = &quot;addressSubGraph&quot;) }
                        ),
                        @NamedSubgraph(
                                name = &quot;addressSubGraph&quot;,
                                attributeNodes = {
                                        @NamedAttributeNode(&quot;city&quot;)
                                }
                        )
                }
        )
)
@Entity
@Table(name = &quot;groups&quot;)
public class Group {

	// ..
}</code></pre><p>So all the tree is retrieved in one query with joining the tables:</p><pre><code class="language-sql">Hibernate:
    select
        group0_.id as id1_2_0_,
        userlist1_.id as id1_3_1_,
        addresslis2_.id as id1_0_2_,
        city3_.id as id1_1_3_,
        group0_.name as name2_2_0_,
        userlist1_.email as email2_3_1_,
        userlist1_.group_id as group_id6_3_1_,
        userlist1_.name as name3_3_1_,
        userlist1_.password as password4_3_1_,
        userlist1_.phone as phone5_3_1_,
        userlist1_.group_id as group_id6_3_0__,
        userlist1_.id as id1_3_0__,
        addresslis2_.city_id as city_id6_0_2_,
        addresslis2_.flat as flat2_0_2_,
        addresslis2_.postal_code as postal_c3_0_2_,
        addresslis2_.street as street4_0_2_,
        addresslis2_.title as title5_0_2_,
        addresslis2_.user_id as user_id7_0_2_,
        addresslis2_.user_id as user_id7_0_1__,
        addresslis2_.id as id1_0_1__,
        city3_.name as name2_1_3_
    from
        groups group0_
    left outer join
        users userlist1_
            on group0_.id=userlist1_.group_id
    left outer join
        address addresslis2_
            on userlist1_.id=addresslis2_.user_id
    left outer join
        city city3_
            on addresslis2_.city_id=city3_.id</code></pre><p>So I have the object with it is references loaded:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-18-at-21.26.20.png" class="kg-image" alt="Understanding the effective data fetching with JPA Entity Graphs (Part-2)" loading="lazy"></figure>]]></content:encoded></item><item><title><![CDATA[Understanding the effective data fetching with JPA Entity Graphs (Part-1)]]></title><description><![CDATA[Understanding JPA fetch strategies, FetchType Lazy and FetchType Eager. Effective entity fetching with Entity Graphs with join queries]]></description><link>https://turkogluc.com/understanding-jpa-entity-graphs/</link><guid isPermaLink="false">646f24c59b6311000195da57</guid><category><![CDATA[JPA]]></category><category><![CDATA[Spring]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Fri, 18 Sep 2020 15:38:33 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1544383835-bda2bc66a55d?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<h3 id="part-1-the-problem">Part-1: The problem</h3><img src="https://images.unsplash.com/photo-1544383835-bda2bc66a55d?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Understanding the effective data fetching with JPA Entity Graphs (Part-1)"><p>JPA provides 2 types of fetching strategy for the entities that have relationship between each other (such as <code>OneToOne</code>, <code>OneToMany</code>..), :</p><ul><li>FetchType.<strong>LAZY</strong></li><li>FetchType.<strong>EAGER</strong></li></ul><p>This configuration alone is criticised for being statically declared and applied to every fetch call. I would like to explain what exactly that means with an example. Having the following entities:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-18-at-16.12.42.png" class="kg-image" alt="Understanding the effective data fetching with JPA Entity Graphs (Part-1)" loading="lazy"><figcaption>Entity-Relation Diagram</figcaption></figure><p></p><p>City Entity:</p><pre><code class="language-java">@Entity
public class City {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private String name;

    // getters and setters

}</code></pre><p>User Entity:</p><pre><code class="language-java">@Entity
@Table(name = &quot;users&quot;)
public class User {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private String name;
    private String email;
    private String password;
    private String phone;

    // getters and setters
}</code></pre><p>Address entity</p><pre><code class="language-java">@Entity
public class Address {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    private String title;
    private String street;
    private String flat;
    private String postalCode;

    @OneToOne
    @JoinColumn(name = &quot;city_id&quot;)
    private City city;

    @ManyToOne
    @JoinColumn(name = &quot;user_id&quot;)
    private User user;

    // getters and setters
}</code></pre><p>I have inserted following entities to the database:</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-18-at-16.44.15.png" class="kg-image" alt="Understanding the effective data fetching with JPA Entity Graphs (Part-1)" loading="lazy"></figure><p>We should keep in mind that address is the owning side of the both relations, because it has the foreign keys, and it is also called child entity. </p><p>Now if I will invoke find all method of the address repository <code>addressRepository.findAll()</code> the following queries are being retrieved:</p><pre><code class="language-sql">-- Hibernate:
    select
        address0_.id as id1_0_,
        address0_.city_id as city_id6_0_,
        address0_.flat as flat2_0_,
        address0_.postal_code as postal_c3_0_,
        address0_.street as street4_0_,
        address0_.title as title5_0_,
        address0_.user_id as user_id7_0_
    from
        address address0_
-- Hibernate:
    select
        city0_.id as id1_1_0_,
        city0_.name as name2_1_0_
    from
        city city0_
    where
        city0_.id=?
-- Hibernate:
    select
        user0_.id as id1_2_0_,
        user0_.email as email2_2_0_,
        user0_.name as name3_2_0_,
        user0_.password as password4_2_0_,
        user0_.phone as phone5_2_0_
    from
        users user0_
    where
        user0_.id=?
-- Hibernate:
    select
        user0_.id as id1_2_0_,
        user0_.email as email2_2_0_,
        user0_.name as name3_2_0_,
        user0_.password as password4_2_0_,
        user0_.phone as phone5_2_0_
    from
        users user0_
    where
        user0_.id=?
-- Hibernate:
    select
        city0_.id as id1_1_0_,
        city0_.name as name2_1_0_
    from
        city city0_
    where
        city0_.id=?</code></pre><p>So what happens here is step by step as follows:</p><ul><li>find all addresses -&gt; returns <code>A1</code>, <code>A2</code>, <code>A3</code> </li><li>For A1, find the referenced city -&gt; returns <code>C1</code></li><li>For A1, find the referenced user -&gt; returns <code>U1</code></li><li>For A2, the city referenced is already stored in the persistence context so it is not retrieved again, find the referenced user -&gt; returns <code>U2</code></li><li>For A3, the user referenced is already in the persistence context so it not retrieved again, find the referenced city -&gt; returns <code>C2</code></li></ul><p>This behaviour is thanks to first level caching mechanism. Once an entity is in managed state, The Entity Manager keeps it in the cache, so that it is not retrieved from database again.</p><p>But why there are so many calls? The problem resides in the fetching strategy. The child entity annotations (<code>OneToOne</code>, <code>ManyToOne</code>) by default configured to use <code>FetchType.EAGER</code>. So when an address is retrieved from database, JPA immediately calls its parents as well. </p><p>If we had very large number of rows in each table the list of separate calls would be incredibly long as well and obviously this is very bad in terms of performance. Eager fetching strategy generates this issue named <code>N+1 problem</code>, which means we are invoking 1 select query and it generates other N number separate calls for its children. That&apos;s it is recommended to use lazy fetching mechanism. So if I update my Address entity to use lazy fetching:</p><pre><code class="language-java">@OneToOne(fetch = FetchType.LAZY)
@JoinColumn(name = &quot;city_id&quot;)
private City city;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = &quot;user_id&quot;)
private User user;</code></pre><p>and invoke the find all method again I can see hibernate logs only 1 select query:</p><pre><code class="language-sql">-- Hibernate:
    select
        address0_.id as id1_0_,
        address0_.city_id as city_id6_0_,
        address0_.flat as flat2_0_,
        address0_.postal_code as postal_c3_0_,
        address0_.street as street4_0_,
        address0_.title as title5_0_,
        address0_.user_id as user_id7_0_
    from
        address address0_</code></pre><p>The retrieved address object contains proxies (not the actual reference) to the parent objects. If I try to access the city or user of the retrieved address within the transaction then it will be retrieved from database again. And if I do this operation in a loop of addresses <code>N+1 Problem</code> will appear again. </p><p>Another problem is that sometimes I would like to retrieve the addresses lazily, but sometimes with its user and (or) city information. So setting the fetch type on the entity affects all retrieve calls and does not give the flexibility for different fetch types. That&apos;s why we can make use of entity graphs in order to generate multiple fetch plans for different purposes.</p><p>Possible solutions and particularly entity graphs are discussed in the <a href="https://turkogluc.com/understanding-the-effective-data-fetching-with-jpa-entity-graphs-part-2/">Part-2</a>, keep reading.</p>]]></content:encoded></item><item><title><![CDATA[Kafka Connect Basics]]></title><description><![CDATA[Getting started with Apache Kafka Connect. Understanding high Level Overview. Running Kafka Connect in Docker.]]></description><link>https://turkogluc.com/apache-kafka-connect-introduction/</link><guid isPermaLink="false">646f24c59b6311000195da56</guid><category><![CDATA[Apache Kafka]]></category><dc:creator><![CDATA[Cemal Turkoglu]]></dc:creator><pubDate>Mon, 14 Sep 2020 12:53:17 GMT</pubDate><media:content url="https://turkogluc.com/content/images/2020/09/4cff1d7bfcc6fe58267f8b8cccb6b372.png" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-12-at-15.35.45.png" class="kg-image" alt="Kafka Connect Basics" loading="lazy"></figure><img src="https://turkogluc.com/content/images/2020/09/4cff1d7bfcc6fe58267f8b8cccb6b372.png" alt="Kafka Connect Basics"><p>Kafka Connect is an open source Apache Kafka component that helps to move the data <strong>IN</strong> or <strong>OUT</strong> of Kafka easily. It provides a scalable, reliable, and simpler way to move the data between Kafka and other data sources. According to direction of the data moved, the connector is classified as:</p><ul><li><code>Source Connector</code>: Reads data from a datasource and writes to Kafka topic.</li><li><code>Sink Connector</code>: Reads data from Kafka topic and writes to a datasource.</li></ul><p>Kafka Connect uses connector plugins that are community developed libraries to provide most common data movement cases. Mostly developers need to implement migration between same data sources, such as PostgreSQL, MySQL, Cassandra, MongoDB, Redis, JDBC, FTP, MQTT, Couchbase, REST API, S3, ElasticSearch. Kafka plugins provides the standardised implementation for moving the data from those datastores. Find all available Kafka Connectors on <a href="https://www.confluent.io/hub/?ref=localhost">Confluent Hub</a>.</p><p>So what Kafka Connect provides is that rather than writing our own Consumer or Producer code, we can use a Connector that takes care of all the implementation details such as fault tolerance, delivery semantics, ordering etc. and get the data moved. For example we can move all of the data from Postgres database to Kafka and from Kafka to ElasticSearch without writing code. It makes it easy for non-experienced developers to get the data in or out of Kafka reliably.</p><h2 id="concepts">Concepts</h2><p>Connector plugins implement the connector API that includes connectors and tasks.</p><ul><li><code>Connector</code>: is a job that manages and coordinates the tasks. It decides how to split the data-copying work between the tasks.</li><li><code>Task</code>: is piece of work that provides service to accomplish actual job. </li></ul><p>Connectors divide the actual job into smaller pieces as tasks in order to have the ability to scalability and fault tolerance. The state of the tasks is stored in special Kafka topics, and it is configured with <code>offset.storage.topic</code>, <code>config.storage.topic</code> and <code>status.storage.topic</code>. As the task does not keep its state it can be started, stopped and restarted at any time or nodes.</p><p>For example JDBC Connector is used to copy data from databases and it creates task per each table in the database.</p><ul><li><code>Worker</code>: is the node that is running the connector and its tasks. &#xA0;</li></ul><p>Kafka Connect workers executes 2 types of working modes:</p><ul><li><code>Standalone mode</code>: All work is performed in a single worker as single process. It is easier to setup and configure and can be useful where using single worker makes sense. However it does not provide fault tolerance or scalability.</li><li><code>Distributed mode</code>: Multiple workers are in a cluster. Configured by REST API. Provides scalability and fault tolerance. When one connector dies, its tasks are redistributed by rebalance mechanism among other workers.</li></ul><h2 id="running-kafka-connect">Running Kafka Connect</h2><p>Kafka Connect ships with Apache Kafka binaries. So there is no need to install it separately, but in order to run it we need to <a href="https://kafka.apache.org/downloads?ref=localhost">download Kafka binaries</a>. The executables are in the bin directory and configurations are in the config directory.</p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-13-at-16.13.47.png" class="kg-image" alt="Kafka Connect Basics" loading="lazy"></figure><p>I personally would prefer you to start practising with distributed mode as it is gets unnecessarily confusing if you work with the standalone and after switch to distributed mode. Also it is recommended to use distributed mode in production, and if we don&apos;t want to have a cluster we can run only 1 worker in distributed mode. </p><h3 id="1-running-kafka-cluster">1 - Running Kafka Cluster</h3><p>Let&apos;s start with getting a Kafka cluster up and running. We can set up a cluster with one zookepeer and one broker in docker environment with using the following docker compose file. </p><pre><code class="language-yaml">version: &apos;2&apos;
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - &quot;2181:2181&quot;
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:latest
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - &quot;9092:9092&quot;
      - &quot;29092:29092&quot;
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1</code></pre><p>Run the <code>docker-compose up -d</code> command to start the containers. One thing to pay attention here is that <code>KAFKA_ADVERTISED_LISTENERS</code> are set to be <code>localhost:29092</code> for outside of docker network, and <code>kafka:9092</code> for inside the docker network. So from out host machine we can access kafka instance with <code>localhost:29092</code>.</p><h3 id="2-running-kafka-connect">2 - Running Kafka Connect</h3><p>We can run the Kafka Connect with <code>connect-distributed.sh</code> script that is located inside the kafka <code>bin</code> directory. We need to provide a properties file while running this script for configuring the worker properties. </p><pre><code class="language-bash">connect-distributed.sh &lt;worker properties file&gt;</code></pre><p>We can create create <code>connect-distributed.properties</code> file to specify the worker properties as follows:</p><pre><code># A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
bootstrap.servers=localhost:29092

# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs
group.id=connect-cluster

# The converters specify the format of data in Kafka and how to translate it into Connect data.
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true

# Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.
offset.storage.topic=connect-offsets
offset.storage.replication.factor=1

# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,
config.storage.topic=connect-configs
config.storage.replication.factor=1

# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.
status.storage.topic=connect-status
status.storage.replication.factor=1

# Flush much faster than normal, which is useful for testing/debugging
offset.flush.interval.ms=10000</code></pre><p><code>group.id</code> is one of the most important configuration in this file. Worker groups are created according to group id. So if we start multiple worker with same group id, they will be in the same worker cluster.</p><p><code>offset.storage.topic</code>, <code>config.storage.topic</code> and <code>status.storage.topic</code> configurations are also needed so that worker status will be stored in Kafka topics and new workers or restarted workers will be managed accordingly.</p><p>Now we can start Kafka connect with the following command:</p><pre><code class="language-bash">connect-distributed.sh /path-to-config/connect-distributed.properties</code></pre><h3 id="3-starting-connector">3 - Starting Connector</h3><p>Now we have Zookeeper, Kafka broker, and Kafka Connect running in distributed mode. As it is mentioned before, in distributed mode, connectors are manages by REST API. Our connector exposed REST API at <a href="http://localhost:8083/?ref=localhost">http://localhost:8083/</a>.</p><p>As an example, we can run a <code>FileStreamSource</code> connector that copies data from a file to Kafka topic. To start a connector we need to send a POST call to <a href="http://localhost:8083/connectors?ref=localhost">http://localhost:8083/connectors</a> endpoint with the configuration of the Connector that we want to run. Example configuration for Connector looks like as follows:</p><pre><code class="language-json">{
  &quot;name&quot;: &quot;local-file-source&quot;,
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;FileStreamSource&quot;,
    &quot;tasks.max&quot;: 1,
    &quot;file&quot;: &quot;/Users/cemalturkoglu/kafka/shared-folder/file.txt&quot;,
    &quot;topic&quot;: &quot;file.content&quot;
  }
}
</code></pre><p>Every connector may have its own specific configurations, and these configurations can be found in the connector&apos;s Confluent Hub page.</p><ul><li><code>connector.class</code> specifies the connector plugin that we want to use. </li><li><code>file</code> is a specific config for <code>FileStreamSource</code> plugin, and it is used to point the file to be read.</li><li><code>topic</code> is the name of the topic that the data read from file will be written to.</li></ul><p>We need to send this json config in the content body of REST call. We can read this config from file for <code>curl</code> command as follows:</p><pre><code class="language-bash">curl -d @&quot;connect-file-source.json&quot; \
-H &quot;Content-Type: application/json&quot; \
-X POST http://localhost:8083/connectors</code></pre><p>After this call connector starts running, it reads data from the file and send to the kafka topic which is <code>file.content</code> in the example. If we start a consumer to this topic: </p><figure class="kg-card kg-image-card"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-14-at-13.41.00.png" class="kg-image" alt="Kafka Connect Basics" loading="lazy"></figure><p>We can see that every line in the file.txt is send to Kafka topic as a message. Note that <code>key.converter.schemas.enable</code> and <code>value.converter.schemas.enable</code> is set to be true for the worker at the beginning. So messages are wrapped with Json schema. </p><p>In order to scale up the worker cluster, you need to follow the same steps of running Kafka Connect and starting Connector on each worker (All workers should have same group id). The high level overview of the architecture looks like as follows:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://turkogluc.com/content/images/2020/09/Screenshot-2020-09-14-at-14.49.33.png" class="kg-image" alt="Kafka Connect Basics" loading="lazy"><figcaption>Kafka Connect Overview</figcaption></figure><h2 id="running-kafka-connect-in-docker">Running Kafka Connect in Docker</h2><p>In the above example Kafka cluster was being run in Docker but we started the Kafka Connect in the host machine with Kafka binaries. If you wish to run Kafka Connect in Docker container as well, you need a linux image that has Java 8 installed and you can download the Kafka and use <code>connect-distribued.sh</code> script to run it. For a very simple example, you can use the following Dockerfile to run workers:</p><pre><code class="language-Dockerfile">FROM openjdk:8-jre-slim

RUN apt-get update &amp;&amp; \
    apt-get install wget -y

COPY start.sh start.sh
COPY connect-distributed.properties connect-distributed.properties

RUN echo &quot;Downloading Apache Kafka&quot; &amp;&amp; \
    wget &quot;http://ftp.man.poznan.pl/apache/kafka/2.6.0/kafka_2.12-2.6.0.tgz&quot; &amp;&amp;\
    tar -xzvf kafka*.tgz &amp;&amp; \
    rm kafka*.tgz &amp;&amp; \
    mv kafka* kafka &amp;&amp; \
    export PATH=&quot;$PATH:$(pwd)kafka/bin&quot;


CMD /kafka/bin/connect-distributed.sh connect-distributed.properties</code></pre><p>You can customise the Dockerfile according to your needs and improve it or you can use Confluent&apos;s Kafka Connect image by adding it to the docker-compose file as follows:</p><pre><code class="language-yml">connect:
    image: confluentinc/cp-kafka-connect:latest
    hostname: connect
    container_name: connect
    depends_on:
      - zookeeper
      - kafka
    ports:
      - &quot;8083:8083&quot;
    volumes:
      - ./shared-folder:/shared-folder
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_ZOOKEEPER_CONNECT: zookeeper:2181
      CONNECT_PLUGIN_PATH: /usr/share/java</code></pre>]]></content:encoded></item></channel></rss>
